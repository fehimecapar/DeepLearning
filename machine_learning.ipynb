{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fehimecapar/DeepLearning/blob/main/machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsqKM5DLlLRm"
      },
      "source": [
        "# Task 2 – MLP, Learning rate, Overfitting, and Hyper-parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y6CxL6eC2Og"
      },
      "source": [
        "### <span style=\"color:red\">Deadline Tuesday, June 3, 2025 at 11:59 p.m<span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaoMWjbZdCLf"
      },
      "source": [
        "# IMPORTANT SUBMISSION INSTRUCTIONS\n",
        "\n",
        "- When you're done, download the notebook and rename it to task02_[name].ipynb\n",
        "- Only submit the ipynb file, no other file is required.\n",
        "- The deadline is strict.\n",
        "- Minimal requirement for passing: solving all code cells.\n",
        "\n",
        "Implementation\n",
        "- Do not change the cells which are marked as \"Do not change\", similarly write your solution to the marked cells. Do not create additional cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8RW2QHHFQYe"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this task you will implement an MLP model for virtual sensing using the flood dataset.\n",
        "The objectives are:\n",
        "- Implementing an MLP model via TensorFlow Functional API.\n",
        "- Getting more familiar with model fitting and overfitting.\n",
        "- Implementing early stopping.\n",
        "- Exploring hyperparameters and their influence.\n",
        "- Selecting model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhw-7EA-10Xm"
      },
      "source": [
        "## Tutorials\n",
        "\n",
        "Some python libraries are required to accomplish the tasks assigned in this homework. If you feel like you need to follow a tutorial before, feel free to do so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMyEloGz__BZ"
      },
      "source": [
        "*   [Scikit-learn Tutorials](https://www.tensorflow.org/tutorials)\n",
        "*   [TensorFlow Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n",
        "*   [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ActTWMg4XZ5v"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LArjND15dGNh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers, losses, callbacks, Input\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "SEED = 24\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpqN-O0F_atv"
      },
      "source": [
        "## System checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1r5yPHY_hsl",
        "outputId": "64da9e9f-b8b2-4786-9aa2-af0aaed858bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "cpus = tf.config.list_physical_devices('CPU')\n",
        "print(gpus)\n",
        "print(cpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6900WHTvC2Oq"
      },
      "source": [
        "Choose your device for computation. CPU or one of your CUDA devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MMCbCpAFao6q"
      },
      "outputs": [],
      "source": [
        "tf.config.set_visible_devices(gpus, 'GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "266Jdm74C2Or"
      },
      "source": [
        "# Subtask 2.1\n",
        "\n",
        "## Backpropagation – Chain Rule\n",
        "\n",
        "### Model\n",
        "Let us assume an MLP with one hidden layer containing one neuron with `sigmoid` function ($S(x) = 1 / (1 + e^{-x})$) as the activation function. The input and the output dimensions are equal to one. The activation function for the input and output layers is `linear`.\n",
        "The mapping $f: x \\to \\tilde{y}$ from the input $x$ to output $\\tilde{y}$ can be written as:\n",
        "\\begin{equation}\n",
        "\\tilde{y} = f(x) = w_2 S(w_1 x)\n",
        "\\end{equation}\n",
        "where $w_1$ and $w_2$ are the weights of the model. Note that we do not have bias for this example. Let us consider mean-squared error as the loss function. The loss $\\ell$ can be obtained as:\n",
        "\\begin{equation}\n",
        "\\ell = (y - \\tilde{y})^2 = (y - w_2 S(w_1 x))^2\n",
        "\\end{equation}\n",
        "where $y$ denotes the reference label. Let's initialize the weights as $w_1 = 0.1$ and $w_2 = 0.1$.\n",
        "\n",
        "### Data\n",
        "\n",
        "Let's assume we have a data set containing three samples as: $x = [1.0, 2.0, 3.0]^{T}$ and $y = [1.0, 4.0, 9.0]^{T}$.\n",
        "\n",
        "#### TODO\n",
        " - Perform gradient descent by hand with a learning rate of 0.1. Train the model for 1 epoch with the batch size of 1.\n",
        " - Report the prediction $\\tilde{y}$, loss, gradients, and the updated weights at every iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R12dNNXRC2Os"
      },
      "source": [
        "<span style='color:red'>**Your answer:**</span>\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uCghpErC2Os"
      },
      "source": [
        "#### TODO\n",
        "- Define two functions that compute the gradients of the loss with respect to $w_1$ and $w_2$ using analytical differentiation and chain rule.\n",
        "- Use the functions you defined and train the model again using gradient descent and the same learning rate.\n",
        "- Report the prediction $\\tilde{y}$, loss, gradients, and the updated weights at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm6JCJEYC2Os",
        "outputId": "1ba60670-a0f9-4bc6-cc1e-0104f6d07818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value 1\n",
            "x = 1.0, y = 1.0\n",
            "Prediction (ŷ) = 0.0525\n",
            "Loss = 0.8978\n",
            "Gradient w.r.t w1 = -0.0473, w2 = -0.9948\n",
            "Updated w1 = 0.1047, w2 = 0.1995\n",
            "----------------------------------------\n",
            "value 2\n",
            "x = 2.0, y = 4.0\n",
            "Prediction (ŷ) = 0.1101\n",
            "Loss = 15.1309\n",
            "Gradient w.r.t w1 = -0.7675, w2 = -4.2957\n",
            "Updated w1 = 0.1815, w2 = 0.6291\n",
            "----------------------------------------\n",
            "value 3\n",
            "x = 3.0, y = 9.0\n",
            "Prediction (ŷ) = 0.3981\n",
            "Loss = 73.9928\n",
            "Gradient w.r.t w1 = -7.5437, w2 = -10.8873\n",
            "Updated w1 = 0.9358, w2 = 1.7178\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def dL_dw2(x, y, w1, w2):\n",
        "    z = w1 * x\n",
        "    s = sigmoid(z)\n",
        "    y_pred = w2 * s\n",
        "    dL = -2 * (y - y_pred) * s\n",
        "    return dL\n",
        "\n",
        "def dL_dw1(x, y, w1, w2):\n",
        "    z = w1 * x\n",
        "    s = sigmoid(z)\n",
        "    y_pred = w2 * s\n",
        "    d_sigmoid = s * (1 - s)\n",
        "    dL = -2 * (y - y_pred) * w2 * d_sigmoid * x\n",
        "    return dL\n",
        "\n",
        "# default weights\n",
        "w1 = 0.1\n",
        "w2 = 0.1\n",
        "lr = 0.1\n",
        "\n",
        "# Data\n",
        "X = [1.0, 2.0, 3.0]\n",
        "Y = [1.0, 4.0, 9.0]\n",
        "\n",
        "for i in range(len(X)):\n",
        "    x = X[i]\n",
        "    y = Y[i]\n",
        "\n",
        "    z = w1 * x\n",
        "    s = sigmoid(z)\n",
        "    y_pred = w2 * s\n",
        "\n",
        "    #loss\n",
        "    loss = (y - y_pred)**2\n",
        "\n",
        "    grad_w1 = dL_dw1(x, y, w1, w2)\n",
        "    grad_w2 = dL_dw2(x, y, w1, w2)\n",
        "\n",
        "    # update weights\n",
        "    w1 -= lr * grad_w1\n",
        "    w2 -= lr * grad_w2\n",
        "\n",
        "    # show\n",
        "    print(f\"value {i+1}\")\n",
        "    print(f\"x = {x}, y = {y}\")\n",
        "    print(f\"Prediction (ŷ) = {y_pred:.4f}\")\n",
        "    print(f\"Loss = {loss:.4f}\")\n",
        "    print(f\"Gradient w.r.t w1 = {grad_w1:.4f}, w2 = {grad_w2:.4f}\")\n",
        "    print(f\"Updated w1 = {w1:.4f}, w2 = {w2:.4f}\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n_8OlMBC2Os"
      },
      "source": [
        "#### TODO\n",
        "- Use automatic differentiation (AD) (`tf.GradientTape`) to compute the gradients.\n",
        "- Train the model again using the same learning rate, epoch, and batch size.\n",
        "- Report the prediction $\\tilde{y}$, loss, gradients, and the updated weights at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4tufg_GC2Os",
        "outputId": "7c1a03f7-4f4f-4914-e50a-46a63cb869b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradyanlar None! Kontrol et: x=1.0, y=1.0, z=0.10000000149011612, s=0.5249791741371155, y_pred=0.05249791964888573, loss=0.8977601528167725\n"
          ]
        }
      ],
      "source": [
        "# train data\n",
        "x_data = tf.constant([1.0, 2.0, 3.0])\n",
        "y_data = tf.constant([1.0, 4.0, 9.0])\n",
        "\n",
        "# weihgts (as tf.Variable)\n",
        "w1 = tf.Variable(0.1)\n",
        "w2 = tf.Variable(0.1)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "for i in range(len(x_data)):\n",
        "    x = x_data[i]\n",
        "    y = y_data[i]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        z = w1 * x\n",
        "        s = sigmoid(z)\n",
        "        y_pred = w2 * s\n",
        "        loss = (y - y_pred) ** 2\n",
        "\n",
        "    # calculate gradients\n",
        "    gradients = tape.gradient(loss, [w1, w2])\n",
        "    grad_w1, grad_w2 = gradients\n",
        "\n",
        "    # error catch\n",
        "    if grad_w1 is None or grad_w2 is None:\n",
        "        print(f\"Gradyanlar None! Kontrol et: x={x}, y={y}, z={z}, s={s}, y_pred={y_pred}, loss={loss}\")\n",
        "        break\n",
        "\n",
        "    # weights updating\n",
        "    w1.assign_sub(learning_rate * grad_w1)\n",
        "    w2.assign_sub(learning_rate * grad_w2)\n",
        "\n",
        "    # show\n",
        "    print(f\"value {i+1}\")\n",
        "    print(f\"x = {x.numpy():.1f}, y = {y.numpy():.1f}\")\n",
        "    print(f\"Prediction (ỹ) = {y_pred.numpy():.4f}\")\n",
        "    print(f\"Loss = {loss.numpy():.4f}\")\n",
        "    print(f\"Gradient w.r.t w1 = {grad_w1.numpy():.4f}, w2 = {grad_w2.numpy():.4f}\")\n",
        "    print(f\"Updated w1 = {w1.numpy():.4f}, w2 = {w2.numpy():.4f}\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc7uRzR2C2Ot"
      },
      "source": [
        "# Subtask 2.2\n",
        "\n",
        "In this task, we consider the data from **Margarethenklippe** as the input and train an MLP that gives the information about **Sennhuette** as the output. For each of the sensors, we have the water level, **W**, and the mass flow rate, **Q**, every 15 minutes.\n",
        "## Pre-processing\n",
        "\n",
        "#### TODO\n",
        " - Load the flood dataset (`Flood_Data.csv`) using *pandas* library.\n",
        " - Drop the first four columns of the DataFrame and columns containing NaNs.\n",
        " - Split the data into inputs and outputs.\n",
        " - Shuffle the data and get the NumPy arrays.\n",
        " - Split the data to 75% for training, 15% for validation, and 15% for testing.\n",
        " - Standardize inputs and outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSsiwZRPC2Ot",
        "outputId": "fe7951a8-0df8-402a-9bde-a03377990397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "columns after dropping NaNs: [' MargarethenklippeMin15W', ' MargarethenklippeMin15Q', ' SennhuetteMin15W', ' SennhuetteMin15Q']\n"
          ]
        }
      ],
      "source": [
        "# TODO load the flood dataset using pandas\n",
        "df = pd.read_csv(\"Flood_Data.csv\", sep=\";\", engine='python')\n",
        "\n",
        "df = df.iloc[:, 4:]\n",
        "\n",
        "df = df.dropna(axis=1)\n",
        "\n",
        "print(\"columns after dropping NaNs:\", df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4wnwjboC2Ot",
        "outputId": "ccc44126-6fb4-49e5-dde8-8609d92a5d04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((514176, 2), (514176, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# TODO split the data into inputs X and outputs y\n",
        "# Inputs: Margarethenklippe data (' MargarethenklippeMin15W', ' MargarethenklippeMin15Q')\n",
        "# Outputs: Sennhuette data (' SennhuetteMin15W', ' SennhuetteMin15Q')\n",
        "\n",
        "X = df[[' MargarethenklippeMin15W', ' MargarethenklippeMin15Q']].values\n",
        "Y = df[[' SennhuetteMin15W', ' SennhuetteMin15Q']].values\n",
        "\n",
        "N_samples = X.shape[0] # number of samples\n",
        "\n",
        "X.shape, Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8B2s0-FzC2Ou"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# TODO shuffle the data and get the NumPy arrays:\n",
        "\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "Y = Y[indices]\n",
        "\n",
        "# TODO split the data to train, validation, and test:\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKh0A5tYC2Ov"
      },
      "source": [
        "Let us define a class for standardizing the data. We will implement a function that applies the scaling (`apply`) and a function that maps the scaled data back into the original scale (`apply_reverse`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ss5kz19ZC2Ov"
      },
      "outputs": [],
      "source": [
        "# TODO create a class for standardizing the data.\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.mean = np.mean(data, axis=0)\n",
        "        self.std = np.std(data, axis=0)\n",
        "\n",
        "    def apply(self, data):\n",
        "        return (data - self.mean) / (self.std + 1e-8)\n",
        "\n",
        "    def apply_reverse(self, data):\n",
        "        return data * (self.std + 1e-8) + self.mean"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uUwjl939F8gQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "71d3sRCTC2Ov"
      },
      "outputs": [],
      "source": [
        "# TODO standardize the data using the class you defined in the cell above.\n",
        "input_scaler = Standardizer()\n",
        "output_scaler = Standardizer()\n",
        "\n",
        "# Fit\n",
        "input_scaler.fit(X_train)\n",
        "output_scaler.fit(Y_train)\n",
        "\n",
        "# Apply\n",
        "X_train_scaled = input_scaler.apply(X_train)\n",
        "X_val_scaled = input_scaler.apply(X_val)\n",
        "X_test_scaled = input_scaler.apply(X_test)\n",
        "\n",
        "Y_train_scaled = output_scaler.apply(Y_train)\n",
        "Y_val_scaled = output_scaler.apply(Y_val)\n",
        "Y_test_scaled = output_scaler.apply(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGAm4Y08C2Ov"
      },
      "source": [
        "# Subtask 2.3\n",
        "\n",
        "## Implementing an MLP using TensorFlow Functional API.\n",
        "\n",
        "#### TODO\n",
        "\n",
        "- Implement an MLP using TensorFlow Functional API.\n",
        "- Print the model architecture using `model.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ysdaWpwFC2Ow"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "def MLP(N_hidden_layers, N_neurons, activation):\n",
        "    input_layer = Input(shape=(X_train_scaled.shape[1],))\n",
        "    x = input_layer\n",
        "    for i in range(N_hidden_layers):\n",
        "        x = layers.Dense(N_neurons, activation=activation)(x)\n",
        "    # Fix: Ensure the output layer has the correct number of neurons\n",
        "    output_layer = layers.Dense(Y_train_scaled.shape[1], activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Recreate the model with the corrected MLP function\n",
        "model = MLP(N_hidden_layers=2, N_neurons=64, activation='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "WyscR0D4C2Ow",
        "outputId": "68bf0f1e-5cd2-4e71-e72b-c5a51409cbbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m192\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,482\u001b[0m (17.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,482</span> (17.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,482\u001b[0m (17.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,482</span> (17.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO print the model architecture\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXR5hI6JC2Ow"
      },
      "source": [
        "# Subtask 2.4\n",
        "\n",
        "## Optimizer, loss function and training\n",
        "\n",
        "#### TODO\n",
        "\n",
        "- Define the optimizer and the loss function using `tf.keras.optimizers` and `tf.keras.losses`, respectively.\n",
        "    - Use stochastic gradient descent (SGD) with a learning rate of 1.0 as the optimizer.\n",
        "    - Use mean-squared error as the loss function.\n",
        "- Compile the model using the optimizer and the loss.\n",
        "- Train the model and get the history of training and validation losses.\n",
        "    - Train the model for 10 epochs.\n",
        "    - Use a batch size of 512.\n",
        "- Plot the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "W_WJTD1RC2Ow"
      },
      "outputs": [],
      "source": [
        "# TODO define the optimizer and the loss function\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1.0)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# TODO compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-0O5LT2C2Ow",
        "outputId": "92e4c386-a77e-48b4-d0f5-b2a9fa9a07df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 1.0771 - val_loss: 0.8792\n",
            "Epoch 2/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.0248 - val_loss: 0.8792\n",
            "Epoch 3/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 1.0248 - val_loss: 0.8792\n",
            "Epoch 4/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.0248 - val_loss: 0.8792\n",
            "Epoch 5/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.0248 - val_loss: 0.8792\n",
            "Epoch 6/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0248 - val_loss: 0.8792\n",
            "Epoch 7/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0248 - val_loss: 0.8792\n",
            "Epoch 8/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.0248 - val_loss: 0.8792\n",
            "Epoch 9/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.0248 - val_loss: 0.8792\n",
            "Epoch 10/10\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.0248 - val_loss: 0.8792\n"
          ]
        }
      ],
      "source": [
        "# TODO train the model\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled, Y_train_scaled,\n",
        "    validation_data=(X_val_scaled, Y_val_scaled),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "wjeB9_0aC2Ox",
        "outputId": "cba70745-8fd9-4b09-9b25-27e3444a15bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def plot_learning_curves(hist):\n",
        "    epochs = np.arange(0, len(hist.history['loss'])) + 1\n",
        "    sns.set(style='ticks')\n",
        "    fig, ax = plt.subplots(1, 1, figsize = (5, 4))\n",
        "    ax.plot(epochs, hist.history['loss'], label = 'Training loss', marker = 'o', ls = '--')\n",
        "    ax.plot(epochs, hist.history['val_loss'], label = 'Validation loss', marker = 'o', ls = '--')\n",
        "\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Learning curves')\n",
        "    ax.legend()\n",
        "    sns.despine(trim=True, offset=5)\n",
        "\n",
        "# TODO Use the function above to plot the learning curves\n",
        "plot_learning_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvwfO3yFC2Ox"
      },
      "source": [
        "**TODO Your answer here**\n",
        "\n",
        "Does the model learn properly? Why?\n",
        "\n",
        "**TODO Your answer here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l77EkBy4C2Ox"
      },
      "source": [
        "<span style='color:red'>**Your answer:**</span>\n",
        "The plotted learning curves show that both the training loss and validation loss remain nearly constant over all epochs.\n",
        "\n",
        "\n",
        "*   The training loss slightly decreases during the first epoch but then stays flat.\n",
        "\n",
        "*   The validation loss does not change significantly throughout training.\n",
        "\n",
        "This behavior suggests that the model is not learning effectively. The likely reasons include:\n",
        "\n",
        "*   A high learning rate (1.0), which may cause unstable weight updates, preventing convergence.\n",
        "*   A large batch size (512), which reduces the frequency of weight updates and can lead to slower learning.\n",
        "\n",
        "*  The chosen optimization settings may hinder proper training.\n",
        "\n",
        "In conclusion, with the current hyperparameters, the model fails to learn effectively. In future steps, adjusting parameters such as the learning rate and batch size may help improve the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fizDvUhQC2Ox"
      },
      "source": [
        "# Subtask 2.5\n",
        "\n",
        "## Learning rate\n",
        "\n",
        "#### TODO\n",
        "- Decrease the learning rate logarithmically, i.e. by a factor of 10, until your model starts to train.\n",
        "    - Train the model for 10 epochs.\n",
        "    - Use a batch size of 512.\n",
        "- Plot the training curves of the loss and the accuracies as in Subtask 1.3. Use the function defined above.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "epochs = 10\n",
        "learning_rates = [0.1, 0.01, 0.001, 1e-4]\n",
        "histories = {}  # hold the loss values each lr values\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nTraining with learning rate = {lr}\")\n",
        "\n",
        "    # create model again\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),\n",
        "        tf.keras.layers.Dense(20, activation='relu'),\n",
        "        tf.keras.layers.Dense(Y_train_scaled.shape[1], activation='linear')\n",
        "    ])\n",
        "\n",
        "    # compile\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "    # train\n",
        "    history = model.fit(\n",
        "        X_train_scaled, Y_train_scaled,\n",
        "        validation_data=(X_val_scaled, Y_val_scaled),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # save history\n",
        "    histories[lr] = history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOaiMP_gHxYc",
        "outputId": "1c09b947-5eb2-449a-ab96-7918856c64b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with learning rate = 0.1\n",
            "\n",
            "Training with learning rate = 0.01\n",
            "\n",
            "Training with learning rate = 0.001\n",
            "\n",
            "Training with learning rate = 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style='whitegrid')\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for lr, hist in histories.items():\n",
        "    plt.plot(hist.history['loss'], label=f'Train Loss (lr={lr})', linestyle='--')\n",
        "    plt.plot(hist.history['val_loss'], label=f'Val Loss (lr={lr})')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses for Different Learning Rates')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "gHD9KLr_IGuY",
        "outputId": "0c7b0bf7-1432-4d54-fcbc-d85d36e89b9b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAJICAYAAADxUwLTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3XlcVNX7B/DPzMCwD5uILIqAgiggoKnIKoi4oLZoaEbiiomSiuWalvlN8WtagiWZG6hhmVqCEqklqImVmKSGJqCAiiT7Ngwz9/cHv7lfRwYYRmAAn/fr5Uvn3HPOfe7MBe8z95xzOQzDMCCEEEIIIYQQBXBVHQAhhBBCCCGk66AEghBCCCGEEKIwSiAIIYQQQgghCqMEghBCCCGEEKIwSiAIIYQQQgghCqMEghBCCCGEEKIwSiAIIYQQQgghCqMEghBCCCGEEKIwSiAIIYQQQgghCqMEghAlrFy5En5+fkq1jY6Ohr29fRtH1Lnk5+fD3t4ex44d6/B929vbIzo6mn197Ngx2NvbIz8/v8W2fn5+WLlyZZvG8zznCmm93NxczJ49G0OGDIG9vT3OnDmj6pBkpKenw97eHunp6TLlJ06cwNixYzFo0CAMHTqULf/qq6/g7+8PBwcHTJ48uaPDJc9Q5e82QjoTNVUHQEhbUvTCPC4uDsOHD2/naEhzNm7ciPj4eKSkpMDKykpune3bt2PXrl34/vvvMWDAgA6OUHGFhYX45ptvMHr0aDg4OKg6HAANFzr+/v547733MGfOHFWH02FWrlyJ/Px8LF26FHp6enB0dGy3fUnfYyk1NTXo6urC2toaw4YNw7Rp02Bubt5iP3fv3sWqVavg5eWF+fPnQ1NTEwBw4cIF/Pe//8WkSZOwePFiGBoattuxPK+rV6/i4sWLmDlzJgQCQYv1V65ciR9//BEZGRkdEF33kJ6ejrfeeot9zeVyYWBggJdeegnvvPMObG1tlep3165d6NevH0aPHt1WoZIXACUQpFvZsmWLzOvvv/8eFy9ebFSu7C9aqY8++ggMwyjV9u2338b8+fOfa//dwcSJExEfH4+TJ09i0aJFcuskJibCzs7uuZKHyZMnY8KECeDz+Ur30ZLHjx8jJiYGFhYWjRKI5zlXSOvU1tYiIyMDCxYswJtvvtlh+w0KCoK3tzcYhkFZWRkyMzNx4MABxMXF4T//+Q8mTJjA1n3ppZdw/fp1qKurs2VXrlyBRCLBmjVrZJLpy5cvg8vl4j//+U+7nr9tISMjAzExMXjllVcUSiC6KgsLC1y/fh1qaqq7fAoJCYGTkxPq6+uRlZWFhIQEpKenIzExESYmJq3uLzY2FoGBgZRAkFahBIJ0K8/e4v/zzz9x8eLFFm/919TUQEtLS+H9PP2ff2upqamp9D+fzmLw4MGwsrJCUlKS3AQiIyMD+fn5iIyMfK798Hg88Hi85+rjeTzPuUJap7i4GADa9AK2uroa2trazdYZOHBgo98xBQUFmD17NlasWAFbW1s2CeZyudDQ0JCp++TJEwCAnp5eo3JNTc02TR5a+7uuu1Pk830ah8Np9Pl1tKFDh2Ls2LHsa2tra3zwwQc4ceIE5s2bp8LIyIuE5kCQF05ISAiCgoLw119/YcaMGRg8eDC2bdsGADhz5gzmz58PT09PODo6YvTo0di5cyfEYrFMH8+Oa5eOi92zZw+OHDmC0aNHw9HREa+99hquX78u01beHAh7e3ts2LABZ86cQVBQEBwdHTFhwgSkpqY2ij89PR2vvvoqnJycMHr0aCQkJCg8r+L3339HREQEfH194ejoCB8fH3z88ceora1tdHyurq4oLCzEwoUL4erqihEjRiAqKqrRe1FeXo6VK1diyJAhGDp0KFasWIGKiooWYwEa7kJkZ2fjxo0bjbYlJiaCw+EgKCgIdXV1+Oyzz/Dqq69iyJAhcHFxwRtvvIHLly+3uA95cyAYhsHnn38Ob29vDB48GCEhIbhz506jtqWlpYiKisLEiRPh6uoKNzc3zJ07F3///TdbJz09HVOmTAEArFq1Cvb29jJjpOXNgaiursbmzZvh4+MDR0dHBAYGYs+ePY3uVLTmvFDWkydPsHr1aowcORJOTk6YNGkSjh8/3qheUlISXn31VfZ9mDhxIg4cOMBuF4lEiImJwZgxY+Dk5IThw4dj+vTpuHjxokw/d+/eRUREBIYNGwYnJye8+uqrOHv2rEwdRft6WnR0NEaNGgWg4U6kvb29zPt+8+ZNzJ07F25ubnB1dcXMmTNx7do1mT6k58qVK1fwwQcfwN3dHT4+Pgq/l0+zsLDA5s2bIRKJsHv3brb82TkQfn5+7Jwdd3d3dg6P9Byqrq5udE4BDXdXX331VTg7O2PYsGFYunQpHj58KBNDc7/r6urqsGPHDgQEBLC/C7Zs2YK6ujqZPhQ5B6Ojo9m7vP7+/my8isw7asmff/6JOXPmYMiQIRg8eDDefPNN/PHHHzJ1CgoK8MEHHyAwMBDOzs4YPnw4IiIiGu2/uc9X+l79888/CAkJweDBg+Hl5SXz2QHy50C05vdlSUkJ3n33Xbi5ubG/L//+++/nmlchnTOTl5cnU75nzx5MmzYNw4cPh7OzM1599VUkJyfL1LG3t0d1dTWOHz/Ofm5PzwMrLCzEqlWrMHLkSPazP3r0aKMY4uPjMWHCBAwePBgvvfQSXn31VZw8eVKp4yFdA30NSl5IpaWlmDdvHiZMmIBJkybB2NgYAHD8+HFoa2tj1qxZ0NbWxuXLl7Fjxw5UVlZixYoVLfabmJiIqqoqBAcHg8Ph4KuvvsLixYtx5syZFr+J/uOPP5CSkoI33ngDOjo6iI+PR0REBH7++Wd27LP0IsjExASLFy+GRCLBzp07YWRkpNBxJycno7a2FtOnT4eBgQGuX7+OgwcP4tGjR9ixY4dMXbFYjDlz5sDZ2Rnvvfcefv31V+zduxe9e/fGG2+8AaDhQnzhwoX4448/MG3aNNja2uKnn35S6L0CGhKImJgYJCYmYtCgQTL7Pn36NIYOHQpzc3MUFxfj22+/RVBQEKZOnYqqqiocPXoUc+fOxbffftvqeQefffYZvvjiC/j4+MDHxwc3btzA7NmzIRKJZOrl5eXhzJkzGDt2LCwtLfHvv//iyJEjePPNN5GUlARTU1PY2toiIiICO3bsQHBwMIYMGQIAcHNzk7tvhmHw9ttvs4mHg4MD0tLSsGXLFhQWFmL16tUy9RU5L5RVW1uLkJAQ3L9/HzNmzIClpSWSk5OxcuVKlJeXY+bMmQCAixcvYtmyZXB3d8fy5csBANnZ2bh69SpbJyYmBrGxsZg6dSqcnZ1RWVmJv/76Czdu3ICHhwcA4M6dO5g+fTpMTU0xb948aGtr4/Tp0wgPD0d0dDQCAgIU7utZAQEB0NPTw6ZNm9ghRTo6Oux+Z8yYAR0dHcydOxdqamo4cuQIQkJCcPDgQQwePFimrw8//BBGRkYIDw9HdXW10u+vq6sr+vTpg0uXLjVZZ/Xq1Thx4gR++uknfPDBB9DW1oa9vT369OmDb775BtevX8fGjRsB/O+c+uKLL/DZZ59h3LhxmDJlCoqLi3Hw4EHMmDEDJ06ckLkDI+93nUQiwdtvv40//vgDr7/+OmxtbXH79m0cOHAAubm5+Pzzz2VibOkcDAgIQG5uLhITE7Fq1Sr2vFT091JTfv31V8ybNw+Ojo5YtGgROBwOjh07hpkzZ+Lw4cNwdnYGAGRmZiIjIwMTJkxAr169UFBQgK+//hpvvfUWkpKSGt1xaerzLSsrw9y5cxEQEIBx48bhxx9/xNatW2FnZ9diIqnI70vp+379+nVMnz4dNjY2OHv2rMK/L5tSUFAAoPGdt7i4OPj5+WHixIkQiURISkrCO++8g9jYWPj6+gJoSLbXrl0LZ2dnvP766wCAPn36AAD+/fdfvP766+BwOJgxYwaMjIyQmpqKNWvWoLKyEqGhoQCAb775Bhs3bkRgYCDeeustCIVCZGVl4c8//8TEiROf69hIJ8YQ0o19+OGHjJ2dnUzZm2++ydjZ2TFff/11o/o1NTWNyt5//31m8ODBjFAoZMtWrFjBjBo1in2dl5fH2NnZMcOGDWNKS0vZ8jNnzjB2dnbMuXPn2LIdO3Y0isnOzo4ZNGgQc+/ePbbs1q1bjJ2dHRMfH8+WhYWFMYMHD2YePXrEluXm5jIDBw5s1Kc88o4vNjaWsbe3ZwoKCmSOz87OjomJiZGp+/LLLzOvvPIK+/qnn35i7OzsmN27d7Nl9fX1zBtvvMHY2dkx3333XYsxvfbaa4y3tzcjFovZstTUVMbOzo5JSEhg+3z6/WcYhikrK2NGjhzJrFq1Sqbczs6O2bFjB/v6u+++Y+zs7Ji8vDyGYRjmyZMnzKBBg5j58+czEomErbdt2zbGzs6OWbFiBVsmFApl4mKYhs/a0dFR5r25fv16k8f77Lkifc8+//xzmXqLFy9m7O3tZc4BRc8LeaTn5FdffdVknf379zN2dnbM999/z5bV1dUxwcHBjIuLC1NRUcEwDMNs3LiRcXNzY+rr65vsa9KkScz8+fObjWnmzJlMUFCQzGcpkUiY4OBgZsyYMa3qS56mjnnhwoXMoEGDmPv377NlhYWFjKurKzNjxgy2THquTJ8+vdljbWl/T3v77bcZOzs79r28fPkyY2dnx1y+fJmtI/2d8OTJE5m2K1asYFxcXGTK8vPzGQcHB+aLL76QKc/KymIGDhwoU97U77oTJ04wAwYMYH777TeZ8q+//pqxs7Nj/vjjD7ZM0XPwq6++kvk5a4m8Y3uaRCJhxowZw8yePVvm57Smpobx8/NjZs2aJVP2rIyMDMbOzo45fvw4W9bc5yt9r56uLxQKGQ8PD2bx4sVsmfQzf/pnXdHflz/++CNjZ2fH7N+/ny0Ti8XMW2+9pdDvS+m5c/ToUebJkydMYWEhk5qaygQEBDD29vbMn3/+KVP/2felrq6OCQoKYt566y2ZchcXF5nfe1KrV69mPDw8mOLiYpnypUuXMkOGDGH7f/vtt5kJEyY0GzvpfmgIE3kh8fl8vPrqq43KpaufAEBlZSWKi4sxdOhQ1NTUIDs7u8V+x48fD319ffZ1U7eW5Rk5ciT7zQ8ADBgwALq6umxbsViMX3/9Ff7+/jA1NWXrWVlZwcvLq8X+Adnjq66uRnFxMVxdXcEwDG7evNmo/vTp02VeDxkyRGZYQGpqKtTU1GTq8Xi8Vk1gnTRpEh49eoTffvuNLUtMTIS6ujo7zpfH47HjwCUSCUpLS1FfXw9HR0e5cTfn0qVLEIlEePPNN8HhcNhy6TfpT+Pz+eByG35NisVilJSUQFtbG9bW1q3er1Rqaip4PB5CQkJkymfPng2GYRoNT2rpvHgeqampMDExQVBQEFumrq6OkJAQVFdXs5+JQCBATU1Ns0OIBAIB7ty5g9zcXLnbS0tLcfnyZYwbN4792SouLkZJSQk8PT2Rm5uLwsJChfpqDbFYjIsXL2L06NHo3bs3W96zZ08EBQXhjz/+QGVlpUyb119/vc3mzUjH11dVVbVJfz/99BMkEgnGjRvHvofFxcXo0aMHrKysGi0PK+93XXJyMmxtbWFjYyPTx4gRIwCgUR/teQ425datW8jNzcXEiRNRUlLCxlhdXQ13d3f89ttvkEgkAGR/r4lEIpSUlKBPnz4QCARyf06b+ny1tbVl5rLw+Xw4OTkpfJwt/b5MS0uDuro6+00/0DAnZsaMGQr1L7V69Wq4u7vDy8sLc+fORUVFBbZs2cLekZF6+n0pKytDRUUFhgwZotDvLoZhkJKSAj8/PzAMI3OeeHp6oqKigh16KhAI8OjRo0bDdUn3RkOYyAvJ1NRU7sTEO3fu4NNPP8Xly5cbXVQoMq7fzMxM5rU0mSgvL291W2l7adsnT56gtrZW7pKnTS2D+qwHDx5gx44dOHfuHMrKymS2PXu8GhoajYYg6Ovry7QrKCiAiYkJO1REytraWqF4AGDChAnYvHkzEhMTMXz4cAiFQvz000/w9vaWScaOHz+OvXv3IicnR2aokaWlpcL7AhreAwDo27evTLmRkZHM/oCGZCUuLg6HDx9Gfn6+zHhmAwODVu1XqqCgAD179oSurq5MuXRlMOlwBKmWzovnUVBQACsrKzZJejYW6Xv1xhtv4PTp05g3bx5MTU3h4eGBcePGwdvbm20TERGBhQsXIjAwEHZ2dvD09MTkyZPZycP3798HwzD47LPP8Nlnn8mN58mTJzA1NW2xr9YoLi5GTU2N3HPS1tYWEokEDx8+RP/+/dny1p5TzZEOkXn2Z0RZubm5YBgGY8aMkbv92QUa5P2uu3fvHu7evQt3d3e5fUgndUu15znYFGny2NzwnoqKCujr66O2thaxsbE4duwYCgsLZeYSyfu93dTn26tXL5kvFYCG48zKymoxXkV+Xz548AAmJiaNhlQ9nZwpIjw8HEOHDkV1dTV++uknJCUlNfoZBoCff/4ZX3zxBW7duiUzt+XZY5SnuLgY5eXlOHLkCI4cOdJkHQCYN28eLl26hKlTp8LKygoeHh4ICgpih3OS7okSCPJCevqbGany8nK8+eab0NXVRUREBPr06QMNDQ3cuHEDW7duZb/tak5T31oyCizj+TxtFSEWizFr1ix2nK+NjQ20tbVRWFiIlStXNjq+jlq5yNjYGCNHjkRKSgrWrVuHc+fOoaqqSmbs7Pfff4+VK1di9OjRmDNnDoyNjcHj8RAbG9uu34Lu2rULn332GV577TW888470NfXB5fLxccff9xhS7O293mhCGNjY5w4cQIXLlxAamoqUlNTcezYMbz88suIiooC0LA86U8//YSzZ8/i4sWLOHr0KA4cOIAPP/wQU6dOZc+v2bNnN3nHTHoh1VJf7a0tV9m5c+cOjI2NGyWMypJIJOBwONi9e3eT36I/Td7vOolEAjs7O6xatUruPnr16iXzWhXnoLTv9957r8k5TtJj/eijj9i5ES4uLtDT0wOHw8HSpUvlxtjU5/s8v/M6cqU3Ozs7jBw5EgAwevRo1NTU4P3338eQIUPYZO/333/H22+/jZdeegnr16+HiYkJ1NXV8d133yExMbHFfUh/XidNmoRXXnlFbh3pwh22trZITk7GL7/8grS0NKSkpODw4cMIDw9HREREWxwy6YQogSDk/125cgWlpaWIiYnBSy+9xJa3xUoibcHY2BgaGhq4d+9eo23yyp51+/Zt5ObmIioqCi+//DJb3tywlJZYWFjg8uXLqKqqkvmGNScnp1X9TJw4EWlpaUhNTUViYiJ0dXVlVtD58ccf0bt3b8TExMh8e/bsxG9FSB/slZubKzOkpbi4uNFdmR9//BHDhw/Hxx9/LFNeXl4uM4FZkW/0pCwsLPDrr7+isrJS5qJSOkTOwsJC8YN5ThYWFsjKyoJEIpH5BlMay9MPQePz+fDz84Ofnx8kEgk++OADHDlyBAsXLmTvgBkYGOC1117Da6+9hqqqKrz55puIjo7G1KlT2fdaXV2dvfhpTnN9tYaRkRG0tLTknpPZ2dngcrlyv2FvCxkZGbh//z4mTZrUZn326dMHDMPA0tKyVXf6nu3j77//hru7e6vO3ea0VT9S0vNFV1e3xfPlxx9/xMsvvyyzepBQKFR4NbiOYm5ujvT09EZL6d6/f/+5+l2+fDnOnDmDL774Ahs2bADQ8J5oaGhgz549MnegvvvuO4X6NDIygo6ODiQSiUI/r9ra2hg/fjzGjx+Puro6LF68GLt27UJYWJjKl70l7YPmQBDy/6QXUE9/Y1VXV4fDhw+rKiQZPB4PI0eOxNmzZ9mx4kBD8pCWltZie3nHxzAM4uLilI7J29sb9fX1+Prrr9kysViMgwcPtqqf0aNHQ0tLC4cPH0ZqairGjBkj85+O9Nu9p2P/888/Gy3DqYiRI0dCXV0dBw8elOnv6SVJn97vs99gnj59Wub9B8BeDCgypMPb2xtisRiHDh2SKd+/fz84HI7MsKD25u3tjaKiIpw6dYotq6+vR3x8PLS1tdlEuqSkRKYdl8tlv32UDo14to6Ojg769OnDbjc2NsawYcNw5MgRPH78uFEs0uEQivTVGjweDx4eHjh79qzMlwH//vsvEhMTMWTIkDa7O/C0goICrFy5Eurq6m36JPAxY8aAx+MhJiam0bnJMEyj906ecePGsU9Pf1Ztba1SK09Jfwba6qLd0dERffr0wd69e+XOH3n6fJH37X98fHyjJVRVzdPTEyKRSOZ9l0gkjX4XtFafPn0wZswYHD9+HEVFRQAa3hMOhyPzHuTn5zdaMhlouPh/9ncXj8dDYGAgfvzxR9y+fbtRm+Z+Xvl8PmxtbcEwTKOV7Uj3QXcgCPl/rq6u0NfXx8qVKxESEgIOh4Pvv/++Uz1FeNGiRbhw4QKmT5+O6dOnQyKR4ODBg+jfvz9u3brVbFsbGxv06dMHUVFRKCwshK6uLn788cfnGsfs5+cHNzc3fPLJJygoKEC/fv2QkpLS6osIHR0d+Pv7s7fWn136z9fXFykpKQgPD4evry/y8/ORkJCAfv36tfpix8jICLNnz0ZsbCzCwsLg4+ODmzdvIjU1tdGyqL6+vti5cydWrVoFV1dX3L59GydPnpS5cwGAnbCZkJAAHR0daGtrw9nZuVE9oOE9Gz58OLZv346CggLY29vj4sWLOHv2LGbOnNnq8dAt+fXXXyEUChuVjx49GsHBwThy5AhWrlyJGzduwMLCAj/++COuXr2K1atXsxfWa9euRVlZGUaMGAFTU1M8ePAABw8ehIODAztfYsKECRg2bBgGDRoEAwMDZGZm4scff5SZUL9+/Xq88cYbmDhxIl5//XX07t0b//77L65du4ZHjx7hhx9+ULiv1liyZAkuXbqEN954A2+88QZ4PB6OHDmCuro6vPvuu0r1+bSbN2+yvyvKy8uRmZmJlJQUcDgcbNmy5bmepP6sPn36YMmSJezP3OjRo6Gjo4P8/HycOXMGr7/+eosJy+TJk3H69GmsX78e6enpcHNzg1gsRnZ2NpKTk/HVV1/BycmpVXFJl2Hevn07xo8fD3V1dYwaNarZh7SJRKJGS8YCDXMHZsyYgY0bN2LevHkICgrCq6++ClNTUxQWFiI9PR26urrYtWsXgIaf0++//x66urro168frl27hkuXLik9T6m9jB49Gs7OzoiKisL9+/dhY2MjMx/tee7izJkzB6dPn8aBAwewfPly+Pj4YN++fZg7dy6CgoLw5MkTHD58GH369Gk0p2PQoEH49ddfsW/fPvTs2ROWlpYYPHgwIiMjkZ6ejtdffx1Tp05Fv379UFZWhhs3buDXX3/FlStX2H336NEDbm5uMDY2RnZ2Ng4ePAgfH592Sc5J50AJBCH/z9DQELt27UJUVBQ+/fRTCAQCTJo0Ce7u7m36DeLzcHR0xO7du7FlyxZ89tlnMDMzQ0REBLKzs1tcJUpdXR27du3Cxo0bERsbCw0NDQQEBGDGjBktPqm7KVwuF1988QU+/vhj/PDDD+BwOPDz88PKlStlhkkpYtKkSUhMTISJiQm7GozUq6++yj6D4cKFC+jXrx/++9//Ijk5mf1PrDWWLFkCPp+PhIQEpKenw9nZGXv37kVYWJhMvQULFqCmpgYnT57EqVOnMHDgQMTGxuKTTz6Rqaeuro7Nmzdj27Zt+OCDD1BfX49NmzbJTSCk79mOHTtw6tQpHDt2DBYWFnjvvfcwe/bsVh9LS9LS0uTeobKwsICdnR3i4+OxdetWHD9+HJWVlbC2tsamTZtkVu6ZNGkSvvnmGxw+fBjl5eUwMTHBuHHjsHjxYvbOVkhICM6dO4eLFy+irq4O5ubmWLJkiczPTr9+/fDdd98hJiYGx48fR2lpKYyMjDBw4ECEh4ez9RTpqzX69++PQ4cO4ZNPPkFsbCwYhoGzszP++9//NnoGhDISExORmJgINTU16OrqwsrKCjNnzsS0adNkhoG1lfnz56Nv377Yv38/du7cCaBh3oKHh0ejhxbKw+VysXPnTuzfvx/ff/89fvrpJ2hpacHS0hIhISFKDY1ydnbGO++8g4SEBKSlpUEikeDs2bMtJhDyJtT36dMHM2bMwPDhw3HkyBF8/vnnOHjwIKqrq2FiYgJnZ2cEBwez9desWQMul4uTJ09CKBTCzc2NvXjuTKTztv7zn//g+PHj4HK5CAgIQHh4OKZPn/5cQ32cnJwwbNgwfP311wgLC4O7uzv+85//YPfu3fj4449haWmJ5cuXo6CgoFECsXLlSqxbtw6ffvopamtr8corr2Dw4MHo0aMHvv32W+zcuRM//fQTvv76axgYGKBfv37s82AAIDg4GCdPnsS+fftQXV2NXr16ISQkBAsXLlT6eEjnx2E609erhBClLFy4EP/88w9SUlJUHQohhJBWOHPmDMLDw3H48GFauYh0GTQHgpAupra2VuZ1bm4uUlNTMWzYMBVFRAghRBHP/v4Wi8WIj4+Hrq4uOwyMkK6AhjAR0sWMHj0ar7zyCnr37o2CggIkJCRAXV29092uJ4QQIuujjz5CbW0tXF1dUVdXh5SUFGRkZGDZsmVyl9wlpLOiBIKQLsbLywtJSUkoKioCn8+Hi4sLli1b1ujBaIQQQjqXESNGYN++ffjll18gFAphZWWF999/X+kFAghRFZoDQQghhBBCCFEYzYEghBBCCCGEKIwSCEIIIYQQQojCaA5EEzIyMsAwDNTV1VUdCiGEEEIIIe1KJBKBw+HA1dW1xbp0B6IJDMOo9AnEDMOgrq6uUz0FmXR+dN4QZdB5Q5RB5w1RBp03nVdrrn3pDkQTpHcenJycVLL/6upq3Lp1C/369Wv2SZ6EPI3OG6IMOm+IMui8Icqg86bzyszMVLgu3YEghBBCCCGEKIwSCEIIIYQQQojCKIEghBBCCCGEKIwSCEIIIYQQQojCKIEghBBCCCGEKIxWYSKEEEKISojFYohEIlWHQTqQUChk/+Zy6XvsjqKurg4ej9dm/VECQQghhJAOxTAMHj16hNLSUlWHQjqYRCKBmpoaHjx4QAlEBzMwMECvXr3A4XCeuy9KIAghhBDSoaTJQ8+ePaGtrd0mFzSkaxCLxRAKhdDQ0GjTb8RJ0xiGQXV1NR4/fgwAMDMze+4+KYEghBBCSIcRi8Vs8mBsbKzqcEgHE4vFAABNTU1KIDqQlpYWAODx48fo2bPnc7/3dO+IEEIIIR1GOueBnkJMSMeS/sy1xbwjSiAIIYQQ0uFo2BIhHastf+YogSCEEEIIIYQojBIIQgghhBBCiMIogSCEEEIIaSV7e/sW/xw7dkzp/kNCQhAWFtYmsfr5+WHDhg1t0ld7iIiIQFRUFPt65cqVCAoKarf91dXVISoqCh4eHnBxccGsWbOQnZ3dYrvMzEysWrUK48aNw4ABA+R+Pvn5+XBxcUF+fn57hN5pdKpVmO7du4c9e/bgzz//xJ07d2BjY4PExMQW2zEMg927d+Pw4cMoLi6Gg4MDVq1aBRcXl/YPmhBCCCEvnCNHjsi8Dg4ORkhIiMyFb58+fZTuf/369S/EcxJu3LiBn3/+GWfOnOmwfW7cuBGnTp3CypUrYWpqil27diE0NBRJSUnQ09Nrst3Vq1fx+++/w9nZmX0g3rMsLS0RGBiI6OhomaSou+lUCcSdO3dw/vx5DB48GBKJBAzDKNRu9+7d2LFjB5YvXw57e3scOnQIs2fPxvfff4/evXu3c9SEEEIIedHI+5LSzMys2S8va2troampqVD//fr1UzKyriUuLg6enp4wNTVtVbvWvJdPe/ToEY4ePYr169djypQpAAAnJyeMGjUKCQkJmDdvXpNtQ0JCMHPmTPbfTZkyZQpmzZqFFStWwMjIqNUxdgWdKrX18/PD+fPnsWPHDgwaNEihNkKhELGxsZg9ezZCQ0Ph7u6Obdu2wcDAAHv27GnniAkhhBBCGouOjoarqyuuX7+O4OBgODk54dChQwCArVu3YuLEiXB1dYWXlxeWLVvGPuRL6tkhTNL+srKyMH36dAwePBhBQUFIS0trk3gTEhIQGBgIR0dH+Pn54fPPP4dEImG3l5eXY+3atfDy8oKTkxN8fHywdOlShbfLU11djZSUFAQGBjZb79ixY7C3t0dGRgZmzZoFFxcXbNmyRanjvHDhAiQSCcaOHcuWGRgYwMPDA6mpqc22VfSO0JAhQ2BgYICTJ08qFWNX0KnuQChzq+7q1auorKzEuHHj2DI+n4+AgAD89NNPbRkeIYQQQtpRrbC+yW1cLgd8dZ5CdTlcDjSUrNuWRCIRIiMjERoaiqVLl8LAwAAA8OTJE4SFhaFnz54oLi7Gvn37EBISgqSkJKipNX1pJhKJsHz5crz11ltYuHAhdu/ejYiICJw7dw6GhoZKxxkfH4+NGzciJCQEvr6+yMjIQExMDCoqKrBixQoAwKZNm5CWlobIyEhYWFigqKhI5oK7pe3yXLt2DdXV1RgyZIhCcUZGRiI4OBhhYWHsg9HEYnGLI1Y4HA774LTs7GwYGxtDX19fpo6trS2OHj2qUBwt4XK5GDx4MC5dusTesehuOlUCoQzppBcbGxuZcltbWxw4cEDpW1yEEEII6VhTVyc1uW2ogynWzx3Bvn7zg2QI68Ry6zraGmPTQk/29Zz//ITyqjq5dfv1NsD2JT5KRtw8kUiEpUuXYvz48TLlmzZtYv8tFovh6uoKb29vXL58GZ6ens92I9Pf8uXL4ePTEK+1tTX8/f2RmpqKyZMnKxWjWCzGzp07MWHCBKxduxYA4OnpCZFIhL1792L+/PkwNDREZmYmgoKC8Morr7BtJ0yYwP67pe3yZGZmQltbW+Hh5tOmTcP8+fNlykJDQ3HlypVm2w0bNgzx8fEAGu6UyJvnIBAIUFZWplAcihgwYAB7x6k76vIJRHl5Ofh8PjQ0NGTKBQIBGIZBWVmZ0gkEwzCorq5uizBbraamRuZvQhRB5w1RBp03RBnKnjdCoRASiQRisRhisfwEQB6GYWTrN/Olc6O6zXfcqjiaIz0u6b+BhovxZ/tPTU3Frl278M8//6CyspItz87Ohru7+/+Hxcgch0QiAZfLxbBhw9gyMzMzaGpq4uHDh80ew7N9Pe3OnTsoKSnBmDFjZLYHBgYiNjYW165dg7e3NxwcHHDs2DEYGxvDy8sL/fv3l+mnpe1PxyL9u7CwEIaGho3iknfsAODt7d2o7vr161FVVdXksQOAjo4O2066/2f7kc69VfRcaO49BQB9fX2UlJSgtrYW6urqCvXZ3sRiMSQSCWpqamSGp0kxDKPww+a6fALRnkQiEW7duqWSfXMrJNC6X497whwwGvS0TtI6ubm5qg6BdEF03hBlKHPeqKmpyV3FZv/aUU224XIaJs5Kxa7wVrjujqUeCtd9HvX19Wxf9fX10NTUBI/Hk+n/xo0bWLRoEXx8fDBz5kwYGhqCw+Fg5syZqKqqYutKk5Gn+9PQ0IBEIpHpT01NTaadPAzDyMT2tH///RcAoKenJ7NdV1eX3V5bW4vIyEjo6Ohg//792Lp1K3r16oVZs2Zh6tSpANDi9mcJhUJUV1dDXV29UVzSYUnScpFIxMb0bN2ePXsqNIRJ2k5bWxsVFRWN+ikuLoa+vr7C58Kzn4+8fQINX3Tr6Ogo1Gd7EwqFqK+vb3bJWj6fr1BfXT6BEAgEqKurg1AolLkLUV5eDg6H02iMW2uoq6urbBWEwqO3UP9YDL2eBjD2tlJJDKTrqampQW5uLvr27cuODyWkJXTeEGUoe94IhUI8ePAAGhoajUYItGbAQHvVfR5qamrsMampqYHD4TQ6xtTUVOjq6uKzzz5j534WFBQ0as/lcsHj8WT6A9CoPw6HI9NOnubqmJiYAAAqKytltkvvjPTo0QOamprQ1NTE+++/j/fffx+3b99GfHw8Nm3aBAcHBwwdOrTF7VIMw7DXbMbGxqioqGgUF4/Hk3nvpN/gS+N42syZM/Hbb781eewA8NJLL+HAgQMAgP79++PJkycQCoUy14h5eXmwsbFReNTKs5/Ps2pqaqCurg5jY2OF+usoampq6NOnT6OROwDwzz//KN5PWwalCtK5Dzk5ORgwYABbnp2dDXNz8+ea/8DhcKCtrf3cMSpDMNQcxafuQphVAg3f/uBpdvmPinQgLS0tlZ27pOui84Yoo7XnDZfLZS++pBNbuwvpcUn//fTkXam6ujqoq6uzCQYAnDp1qlF7Docj076p/p5tJ8+zfT2tX79+MDIyarQaUkpKCtTV1eHi4tKonYODA1avXo3vvvsOubm5GD58uMLbpUN+OBwObGxsUFJSAqFQKHMOyTv2po5zw4YNCg1hkrbz9vYGl8vF2bNn2bsjZWVluHjxIhYuXKjwOdncewoADx8+hLW1dac6x3k8HrhcLrS0tOReHys6fAnoBgmEm5sbdHV1cfr0aTaBEIlESElJgbd307c3OztNKwHEOhzwqiQozXgAY3flH0ZDCCGEkM7Bw8MDBw4cwEcffYSAgABkZGTg+++/b/f93r9/H8nJyTJlXC4XY8aMwcKFC7Fx40YYGRnBx8cH165dw+7du9khVkDDBOaAgAD0798fPB4PJ06cgLq6Ont3oaXt8ri5uUEikeDmzZvN1mvOs4votKRXr16YMmUKtmzZAi6XC1NTU8TGxkJPTw/Tpk1j6504cQKrV6/G/v37MWzYMAANw5ykE7aLi4tRVVXFvqc+Pj4yd+L++usvhVeX6oo6VQJRU1OD8+fPA2i4nVdZWcl+MMOGDYORkRFmzpyJBw8esEu0amhoICwsDNHR0TAyMoKdnR2+/vprlJaWYs6cOSo7lufF4XAgtFKD9k0RSv94AMMhFuDyO08WSwghhJDW8/HxwfLly3Hw4EEcO3YMbm5uiI2NbfFZCM8rLS2t0TMjeDwebt68iZCQEKipqWH//v34+uuvYWJigkWLFmHBggVsXTc3N5w4cQL5+fngcrmws7PDrl27YGtrq9B2eaytrWFnZ4e0tDSlEwhlrF27Fjo6Ovjkk09QVVUFNzc37Nu3T2Z1Jukch6fnV9y5cwfvvPOOTF/S12fPnoWlpSWAhmV6b9y4gWXLlnXA0agGh1H0cc8dID8/H/7+/nK3xcXFYfjw4QgJCUFBQQHOnTvHbmMYBl9++SUOHz6M4uJiODg4YNWqVXB1dVU6lszMTAANTydUherqaty6eROGGQzEZUL08LWG4VALlcRCuo7q6mrcunULDg4ONBSFKIzOG6IMZc+b2tpa5OTkwNrampZZfwFJJx5LJ5jHx8cjLi4OKSkprRpC05kdOnQI+/fv73TH1NLPXmuufTvVHQhLS0tkZWU1W0e6ju/TOBwOwsLCZJ7Y2C1wONBz7YnSX+6j5PcC6LuYgavWqR4eTgghhBCitKlTp+LLL7/EuXPnmvwSuSuRSCSIi4tDeHh4p0oe2hpdjXZy2vZG0OilC8Mh5qoOhRBCCCGkTWlqamLz5s3sUq1d3ePHj/HKK69g0qRJqg6lXXWqOxCkMQ6Pi94zBnfrLJYQQgghLy4Pj6af09HV9OrVS2buSHdFdyC6AEoeCCGEEEJIZ0EJRBfBSBhU/F2Eh4lZLT5xkRBCCCGEkPZCCUQXIakT43HKP6j8uwhV2cWqDocQQgghhLygKIHoIniaatB3MQMAlFzOp7sQhBBCCCFEJSiB6EIMhpiDo8ZF7cMK1OSVqTocQgghhBDyAqIEogtR0+FD4GgKAChOz1dxNIQQQggh5EVECUQXY/iSBcDloOZeKWofVqg6HEIIIeSFtGDBAowZM6bJ7fHx8bC3t8f9+/cV6s/e3h579uxptk5ISEinfmhuVFQUIiIi2NfR0dFwdXVtt/0xDIMvv/wSvr6+cHZ2RnBwMK5du9Ziu3v37mHdunWYPHkyBg4ciKCgoEZ1KisrMWzYMPzxxx/tEHnXRwlEF6Ourwm9ASYAgOL0PBVHQwghhLyYgoKCcO/ePVy/fl3u9qSkJLi4uKBPnz4dHJlqFBYW4vDhw5g/f36H7XP37t3YsWMHQkNDERsbCxMTE8yePRt5ec1fH925cwfnz5+HlZUVbG1t5dbR1dXFm2++ie3bt7dH6F0eJRBdkNFwS2ia60Hg1EvVoRBCCCEvJH9/f2hrayMxMbHRtvz8fGRkZMj9Zru7OnLkCKysrODo6NiqdrW1tUrtTygUIjY2FrNnz0ZoaCjc3d2xbds2GBgYtHgnx8/PD+fPn8eOHTswaNCgJuu99tpr+O233/D3338rFWN3RglEF8Q31kbvNwZD19ZI1aEQQgghLyQtLS34+/vj9OnTkEgkMtuSkpLA4/Ewfvx4PH78GKtWrYK/vz+cnZ0xZswYbNu2DXV1de0SV0pKCiZPngwnJyd4enpi06ZNEAqF7HaRSISoqCj4+vrC0dERnp6eWLBgASoqKhTa3pQTJ04gMDCw2Trp6ekYOHAg0tLSsGTJEri5ueGdd95R6jivXr2KyspKjBs3ji3j8/kICAhAampqs225XMUufy0sLODs7Ixjx44pFWN3pqbqAAghhBBCGIYBIxK2XLGdcNQ1wOFwWtVm4sSJOHnyJNLT0+Hu7s6WJyYmYuTIkTA2NkZWVhYMDAywatUqCAQC5ObmIjo6GkVFRdi0aVObHsPZs2cRERGBCRMmIDIyEtnZ2di+fTsePnyIHTt2AABiY2ORkJCA5cuXo3///igpKcHFixfZhKal7fLcu3cPBQUFcHNzUyjOjRs3YtKkSdi5cyd7MS+RSBolYs/icDjg8XgAgOzsbACAjY2NTB1bW1scOHAAtbW10NTUVCie5ri6uuLSpUvP3U93QwlEFyaurUdpxgMw9RL08Oqr6nAIIYQQpTAMgwdxayDMz1JZDBqWA2D+1sZWJREeHh4wMjJCUlISm0Dcvn0bt2/fxpw5cwA0TI5esWIF28bNzQ1aWlpYuXIl1q1bBy0trTY7hpiYGLi4uOCTTz4BAHh7e0NLSwvr1q1DVlYW7O3tkZmZCU9PT8yYMYNt9/Sdg5a2y5OZmQmg4VgV4ePjg8jISDYZAIDVq1fj+PHjzbazsLDAuXPnAADl5eXg8/nQ0NCQqSMQCMAwDMrKytokgRgwYADi4uJQWVkJXV3d5+6vu6AEogsTFlWh+OJ9cHgcGLiaQ02Xr+qQCCGEECW17tv/zkBNTQ1jx45FUlIS1q1bBz6fj6SkJGhpaSEgIABAQ3J04MABfPPNN8jPz5cZTpSXlwc7O7s2iaWqqgq3bt2SSVYAYPz48Vi3bh3++OMP2NvbY+DAgdizZw+io6Ph4+MDR0dHmSE9LW2Xp6ioCFwuF4aGhgrF6unp2ahs0aJFMkmLPHx+x1/nGBoagmEYPHnyhBKIp1AC0YVpWQqgaa6H2gcVKPm9ACa+1qoOiRBCCGk1DocD87c2drkhTEDDakyHDx9GWloa/P39kZiYCD8/P+jo6AAADhw4gKioKMydOxfDhw+HQCBAZmYmNmzYIJNMPK+KigowDANjY2OZcj09PfD5fJSVNTyA9u233waXy8Xx48cRExMDIyMjzJgxA+Hh4eBwOC1ul0coFEJNTU3h9+/ZGAHA3NwcvXo1vzjM0/0LBALU1dVBKBTK3IUoLy8Hh8OBvr6+QrG0RJq0KDvZu7uiBKIL43A4MBreGw+O30TZnw9hNNwSPC11VYdFCCGEtBqHwwGH//xDTjqam5sbLCwskJSUBGNjY+Tn52PNmjXs9uTkZPj5+SEyMpItu3v3bpvHoaenBw6Hg+LiYpnyiooK1NXVsRfUfD4fixcvxuLFi3Hv3j189913iI6OhqWlJV5++eUWt8ujr68v92K+KfISjdYOYZLOfcjJycGAAQPYOtnZ2TA3N2+T4UtAQ0ICAAYGBm3SX3dBCUQXp21jCL6JNuqKqlGa8RDGI1+M9aYJIYSQzoDD4SAoKAhxcXHQ1NSEgYEBvLy82O21tbVQV5f9cu/kyZNtHoeOjg4cHByQnJyM0NBQtvz06dMAgCFDhjRqY2VlhWXLluHIkSPspOTWbJeytm4YAZGfn9/kcxVa0tohTG5ubtDV1cXp06fZBEIkEiElJQXe3t5KxSBPQUEB9PT0YGJi0mZ9dgeUQHRx0rsQjxKzUHr1AQyHmoPLp4+VEEII6ShBQUGIjY3FsWPHEBwcLJMwjBw5EnFxcTh48CD69u2LH374Affu3VN6X0VFRUhOTm5U7uvri0WLFiE8PBzLly/HpEmTkJOTg+3btyMwMJCd4Lxw4UIMGjQIAwcOhJaWFn7++WeUlZVhxIgRCm2Xx9nZGWpqavjrr7+UTiAsLS1haWmpcH0NDQ2EhYUhOjoaRkZGsLOzw9dff43S0lJ2AjsAXLlyBaGhofj444/ZOyg1NTU4f/48gIYEobKykn1Phw0bBiOj/y2T/9dff8HV1VXhpV9fFHSl2Q3o2vWAusE9iEprUfbnIxi+pPgPICGEEEKej52dHezt7ZGVlYWJEyfKbAsPD0dJSQm7jGpgYCDWrl2LBQsWKLWvGzduyH12wvnz5+Hv74/PPvsMO3fuxMKFC2FgYIDXX39dZviUm5sbTp8+jX379kEsFsPa2hpbt27FyJEjFdouj7a2Nry8vJCamorJkycrdVzKmDdvHhiGwd69e1FcXAwHBwfs2bMHvXv3ZuswDAOxWCyzROyTJ08avYfS13FxcRg+fDiAhjsav/76K959990OOJquhcMwDKPqIDoj6ZJkTk5OKtl/dXU1bt26BQcHB2hra7dYv/yvQlTnlcFouCX4Ri3XJ91Ta88bQgA6b4hylD1vamtrkZOTA2tr6zYbp05U79y5c4iMjMSlS5eaXZpWLBazz2h4ehnXzuiXX35BZGQkUlNT2UnxXVlLP3utufal+zHdhMDRFL3G2VHyQAghhJAON2rUKFhbW+Pbb79VdShtZu/evZg1a1a3SB7aGiUQhBBCCCHkuXA4HHzwwQdt+mA8VaqqqsKwYcNkJqST/6E5EN2M8N9qlFzJg46NEfQG0IoBhBBCCOkYzs7OcHZ2VnUYbUJHRweLFi1SdRidFt2B6Gaq/nmCiptFKL6cB5reQgghhBBC2holEN2MvosZuHwe6v6tRtXd4pYbEEIIIYQQ0gqUQHQzPE016LuYAQDdhSCEEEIIIW2OEohuyGCIOThqXAgfVaLmfpmqwyGEEEIIId0IJRCdlFgsQVmJSKk7CGo6fAicTAEAxel5bR0aIYQQQgh5gVEC0QlJJAx2bkrDhdP/oqykVqk+DIdaAFwOau6XoeZBeRtHSAghhBBCXlSUQHRCXC4Hhj0aHghX+KBCqT7U9TVh4GYOYy8r8I3p4XKEEEIIIaRtUALRSfUy1wMAPCpQ/u6Bia81jIb3Bk+DHvdBCCGEtCV7e/sW/xw7dkzp/kNCQhAWFtYmsfr5+WHDhg1t0ld7iIiIQFRUFPt65cqVCAoKarf91dXVISoqCh4eHnBxccGsWbOQnZ2tUNurV68iODgYzs7OGDVqFL788stGw80PHTqEsLAwjBgxAvb29khOTm7UzxdffIFZs2a1yfGoAl1ZdlKm5gIAD5S+A0EIIYSQ9nPkyBGZ18HBwQgJCZG58O3Tp4/S/a9fvx5cbvf/nvfGjRv4+eefcebMmQ7b58aNG3Hq1CmsXLkSpqam2LVrF0JDQ5GUlAQ9Pb0m2927dw9z5syBh4cHlixZgqysLGzduhU8Hg9z5sxh633//fcAAB8fH5w4cUJuXzNmzMBXX32Fy5cvY8SIEW16fB2BEohOyvT/70AUPqgAwzDgcDhK91V55wlKfsuH6Xg78A26xyPmCSGEEFVycXFpVGZmZia3XKq2thaampoK9d+vXz8lI+ta4uLi4OnpCVNT01a1a817+bRHjx7h6NGjWL9+PaZMmQIAcHJywqhRo5CQkIB58+Y12XbPnj0wNDTEtm3bwOfz4e7ujuLiYuzatQshISHg8/kAgISEBHC5XOTn5zeZQAgEAowZMwZxcXFdMoHo/qltF9Wjpw64XEBYW4+SJ9XP1VfZn49Q+6ACJb8VtFF0hBBCCGlOdHQ0XF1dcf36dQQHB8PJyQmHDh0CAGzduhUTJ06Eq6srvLy8sGzZMjx+/Fim/bNDmKT9ZWVlYfr06Rg8eDCCgoKQlpbWJvEmJCQgMDAQjo6O8PPzw+effw6JRMJuLy8vx9q1a+Hl5QUnJyf4+Phg6dKlCm+Xp7q6GikpKQgMDGy23rFjx2Bvb4+MjAzMmjULLi4u2LJli1LHeeHCBUgkEowdO5YtMzAwgIeHB1JTU5ttm5qaCn9/fzZRAIDx48ejvLwcGRkZbJmid47Gjh2L8+fPo7i46z34l+5AdFI8NS70DNRRVizCw7wyGPXQUbovoxGWqM4tQcVfhTB27w01XY02jJQQQghpG7X1wia3cTlc8HnqitUFB3w1vlJ125JIJEJkZCRCQ0OxdOlSGBgYAACePHmCsLAw9OzZE8XFxdi3bx9CQkKQlJQENbWmL81EIhGWL1+Ot956CwsXLsTu3bsRERGBc+fOwdDQUOk44+PjsXHjRoSEhMDX1xcZGRmIiYlBRUUFVqxYAQDYtGkT0tLSEBkZCQsLCxQVFclccLe0XZ5r166huroaQ4YMUSjOyMhIBAcHIywsDFpaDSMqxGJxi0veczgc8Hg8AEB2djaMjY2hr68vU8fW1hZHjx5tso/q6mo8fPgQNjY2MuU2NjbgcDjIzs7G8OHDFToOKVdXV4jFYly5ckUmoekKKIHoxPr004a+wPj/50MoT8tSH5oWAtQWlKPk9wcw8bVuowgJIYSQtvPWd0ua3OZq5ohV3uHs63kn3oNQXCe37kCT/vjAbxn7OjxxLSqElXLr2hpaYdOYlcoF3AKRSISlS5di/PjxMuWbNm1i/y0Wi+Hq6gpvb29cvnwZnp6ezfa3fPly+Pj4AACsra3h7++P1NRUTJ48WakYxWIxdu7ciQkTJmDt2rUAAE9PT4hEIuzduxfz58+HoaEhMjMzERQUhFdeeYVtO2HCBPbfLW2XJzMzE9ra2ujdu7dCsU6bNg3z58+XKQsNDcWVK1eabTds2DDEx8cDaLhTIm+eg0AgQFlZ0w/fraioYOs9jc/nQ0tLq9m2TREIBDA3N8eff/5JCQRpO336acPBoS+0tZ9/GVaj4ZZ4cOwmyv58CKPhluBpqbfciBBCCCHPRXqx/7Tz58/jiy++wJ07d1BZ+b/EJjc3t9kEgsvlwt3dnX1taWkJTU1NFBYWKh1fdnY2SkpKGl3Ajh8/HrGxsbh+/Tp8fHwwcOBAHD9+HCYmJvDy8oKdnZ1M/Za2y1NUVNSqOye+vr6Nyj788ENUVVU1205HR/lRHO3NwMAARUVFqg6j1SiBeEFoWxtCo6cOhI+rUHr1AYw9rFQdEiGEECIj7rVPm9zG5ciOK9/9ctNj4LmQXXhkZ9BGheu2JS0trUYXr9evX8fChQvh7++PefPmwdjYGBwOB6+//jqEwqaHWgGApqamzPh7AFBXV2+xXXOk35wbGxvLlEtfS7e///770NfXx759+7BlyxaYmZlh/vz5eOONNxTaLo9QKGx0PM3p0aNHozIrKyuFhjBJCQQCmaRNqry8vNGwpqdJ71pI70RI1dXVoaamptm2zeHz+c/1+akKJRCdXFlpDXLvlKKPjTF09ZSfu8DhcGA4vDcenfwbpRkPYfiSBbh8+vgJIYR0Hppqiv8/115125K8FRTPnDkDXV1dfPrpp+xk24IC1S1yIp2X8exE3idPngAAe2Gsp6eHNWvWYM2aNcjKykJcXBw+/PBD2NnZYejQoS1ul0dfX7/RBXlrtXYIk42NDf7991+UlZXJXPRnZ2c3mt/wNG1tbZiZmTV6XkROTg4Yhmm2bXMqKirQv39/pdqqEl1BdnInj9zAo/xyvPqmKxxdLZ6rL93+xtC17wE9+x7gqPPaKEJCCCGEKKq2thbq6uoyycXJkydVFo+1tTWMjIyQnJyMgIAAtvz06dNQV1eHs7Nzozb29vZYtWoVjh49irt37zZKEFra/vS+i4uLUV1drfRw7dYOYfL09ASXy0VKSgqmTp0KoOEuy4ULF7Bw4cJm+/H29sbZs2fx7rvvQl29YSj4qVOnIBAI4Orq2urYJRIJHjx4gNdee63VbVWNEohOrpe5Hh7ll+NhftlzJxAcLgdmEwe0UWSEEEIIaS0PDw8cOHAAH330EQICApCRkcE+eKw93b9/v9ETkblcLsaMGYOFCxdi48aNMDIygo+PD65du4bdu3dj5syZ7ByFadOmISAgAP379wePx8OJEyegrq7OJgctbZfHzc0NEokEN2/ebLZec1r7zX+vXr0wZcoUbNmyBVwuF6ampoiNjYWenh6mTZvG1jtx4gRWr16N/fv3Y9iwYQCAOXPm4OTJk4iMjMT06dNx+/Zt7NmzB0uXLpUZipWZmYmCggL2rs6ff/4JADAyMmL7AhruXlRXVyt97KpECUQnJ32g3IO81s/ub8nzPqCOEEIIIa3j4+OD5cuX4+DBgzh27Bjc3NwQGxvb4rMQnldaWlqjZ0bweDzcvHkTISEhUFNTw/79+/H111/DxMQEixYtwoIFC9i6bm5uOHHiBPLz88HlcmFnZ4ddu3bB1tZWoe3yWFtbw87ODmlpaR16Eb127Vro6Ojgk08+QVVVFdzc3LBv3z6Z1ZkkEkmjJWKtrKywZ88ebN68GfPnz4eRkREiIiIwe/Zsmf4PHTqE48ePs6/37t0LQHYoFdDwXAkLCws4OTm116G2Gw7T0syTF1RmZiYAqOxDra6uxq1bt9DDsDcO7LwCDU01vPdRIDjc57/gl4jEKL36EFX/PIHldOc26ZN0DtLzxsHBoU1W7yIvBjpviDKUPW9qa2uRk5MDa2trpZ4kTLo2sVjMPkWax+MhPj4ecXFxSElJeeG+1HzttdcwatQoLFq0qEP219LPXmuufelJ1J2csYk21NS4ENbWo/hJ82P8WqP09wLUPqxAxd9db+kwQgghhHQPU6dORW1tLc6dO6fqUDrUb7/9hry8PLz11luqDkUplEB0clweF6YWDQ8tedhGw5i46jwYDDUHAJSk57e4/BkhhBBCSHvQ1NTE5s2bIRKJVB1Kh6qsrERUVFSjB9N1FTQHogswt9RHwb1SPMgvg6Pb802kltJ3MUNJej7qnlSj6p9i6PY3brkRIYQQQkgb8/DwUHUIHW7UqFGqDuG5UALRBQx+qTf62BjD0krxpzW2hKehBn3XhiSiOD0POv2MXrixh4QQQgghpPVoCFMXYN7bAINczKFvqNWm/Rq4mYOjxoXwUSVq7rf9Kk+EEEIIIaT7oQTiBaamw4e+sykAoDg9T8XREEIIIYSQroCGMHURBfdLkfvPv7Dsawgrm7abr2Aw1BLimnoYDrNssz4JIYQQQkj3RXcguojrv+fjbNLf+DvzUZv2qy7QQK8J9tAw0Wm5MiGEEEIIeeFRAtFFmPfWBwA8zG/fuQq0pCshhBBCCGkOJRBdhJllQwLxqKAMjKTtL/JFZbUoTL6NwuQ7bd43IYQQ0t0sWLAAY8aMaXJ7fHw87O3tcf/+fYX6s7e3x549e5qtExISgrCwsFbF2ZGioqIQERHBvo6Ojoarq2u77Y9hGHz55Zfw9fWFs7MzgoODce3aNYXa3r17F7NmzYKLiws8PDywZcsW1NXVydQ5deoUFi9eDG9v7yY/nx9++AHjxo2DWCxui0PqMiiB6CJ69NSFOp+HOqEYT4ra7onUUhJhPcr/eoyKm49RV1rT5v0TQggh3UlQUBDu3buH69evy92elJQEFxcX9OnTp4MjU43CwkIcPnwY8+fP77B97t69Gzt27EBoaChiY2NhYmKC2bNnIy+v+YVhysrKMHPmTIhEIkRHR2Pp0qX45ptvsHnzZpl6ycnJyMvLg6+vb5N9TZgwAXV1dThx4kQbHFHXQQlEF8HlcWFq3vC0wgf5pW3ev0ZPXWhbGwIMUHKloM37J4QQQroTf39/aGtrIzExsdG2/Px8ZGRkICgoSAWRqcaRI0dgZWUFR0fHVrWrra1Van9CoRCxsbGYPXs2QkND4e7ujm3btsHAwKDFOzkJCQmoqqpCTEwMvLy8MGXKFLz77rtISEhAYWEhW+/TTz/FiRMnsGHDhib74vF4eOWVVxAfH6/UcXRVlEB0Ie09D8JoRG8AQMWNQtRXCNtlH4QQQkh3oKWlBX9/f5w+fRoSiURmW1JSEng8HsaPH4/Hjx9j1apV8Pf3h7OzM8aMGYNt27Y1Gi7TVlJSUjB58mQ4OTnB09MTmzZtglD4v//TRSIRoqKi4OvrC0dHR3h6emLBggWoqKhQaHtTTpw4gcDAwGbrpKenY+DAgUhLS8OSJUvg5uaGd955R6njvHr1KiorKzFu3Di2jM/nIyAgAKmpqc22TU1Nhbu7OwwMDNiycePGQSKR4OLFi2wZl6vYZfK4ceNw69Yt/P333607iC6MlnHtQswsDQA0zINoD1oWAmhZClCTX46S3wtgMsqmXfZDCCGEdAcTJ07EyZMnkZ6eDnd3d7Y8MTERI0eOhLGxMbKysmBgYIBVq1ZBIBAgNzcX0dHRKCoqwqZNm9o0nrNnzyIiIgITJkxAZGQksrOzsX37djx8+BA7duwAAMTGxiIhIQHLly9H//79UVJSgosXL7IJTUvb5bl37x4KCgrg5uamUJwbN27EpEmTsHPnTvYiXSKRNErEnsXhcMDj8QAA2dnZAAAbG9lrFVtbWxw4cAC1tbXQ1NSU2092djZee+01mTKBQAATExO239awtbWFvr4+Ll68iAEDBrS6fVdECUQX0t+hJ+Yt9ULPXnrttg/D4b1Rk38DZX8+gtGI3uBpqbfbvgghhBAphmEgEaru7jdXQwMcDqdVbTw8PGBkZISkpCQ2gbh9+zZu376NOXPmAGiYHL1ixQq2jZubG7S0tLBy5UqsW7cOWlpabXYMMTExcHFxwSeffAIA8Pb2hpaWFtatW4esrCzY29sjMzMTnp6emDFjBtvu6TsHLW2XJzMzE0DDsSrCx8cHkZGRbDIAAKtXr8bx48ebbWdhYYFz584BAMrLy8Hn86GhoSFTRyAQgGEYlJWVNZlAlJeXQyAQNCrX19dHWZlyX9La29vjzz//VKptV0QJRBeircOHtg6/fffR1wAapjoQFlah9OoDGHtYtev+CCGEEIZhkLlyDSr+zlJZDHoOA+C0aWOrkgg1NTWMHTsWSUlJWLduHfh8PpKSkqClpYWAgAAADcd24MABfPPNN8jPz5cZTpSXlwc7O7s2ib+qqgq3bt2SSVYAYPz48Vi3bh3++OMP2NvbY+DAgdizZw+io6Ph4+MDR0dHmaE6LW2Xp6ioCFwuF4aGhgrF6unp2ahs0aJFMkmLPHx++14DPQ9DQ0MUFRWpOowOQwkEkcHhcGA80gq1hZUwcDVTdTiEEEJeFK389r+zCAoKwuHDh5GWlgZ/f38kJibCz88POjoND2g9cOAAoqKiMHfuXAwfPhwCgQCZmZnYsGGDTDLxvCoqKsAwDIyNjWXK9fT0wOfz2W/W3377bXC5XBw/fhwxMTEwMjLCjBkzEB4eDg6H0+J2eYRCIdTU1BROvp6NEQDMzc3Rq1evZts93b9AIEBdXR2EQqHMXYjy8nJwOBzo6+s32Y9AIJA7p6OsrKzZds1RV1dv08+zs6MEoou5n1OMP3/LQ4+eunD3tW2XfejYGkHH1qhd+iaEEEKexeFw4LRpY5cbwgQ0DEmysLBAUlISjI2NkZ+fjzVr1rDbk5OT4efnh8jISLbs7t27bRLz0/T09MDhcFBcXCxTXlFRgbq6OvbCmM/nY/HixVi8eDHu3buH7777DtHR0bC0tMTLL7/c4nZ59PX15V7MN0Xe+9zaIUzSuQ85OTky8w6ys7Nhbm7e5PAladtn5zpUVFSgqKio0ZwKRVVUVMhMyu7uKIHoYkqfVCMjPQ+9+xq2WwLxNOmTqZX5pUoIIYQoisPhgNfMRV9nxeFwEBQUhLi4OGhqasLAwABeXl7s9traWqiry84nPHnyZJvHoaOjAwcHByQnJyM0NJQtP336NABgyJAhjdpYWVlh2bJlOHLkiNzJwy1tl7K2tgbQsHytra1y1yatHcLk5uYGXV1dnD59mk0gRCIRUlJS4O3t3Ww/3t7e2LVrl8xciOTkZHC5XHh4eCgVf0FBAUaMGKFU266IEoguxuz/l3J99KAcEgkDLrf9Luyr75fiyYV70BvYEwYuNJyJEEIIkScoKAixsbE4duwYgoODZRKGkSNHIi4uDgcPHkTfvn3xww8/4N69e0rvq6ioCMnJyY3KfX19sWjRIoSHh2P58uWYNGkScnJysH37dgQGBrITnBcuXIhBgwZh4MCB0NLSws8//4yysjL24rel7fI4OztDTU0Nf/31l9IJhKWlJSwtLRWur6GhgbCwMERHR8PIyAh2dnb4+uuvUVpayk5gB4ArV64gNDQUH3/8MXsHZdq0aYiPj0d4eDjCwsJQWFiILVu2YNq0aTA1NWXb/vPPP/jnn3/Y17dv30ZycjK0tLTg4+PDlldXVyM7Oxvh4eFKHXtXRAlEF2Ns0vBEalGdGP8+rmzXFZnqnlSj9kEF6ivroO9kCg6PHhtCCCGEPMvOzg729vbIysrCxIkTZbaFh4ejpKSEXUY1MDAQa9euxYIFC5Ta140bN+Q+O+H8+fPw9/fHZ599hp07d2LhwoUwMDDA66+/LjN8ys3NDadPn8a+ffsgFothbW2NrVu3YuTIkQptl0dbWxteXl5ITU3F5MmTlTouZcybNw8Mw2Dv3r0oLi6Gg4MD9uzZg969e7N1GIaBWCyWWSJWX18fBw4cwEcffYTw8HDo6OhgypQpWLp0qUz/p0+fRkxMDPv6xIkTOHHihMxQKgC4cOECNDU1W7zz0Z1wGOkYFSJDuiSZk5OTSvZfXV2NW7duwcHBAdra2jLb9u+8hPvZxZg83QWDhyqerbeWRCRG7u7fIa4WwXScHQSDerbbvkjbaO68IaQpdN4QZSh73tTW1iInJwfW1tbNjlMnXcu5c+cQGRmJS5cuNbs0rVgsZp/R8PQyrl1ZREQEdHR02vy5Hm2tpZ+91lz70lfKXZCZ5f8/kTqvtF33w1XnwWCIBQCgJD0PlGsSQgghRJ5Ro0bB2toa3377rapD6VB5eXk4f/483n77bVWH0qEogeiCzP8/gXiQ3z5PpH6avksvcDV4qCuuQdU/T9p9f4QQQgjpejgcDj744IM2fTBeV1BYWIgNGzagT58+qg6lQ9EciC7IzNIAACAS1oNhmHZdIYmnoQYDV3MUX85D8eV86PQzphWZCCGEENKIs7MznJ2dVR1Ghxo6dCiGDh2q6jA6HCUQXZCxiQ7e2xgITS31liu3AQM3c5T8XgBhYSVq8sqg3cegQ/ZLCCGEEEI6H0oguiAOl9NhyQMA8LTVYexlBTUdPrQslXtCIyGEEEII6R4ogSAKMfz/ydSEEEIIIeTFRglEF/UwvwwpP9yEmhoXM+YP79B9MxIGnHZ8gB0hhBBCCOm8KIHootT5PNy7+wRq6lxIxBJwO+ghb6VXH6DkSj7MXx0IjZ66HbJPQgghhBDSedAyrl2UcQ8d8DXUUC+SoOhxZYftt+ZBOeor61B8Jb/D9kkIIYQQQjoPSiC6KA6XAzNLAQDgYV77Pw9Cymh4w+PhK7P+RV1JTYftlxBCCCGEdA6UQHRh0udBPMwv7bB9apjoQMfGEGCAEroLQQgh5AVlb2/f4p9jx44p3X9ISAjCwsLaJFY/Pz9s2LChTfpqDxEREYiKimJfr1y5EkFBQe22v7q6OkRFRcHDwwMuLi6YNWsWsrOzFWp79epVBAcHw9nZGaNGjcKXX34JhmFk6jAMgy+//BK+vr5wdnZGcHAwrl27JlOnuLgYGzduxNSpU+Ho6AhXV9dG+5JIJAgMDMQPP/yg9LG2F5oD0YWxT6TuwDsQAGA4ojeqsktQfuMxjEb2gbqeRofunxBCCFG1I0eOyLwODg5GSEiIzIXv8zydeP369eByu//3vDdu3MDPP/+MM2fOdNg+N27ciFOnTmHlypUwNTXFrl27EBoaiqSkJOjp6TXZ7t69e5gzZw48PDywZMkSZGVlYevWreDxeJgzZw5bb/fu3dixYweWL18Oe3t7HDp0CLNnz8b333+P3r0bRnIUFhbi1KlTcHZ2hqOjI7Kyshrtj8vlYv78+YiOjsb48eOhptZ5Lts7TySk1cx6NyQQhQ/KIRZLwOugidRa5gJo9dZHTV4ZSn8vgMkomw7ZLyGEENJZuLi4NCozMzOTWy5VW1sLTU1Nhfrv16+fkpF1LXFxcfD09ISpqWmr2rXmvXzao0ePcPToUaxfvx5TpkwBADg5OWHUqFFISEjAvHnzmmy7Z88eGBoaYtu2beDz+XB3d0dxcTF27dqFkJAQ8Pl8CIVCxMbGYvbs2QgNDQUADBkyBGPHjsWePXvwwQcfAGi4g3Xp0iUAQHR0tNwEAgDGjx+PjRs34pdffsHo0aNbfbztpfuntt2YkbEOjHpow8rWGLXVog7dt+FwSwBA2Z+PIBbWd+i+CSGEkM4uOjoarq6uuH79OoKDg+Hk5IRDhw4BALZu3YqJEyfC1dUVXl5eWLZsGR4/fizT/tkhTNL+srKyMH36dAwePBhBQUFIS0trk3gTEhIQGBgIR0dH+Pn54fPPP4dEImG3l5eXY+3atfDy8oKTkxN8fHywdOlShbfLU11djZSUFAQGBjZb79ixY7C3t0dGRgZmzZoFFxcXbNmyRanjvHDhAiQSCcaOHcuWGRgYwMPDA6mpqc22TU1Nhb+/P/h8Pls2fvx4lJeXIyMjA0DDEKfKykqMGzeOrcPn8xEQECDTv6J3l7S0tODj44Pjx48rVL+jdLo7EHfv3sXGjRuRkZEBHR0dTJ48GUuWLJH5sOQpKSnB9u3bkZqaitLSUlhaWmLGjBmYPn16B0Xe8ThcDhat8lPJvrWtDGD4kgV0B5iAp9HpTiNCCCFdUF0zX0hxuRyoqfMUqsvhcqCuZN22JBKJEBkZidDQUCxduhQGBgYAgCdPniAsLAw9e/ZEcXEx9u3bh5CQECQlJTU7TEUkEmH58uV46623sHDhQuzevRsRERE4d+4cDA0NlY4zPj4eGzduREhICHx9fZGRkYGYmBhUVFRgxYoVAIBNmzYhLS0NkZGRsLCwQFFRkcwFcUvb5bl27Rqqq6sxZMgQheKMjIxEcHAwwsLCoKWlBQAQi8WN5iA8i8PhgMdr+Iyzs7NhbGwMfX19mTq2trY4evRok31UV1fj4cOHsLGRHXVhY2MDDoeD7OxsDB8+nJ1L8Ww9W1tbHDhwQKk7J66urtixYwckEkmnGdbWqa78ysrKMHPmTPTt2xfR0dEoLCzE5s2bUVtbi3Xr1jXb9p133kF2djaWLVsGMzMzpKam4oMPPgCPx8Prr7/eQUfw4uBwOOjhY63qMAghhHQjm1cnN7mtn0NPvDF3GPv6kw9+gqhOLLeula0RZi4cyb7e8Z9zqK6qk1vXvLc+5i7xUjLi5olEIixduhTjx4+XKd+0aRP7b7FYDFdXV3h7e+Py5cvw9PRstr/ly5fDx8cHAGBtbQ1/f3+kpqZi8uTJSsUoFouxc+dOTJgwAWvXrgUAeHp6QiQSYe/evZg/fz4MDQ2RmZmJoKAgvPLKK2zbCRMmsP9uabs8mZmZ0NbWZucFtGTatGmYP3++TFloaCiuXLnSbLthw4YhPj4eQMOdEnnzHAQCAcrKmp5TWlFRwdZ7Gp/Ph5aWFtu2vLwcfD4fGhqy80MFAgEYhkFZWVmrE4gBAwagsrISd+/eRf/+/VvVtr10qgQiISEBVVVViImJYbN0sViMDz/8EGFhYU2OjysqKkJ6ejo2bdqEV199FQDg7u6OzMxMJCUlvRAJRE11HbS0m79L054YsQScDpqDQQghhHQV0ov9p50/fx5ffPEF7ty5g8rK/z3LKTc3t9kEgsvlwt3dnX1taWkJTU1NFBYWKh1fdnY2SkpKZIb0AA1Dc2JjY3H9+nX4+Phg4MCBOH78OExMTODl5QU7OzuZ+i1tl6eoqKhVd058fX0blX344Yeoqqpqtp2Ojo7C++iMpO9RUVERJRDypKamwt3dnU0eAGDcuHFYv349Ll68yCYHz6qvb7g1+WxGqauri+rq6naLtzOorBDiq+1pqKqqw8qPx3bYRGopsbAe/6bmojq7BFaz3cBtp9vAhBBCur+VH49tchuXy5F5HflBQJN1Oc/UjVjT9HDfZ+u2JS0trUYXr9evX8fChQvh7++PefPmwdjYGBwOB6+//jqEQmGz/WlqajYa0q2urt5iu+ZIvzk3NjaWKZe+lm5///33oa+vj3379mHLli0wMzPD/Pnz8cYbbyi0XR6hUNjiEPWn9ejRo1GZlZWVQkOYpAQCgUzSJlVeXt5oWNPTpNeY0jsRUnV1daipqWHbCgQC1NXVQSgUytyFKC8vB4fDaXYfTZG+R7W1ta1u2146VQKRnZ2N1157TaZMIBDAxMSk2fV5zczM4OnpiV27dsHa2hq9evVCamoqLl68iK1bt7Z32Cqlo8uHUFgPcb0ERY8q0Mui9Sfm8+CqcVGdU4L6CiHK/yqEgat5h+6fEEJI98FvxZy69qrblp6+cJU6c+YMdHV18emnn7Lj2QsKCjo6NJb0S9vi4mKZ8idPngAAe8Grp6eHNWvWYM2aNcjKykJcXBw+/PBD2NnZYejQoS1ul0dfX7/RBXlrtXYIk42NDf7991+UlZXJXMxnZ2c3mrfwNG1tbZiZmTW6Hs3JyQHDMGxb6d85OTkYMGCATP/m5uZKrRxVXl4OADJfsKtap0ogysvLG40tAxpOsObGpQENqxMsXbqUHW/H4/Gwdu3aFmf2N4dhGJXdwaipqZH5uzmm5nq4n12Ce9lFEBiqt3dojei49ERZah6K0/Oh3k8fHF77fZtDmtea84YQKTpviDKUPW+EQiEkEgnEYjHEYvlzGLoq6XFJ/80wTKNjrKmpgZqaGrsdAL7//vtG7RmGkWnfVH/PtpPn2b6e1qdPHxgZGeH06dPw8/vfnZqkpCSoq6vD0dGxUbt+/fphxYoVOHr0KO7cudPoIWjNbZceM8MwsLKyQnFxMSoqKqCtrd1kvNLVoOQd5/r16xUawiRt5+7uDi6Xi+TkZHYZ17KyMly4cAFvv/12s++jp6cnzp49i2XLlkFdXZ19nwQCAZydnSEWizF48GDo6uri1KlT7HAjkUiElJQUeHl5Nfn5NfX5AEBeXh6Ahs/qeX5mxGIxJBIJampqZFbYkmIYRm7SK0+nSiCUxTAMVq1ahdzcXHzyyScwMTHBpUuX8PHHH0NfX7/FSTxNEYlEuHXrVhtH2zq5ubkt1lHTaJgYduuv+1DT7tiHyjVgoMsHUFmHu+f/gsisW5xWXZoi5w0hz6LzhihDmfNGTU3tuYbddFb19fXsMBPp8Opnh50MHTqU/XZ+1KhR7HzNZ9tLL5Zb6o9hGJl28jAMg9zcXCQmJsqUczgc+Pv7Y+7cudiyZQsEAgE8PT2RmZmJPXv24I033oCmpiZqa2sxa9YsjBo1Cra2tuDxeEhMTGQTDEW2P0soFGLQoEGQSCT4888/ZZIM6cpK0nYikYg99mf7MjMza/K4nyZtZ2BggJdffhn//e9/IRaL0bNnT+zduxe6urqYNGkSWy8xMREffvghdu3axa4SNWPGDCQmJmLZsmWYOnUq/vnnH+zduxfh4eGQSCRs21mzZiE2NhZ6enro168fvv32W5SWluKNN96QiV/68Lw7d+5AIpGwn8/AgQNhbv6/ER1//vknrK2toaWl9VzDmIRCIerr65sd1aPokLJOdaUnEAjk3sp69jbTs3755RckJyfjhx9+gL29PQBg+PDhePLkCTZv3qx0AqGurq6yB7nU1NQgNzcXffv2ZZcqawqnvhDZt26groYHBweHDopQVoWoEOW/FkDvERc9fQe065hS0rTWnDeESNF5Q5Sh7HkjFArx4MEDaGhoKDWcozNTU1Njj0m6HOuzxzh69GhERkbi0KFDOHnyJFxdXfHFF1+wTxqW1udyueDxeC32x+FwZNrJw+FwcOnSJfbBZVI8Hg+ZmZkIDQ2FpqYmDhw4gG+//RYmJiYIDw/H/Pnz2WFWQ4YMwalTp5Cfnw8ulws7Ozt8/vnnGDhwoELbpRiGYecH2Nvbw87ODunp6TKTw3k8HjgcDntM0m/7NTU12+Scef/996Gnp4eYmBhUVVXB1dUVe/fuhYmJiUwMYrEY6urq7D7t7Ozw1VdfISoqChERETAyMsKiRYswd+5cmW/uFyxYAB6Ph4MHD6K4uBgDBgzA7t27G11Tvvfee3Jf/+c//5EZTvXrr79i7NixbXLsampq6NOnT6NVogDgn3/+UbgfDtPSzJMONGPGDBgYGGDnzp1sWUVFBV566SV8/PHHTU6i3r17N7Zv344bN27IfICHDh3Chg0bcO3atVb/p5iZmQmg4emEqlBdXY1bt27BwcFB5raePMX/ViFm08/g8bgNE6nVOn41JEldPXJif4NEKEavSQOgZ9d4ohNpf605bwiRovOGKEPZ86a2thY5OTmwtrbudgkEaZn0roqmpiZ4PB7i4+MRFxeHlJQUhYfPvEju3LmDyZMn48cff1R4udumtPSz15pr30617qa3tzcuXbrEThYBgOTkZHC5XHh4eDTZzsLCAmKxuNFjwG/cuAFjY+Nu/42aobE2NLXUIRZL8PjR801GUhaXrwYDt4bbbaV/PFBJDIQQQgjpWqZOnYra2lqcO3dO1aF0Snv37sXkyZOfO3loa51qCNO0adMQHx+P8PBwhIWFobCwEFu2bMG0adNkngExc+ZMPHjwAD/99BOAhsTD3NwcERERCA8PR8+ePXHhwgUcP34cixcvVtXhdBgOhwPnoRZgJAz4GqpbRtXAzRxgAH03WomJEEIIIS3T1NTE5s2bn3s1pu5IIpHAysoKL7/8sqpDaaRTJRD6+vo4cOAAPvroI4SHh0NHRwdTpkzB0qVLZeo9OwtfV1cX+/fvx/bt27F161ZUVFTA0tISK1euxJtvvtnRh6ESY192VHUI4Gmpw9jTStVhEEIIIaQLaW6UyYuMy+ViwYIFqg5Drk6VQACAra0t9u/f32wd6Vq+T7OyssKnn37aPkERpUhEYnqwHCGEEEJIN9Op5kCQ5yOqEyMvpxji+sZr+3YkYVEV8o9k4uEPf6s0DkIIIYQQ0vYogegmGIbBZ/85i30xl1Q2kVqKo8ZFTX4ZqnNKIHzc+HHxhBBCCCGk66IEopvgcDgwNWt4iveDvFKVxsI31IKufcNaysXp+SqNhRBCCCGEtC1KILoR894ND9t7mK+Kp1HLMhpuCQCozPoXdcU1Ko6GEEIIIYS0FUoguhEzy86TQGiY6EDH1ggAUHKF7kIQQgghhHQXlEB0I+a9DQAAhQ/LUV8vbr5yBzD8/7sQ5TcfQ1Req+JoCCGEEEJIW6AEohvRN9SClrY6JGIGjx+q/oEsWuYCaPXWByQMyq8XqjocQgghpM0sWLAAY8aMaXJ7fHw87O3tcf/+fYX6s7e3x549e5qtExISgrCwsFbF2ZGioqIQERHBvo6Ojoarq2u77Y9hGHz55Zfw9fWFs7MzgoODce3aNYXa3r17F7NmzYKLiws8PDywZcsW1NXVNar37bffIjAwEE5OTpg0aRJ+/vlnme11dXXYsmULZsyYARcXF9jb26O4uLhRP7NmzcIXX3yh1HF2RpRAdCMcDgdmlgYAOscwJgAw9rSC6Tg7GLl3rkewE0IIIc8jKCgI9+7dw/Xr1+VuT0pKgouLC/r06dPBkalGYWEhDh8+jPnz53fYPnfv3o0dO3YgNDQUsbGxMDExwezZs5GXl9dsu7KyMsycORMikQjR0dFYunQpvvnmG2zevFmmXlJSEt5//32MGzcOu3fvhouLCxYtWiSTpNTW1uLbb7+FhoYGhgwZ0uQ+w8LCsHfvXpSVdY7rs+dFCUQ34zaiN8ZMHggrW2NVhwIA0LIQQDCoJzg8OtUIIYR0H/7+/tDW1kZiYmKjbfn5+cjIyEBQUJAKIlONI0eOwMrKCo6Ojq1qV1ur3BBnoVCI2NhYzJ49G6GhoXB3d8e2bdtgYGDQ4p2chIQEVFVVISYmBl5eXpgyZQreffddJCQkoLDwfyMmduzYgQkTJmDJkiUYMWIENmzYACcnJ+zcuZOtIxAIcOXKFezduxcTJkxocp8jRoyAQCDA8ePHlTrezoau6rqZgYPNMcLbBj166qo6lEYYsQQSkernZhBCCCHPS0tLC/7+/jh9+jQkEtkHuCYlJYHH42H8+PF4/PgxVq1aBX9/fzg7O2PMmDHYtm2b3OEybSElJQWTJ0+Gk5MTPD09sWnTJgiFQna7SCRCVFQUfH194ejoCE9PTyxYsAAVFRUKbW/KiRMnEBgY2Gyd9PR0DBw4EGlpaViyZAnc3NzwzjvvKHWcV69eRWVlJcaNG8eW8fl8BAQEIDU1tdm2qampcHd3h4GBAVs2btw4SCQSXLx4EQCQl5eH3Nxcmf4BYPz48fj1119lPj8Oh6NQzGPHjsWJEycUqtvZqak6APJiqPi7CP+ez4W+Sy8YDafhTIQQQmQxDIN6UftcVCtCTZ2v8IWg1MSJE3Hy5Emkp6fD3d2dLU9MTMTIkSNhbGyMrKwsGBgYYNWqVRAIBMjNzUV0dDSKioqwadOmNj2Gs2fPIiIiAhMmTEBkZCSys7Oxfft2PHz4EDt27AAAxMbGIiEhAcuXL0f//v1RUlKCixcvshfELW2X5969eygoKICbm5tCcW7cuBGTJk3Czp07weU2fJctkUgaJWLP4nA44PF4AIDs7GwAgI2NjUwdW1tbHDhwALW1tdDU1JTbT3Z2Nl577TWZMoFAABMTE7Zf6d/W1taN+heJRMjLy4Otra0ih8tydXXFV199heLiYhgZGbWqbWdDCUQ39KSoEgX3StHHxggGRtqqDgcAwIgZ1FcIUfr7Axi4mYOrzlN1SIQQQjoJhmHwXezHeHTvH5XFYGbVH6+GrWpVEuHh4QEjIyMkJSWxCcTt27dx+/ZtzJkzB0DD5OgVK1awbdzc3KClpYWVK1di3bp10NLSarNjiImJgYuLCz755BMAgLe3N7S0tLBu3TpkZWXB3t4emZmZ8PT0xIwZM9h2T985aGm7PJmZmQAajlURPj4+iIyMZJMBAFi9enWLw3ssLCxw7tw5AEB5eTn4fD40NDRk6ggEAjAMg7KysiYTiPLycggEgkbl+vr67BwF6d/P1pO+VmYuw4ABAwAA169fh6+vb6vbdyaUQHRDycdv4G5WEca/5oShI61UHQ4AQG9ADzy5eA/15UKUZxbCwM1c1SERQgjpRDho3bf/nYGamhrGjh2LpKQkrFu3Dnw+H0lJSdDS0kJAQACAhuTowIED+Oabb5Cfny8znCgvLw92dnZtEktVVRVu3bolk6wADUNu1q1bhz/++AP29vYYOHAg9uzZg+joaPj4+MDR0ZG9CwCgxe3yFBUVgcvlwtDQUKFYPT09G5UtWrRIJmmRh8/nK9R/ZyV9f4qKilQcyfOjBKIbMrPUx92sIjzMKwXQORIIDo8Lw2GWKDpzFyW/FUB/cC+aWE0IIQRAw9CUV8NWdbkhTEDDakyHDx9GWloa/P39kZiYCD8/P+jo6AAADhw4gKioKMydOxfDhw+HQCBAZmYmNmzYIJNMPK+KigowDANjY9lFVPT09MDn89lvzN9++21wuVwcP34cMTExMDIywowZMxAeHg4Oh9PidnmEQiHU1NQUfv+ejREAzM3N0atXr2bbPd2/QCBAXV0dhEKhzF2I8vJycDgc6OvrN9mPQCCQO6ejrKyMbSf9u6KiAiYmJjL9P729NaQJkLITxzsTSiC6IfPeneeJ1E8TOJqi+NJ91FcIUX6zCPpOpqoOiRBCSCfB4XCgztdouWIn4+bmBgsLCyQlJcHY2Bj5+flYs2YNuz05ORl+fn6IjIxky+7evdvmcejp6YHD4TR6BkFFRQXq6urYC14+n4/Fixdj8eLFuHfvHr777jtER0fD0tISL7/8covb5dHX15d7Md8UeYlGa4cwSec+5OTksEODgIa5C+bm5k0OX5K2lc5xkKqoqEBRURHbr/Tv7OxsmXkW2dnZUFdXR+/erZ/PKU1anp683VVRAtENSZ8F8fhRBepFYqh1kvkGXDUuDIda4N/UXJRcyW9Y3pXb9W5ZE0IIIVIcDgdBQUGIi4uDpqYmDAwM4OXlxW6vra2Furq6TJuTJ0+2eRw6OjpwcHBAcnIyQkND2fLTp08DgNxnFFhZWWHZsmU4cuRIowtqRbZLSSca5+fnt3pisVRrhzC5ublBV1cXp0+fZhMIkUiElJQUeHt7N9uPt7c3du3aJTMXIjk5GVwuFx4eHgCA3r17o2/fvkhOTsbo0aPZtqdOnYK7u7tSw6ny8/MBNJ6Y3RVRAtENCQw0oa3LR3VlHQoflsOij2JjEjuCvksvFKfnQVRSg5q8MmhbGag6JEIIIeS5BAUFITY2FseOHUNwcLBMwjBy5EjExcXh4MGD6Nu3L3744Qfcu3dP6X0VFRUhOTm5Ubmvry8WLVqE8PBwLF++HJMmTUJOTg62b9+OwMBAdoLzwoULMWjQIAwcOBBaWlr4+eefUVZWhhEjRii0XR5nZ2eoqanhr7/+UjqBsLS0hKWlpcL1NTQ0EBYWhujoaBgZGcHOzg5ff/01SktL2QnsAHDlyhWEhobi448/Zu+gTJs2DfHx8QgPD0dYWBgKCwuxZcsWTJs2Daam/xsdsXjxYixfvhx9+vTB8OHDcerUKVy/fh0HDx6UieX8+fOoqanBX3/9BQD4+eefoaOjg379+qFfv35svb/++gva2tpwcHBQ5i3qVCiB6IYankitj7t/F+FBXlmnSiC4fDX0DOgHNYEGtMwbr4BACCGEdDV2dnawt7dHVlYWJk6cKLMtPDwcJSUl7DKqgYGBWLt2LRYsWKDUvm7cuCH32Qnnz5+Hv78/PvvsM+zcuRMLFy6EgYEBXn/9dZnhU25ubjh9+jT27dsHsVgMa2trbN26FSNHjlRouzza2trw8vJCamoqJk+erNRxKWPevHlgGAZ79+5FcXExHBwcsGfPHpnhRQzDQCwWyywRq6+vjwMHDuCjjz5CeHg4dHR0MGXKFCxdulSm/6CgINTU1GD37t348ssvYW1tjZiYGLi6usrU+/DDD1FQUMC+Xr16NYCGuyqLFy9my1NTUxEQECCz+lRXxWEYhlF1EJ2RdEkyJycnley/uroat27dgoODA7S1W78U68+ns5B25g5chvXGpODB7RAh6Yye97whLyY6b4gylD1vamtrkZOTA2tr62bHqZOu5dy5c4iMjMSlS5eaXZpWLBazz2joDhfSiiorK4OHhwf27duHl156SSUxtPSz15prX1oGp5tydDNH8OyX4DdOsTWZVUVcI1J1CIQQQgh5TqNGjYK1tTW+/fZbVYfSKcXHx8PNzU1lyUNbowSimzIx1YP9IFPoCjrvtztFv+QgJ/Y31HSy1aIIIYQQ0jocDgcffPBBmz4YrzsxMDDA2rVrVR1Gm6E5EERlJHViMPUSFF/Oh8WU1q+nTAghhJDOw9nZGc7OzqoOo1N68803VR1Cm6I7EN1Y/r0S/JKchds3C1UdilyGwywADlCdW4LawkpVh0MIIYQQQhRACUQ3dufWY6T+dAd/X3+k6lDk4htoQW9Aw9MdS9LzVBwNIYQQQghRBCUQ3Zi5ZcOwoAf5paoNpBmGwxrWfK68/QR1T6pVHA0hhBBCCGkJJRDdmFnvhgSiqLASojqxiqORT8NEBzq2RgCA4iv5Ko6GEEIIIYS0hBKIbkxPoAkdPQ0wEgaPHpSrOpwmGQ7/310IsbBexdEQQgghhJDmUALRjXE4HHYY08NOPIxJy1wAk9G26DtnCHgatDAYIYQQQkhnRglEN2cmTSDyOvezFgxczKCmy1d1GIQQQohC7O3tW/xz7NgxpfsPCQlBWFhYm8Tq5+eHDRs2tElf7SEiIgJRUVHs65UrVyIoKKjd9ldXV4eoqCh4eHjAxcUFs2bNQnZ2tkJtr169iuDgYDg7O2PUqFH48ssvwTCMTB2GYfDll1/C19cXzs7OCA4OxrVr1xr1VVhYiMWLF8PV1RXDhg3DmjVrUFkpuyrlxYsXERkZidGjR8Pe3l7u5/jHH39g+PDhjdq2J/q6t5sz620AACgqrFBtIK1QX1UHNR1KJgghhHReR44ckXkdHByMkJAQmQvfPn36KN3/+vXrweV2/+95b9y4gZ9//hlnzpzpsH1u3LgRp06dwsqVK2Fqaopdu3YhNDQUSUlJ0NPTa7LdvXv3MGfOHHh4eGDJkiXIysrC1q1bwePxMGfOHLbe7t27sWPHDixfvhz29vY4dOgQZs+eje+//x69e/cGAIhEIsydOxcA8Mknn6C2thZRUVGIjIxEbGws21daWhr+/vtvvPTSSygrk/9l8JAhQ9C/f3/s3bsXERERbfEWtYgSiG6ur60x3n7PB8YmuqoOpUXiGhEeJWWh9kEF+s5/CTxNOj0JIYR0Ti4uLo3KzMzM5JZL1dbWQlNTU6H++/Xrp2RkXUtcXBw8PT1hamraqnateS+f9ujRIxw9ehTr16/HlClTAABOTk4YNWoUEhISMG/evCbb7tmzB4aGhti2bRv4fD7c3d1RXFyMXbt2ISQkBHw+H0KhELGxsZg9ezZCQ0MBNFzgjx07Fnv27MEHH3wAAPjxxx9x584dnDp1CjY2NgAAgUCAOXPm4Pr16+wD+d577z2sXLkSAJCent5kbFOmTEFUVBTefvttqKurt/p9aa3un9q+4DQ01WBiqgcul6PqUFrE1VRDfUUdJHVilF17qOpwCCGEEKVFR0fD1dUV169fR3BwMJycnHDo0CEAwNatWzFx4kS4urrCy8sLy5Ytw+PHj2XaPzuESdpfVlYWpk+fjsGDByMoKAhpaWltEm9CQgICAwPh6OgIPz8/fP7555BIJOz28vJyrF27Fl5eXnBycoKPjw+WLl2q8HZ5qqurkZKSgsDAwGbrHTt2DPb29sjIyMCsWbPg4uKCLVu2KHWcFy5cgEQiwdixY9kyAwMDeHh4IDU1tdm2qamp8Pf3B5//v1ES48ePR3l5OTIyMgA0DHGqrKzEuHHj2Dp8Ph8BAQEy/aempsLe3p5NHgDAw8MDBgYGOH/+PFum6F2o0aNHo6KiQqZte6IEgnQaHA6HXZGp9I8CSDrp0rOEEEKIIkQiESIjIzFp0iTs3r0bHh4eAIAnT54gLCwMsbGxWLNmDQoKChASEoL6+uZXIhSJRFi+fDleffVVxMTEwMjICBERESgpKXmuOOPj47F+/Xp4eXlh165deOWVVxATE4P//ve/bJ1Nmzbhl19+wbJly7Bnzx689957MhfSLW2X59q1a6iursaQIUMUijMyMhIjRozArl27MHnyZACAWCxGfX19s3/E4v9dT2RnZ8PY2Bj6+voyfdva2jY7D6K6uhoPHz6UueAHABsbG3A4HLat9O9n69na2uLBgweora1l6z1bh8PhwNraWuH5GE/T1dVFv379cOnSpVa3VQaNEXkB5OWW4LcLOdA30ob/+AGqDqdZegNM8OTiPdSXCVGW+QiGQyxUHRIhhJAO0uwXR1wOuGpcxepyAK46T6m6bUkkEmHp0qUYP368TPmmTZvYf4vFYri6usLb2xuXL1+Gp6dns/0tX74cPj4+AABra2v4+/sjNTWVvaBuLbFYjJ07d2LChAlYu3YtAMDT0xMikQh79+7F/PnzYWhoiMzMTAQFBeGVV15h206YMIH9d0vb5cnMzIS2tjY7L6Al06ZNw/z582XKQkNDceXKlWbbDRs2DPHx8QAa7pTIm+cgEAianGMAABUVFWy9p/H5fGhpabFty8vLwefzoaGh0ah/hmFQVlYGTU3NJuPQ19dvNo7mDBgwAH/++adSbVuLEogXQE11Hf7KeAATU91On0BwuBwYDbPE45/uovS3Ahi4mIHDoxtlhBDyIri749cmt2lbG8LitUHs6+zP08HUS+TW1bIUwHKaM/s6d/dvENfI/3Zfw1QXfUJclAtYAdKL/aedP38eX3zxBe7cuSOzck5ubm6zCQSXy4W7uzv72tLSEpqamigsLFQ6vuzsbJSUlMgM6QEahubExsbi+vXr8PHxwcCBA3H8+HGYmJjAy8sLdnZ2MvVb2i5PUVERDA0NFY7V19e3UdmHH36IqqqqZtvp6OgovI+uzNDQEEVFRR2yL0ogOiGGYXDlyhUUFxdjwIDnv+CXLuX67+NK1Anrwe/kz1rQG2SKJ5fyUF9Zh/Kbj6Hv1EvVIRFCCCGtpqWl1eji9fr161i4cCH8/f0xb948GBsbg8Ph4PXXX4dQKGy2P01NzUbDgtTV1Vts1xzpt93GxsYy5dLX0u3vv/8+9PX1sW/fPmzZsgVmZmaYP38+3njjDYW2yyMUClsc5vS0Hj16NCqzsrJqtIzqszic/80DFQgEcpc7LS8vbzSs6WnSuwXSOxFSdXV1qKmpYdsKBALU1dVBKBTK3IUoLy8Hh8ORqScvjrKyMpiZmTV7PE2RTuLuCJ37SvIFxTAMrl27BrFYjDt37jS7ooMi9ASa0BNooKJciEcPytHH2qhtAm0nXDUuDIea49/zuai4WUQJBCGEvCBsI9yb3vjMYiA2C4c3XfeZdUP6zntJ4bpt6ekLV6kzZ85AV1cXn376KTtBtqCgoP2CaIGBgQEAoLi4WKb8yZMnAMBe8Orp6WHNmjVYs2YNsrKyEBcXhw8//BB2dnYYOnRoi9vl0dfXb3RB3lqtHcJkY2ODf//9F2VlZTIJg7w5CU/T1taGmZlZo/kJOTk5YBiGbSv9OycnR+ZL4OzsbJibm7MrR9nY2OD27dsyfTEMg5ycHHauTGuVl5ezn2d7owSiE+JyuRgyZAiuXLmCtLQ02NjYNBpz11pmlgaouFmIh3mlnT6BAAD9wWbgaqpBz6GnqkMhhBDSQbh8xecitFfd9lZbWwt1dXWZ5OLkyZMqi8fa2hpGRkZITk5GQEAAW3769Gmoq6uzy4k+zd7eHqtWrcLRo0dx9+7dRglCS9uf3ndxcTGqq6uhra2tVPytHcLk6ekJLpeLlJQUTJ06FUDDt/4XLlzAwoULm+3H29sbZ8+exbvvvssulXrq1CkIBAK4uroCANzc3KCrq4vTp0+zCYRIJEJKSgq8vb1l+vrhhx+Qm5uLvn37AgB+/fVXlJaWyh32poiCggJYW1sr1ba1KIHopFxdXXHr1i1UVFTgxx9/xJQpU+R+k6Eos976uH2zEA/zO/cTqaW4fB7deSCEENLteHh44MCBA/joo48QEBCAjIwMfP/99+2+3/v37yM5OVmmjMvlYsyYMVi4cCE2btwIIyMj+Pj44Nq1a9i9ezdmzpzJzlGYNm0aAgIC0L9/f/B4PJw4cQLq6upsctDSdnnc3NwgkUhw8+bNZus1p7m7BvL06tULU6ZMwZYtW8DlcmFqaorY2Fjo6elh2rRpbL0TJ05g9erV2L9/P4YNGwYAmDNnDk6ePInIyEhMnz4dt2/fxp49e7B06VJ2KJaGhgbCwsIQHR0NIyMj2NnZ4euvv0ZpaanMw+YCAwMRGxuLxYsXY9myZaipqcGWLVvYp1dLFRQUIDMzEwBQU1Mj8zk+O2/lr7/+wqxZs1r1fiiLEohOisvlwsHBAVevXsX9+/eRkZEBNzc3pfuTzoN40EUSiKcxEgbi6jqo6Wq0XJkQQgjpxHx8fLB8+XIcPHgQx44dg5ubG2JjY1t8FsLzSktLa/TMCB6Ph5s3byIkJARqamrYv38/vv76a5iYmGDRokVYsGABW9fNzQ0nTpxAfn4+uFwu7OzssGvXLtja2iq0XR5ra2vY2dkhLS1N6QRCGWvXroWOjg4++eQTVFVVwc3NDfv27ZNZFUkikUAsFsvMr7CyssKePXuwefNmzJ8/n11Gd/bs2TL9z5s3DwzDYO/evSguLoaDgwP27Nkjs9qUuro6vvrqK2zcuBHLli2DmpoaAgICsHr1apm+0tPTsWrVKvb1059jVlYWW37jxg0UFxe3+3kkxWFamnnygpJme05OTirZf3V1NW7dugWxWIy0tDSoqanhzTffbDTJSVGV5bXY/tFZmPbSw9wlnuB2kZWNah6Uo/DUbfC01WE53fm57sK8CKTnjYODg9K3g8mLh84bogxlz5va2lrk5OTA2tpaqScJk65NLBazT5Hm8XiIj49HXFwcUlJS6P/45xAVFYUbN24gLi6uyTot/ey15tq3a1xFvsAGDRoEKysr1NfXIzk5WeapkK2hK9DEyo/HYn6kd5dJHgBAXaCJ+gohah9UoDa/XNXhEEIIIaQNTZ06FbW1tTh37pyqQ+myKisrcfToUSxevLjD9tl1riRfUBwOB4GBgdDQ0MCjR4+Qnp6udF/q7fSgnPakpsuHwNEUAFCcnqfiaAghhBDSljQ1NbF582aIRCJVh9JlPXjwAO+88w5eeqmZ1cbaGCUQXYCenh78/f0BAJcvX36uB8YAaHG95M7G8CVLgANU55ai9lHjNZMJIYQQ0nV5eHg0mhBMFGdnZ4c333yzQ/dJCUQXMWDAANjZ2UEikeDUqVNKZepFjyqw57ML2L09reXKnYi6gSb0HEwAACV0F4IQQgghRKUogegiOBwORo8eDR0dHRQXF+PixYut7kNLh4+C+6V49KAcwtr6doiy/RgOswQAVN55gron1SqOhhBCCCHkxUUJRBeipaWFMWPGAAD++OMP3L9/v1XtdfU0INDXBBjgUUHXWs5Vo4cOdPo1PACv8s4TFUdDCCGEEPLiogSii7GxsWEfMJKcnAyhUNiq9ma9G54H0VUeKPc0Y08rWE5zgtGI3i1XJoQQQggh7YISiC7Ix8cH+vr6qKiowM8//9yqtmaWBgCAB3mlbR9YO9PooQOt/38gHiGEEEIIUQ1KILogPp/PrlZw48YN/PPPPwq3Ne/CdyCeVldcDeFjWpGJEEIIIaSjUQLRRVlaWrLr/aakpKC6WrGJxWb//w3+k6IqCGu75prLNflluB//Jx6cuAVxTdc8BkIIIYSQrooSiC5s5MiR6NGjB2pqapCSkqLQ8x10dDVg2dcQA5x6obama63EJMXvoQM1HXXUlwvx6PTtLvdcC0IIIV3fggUL2IVN5ImPj4e9vb3CC57Y29tjz549zdYJCQlBWFhYq+LsSFFRUYiIiGBfR0dHw9XVtd32xzAMvvzyS/j6+sLZ2RnBwcG4du2aQm3v3r2LWbNmwcXFBR4eHtiyZQvq6uoa1fv2228RGBgIJycnTJo0Se7Q8YqKCqxevRrDhg2Dq6srIiIi8PjxY5k6mZmZWLVqFcaNG4cBAwbI/Rzz8/Ph4uKC/Px8xd4AFaIEogtTU1PDuHHjwOVycffuXdy8eVOhdrMXe+D10KHQN9Rq5wjbB09TDWaTBoDD46A6uwQlVzr/DxohhJDuJSgoCPfu3cP169flbk9KSoKLiwv69OnTwZGpRmFhIQ4fPoz58+d32D53796NHTt2IDQ0FLGxsTAxMcHs2bORl9f8M6PKysowc+ZMiEQiREdHY+nSpfjmm2+wefNmmXpJSUl4//33MW7cOOzevRsuLi5YtGhRoyRlyZIluHjxIj744ANs3boVOTk5mDdvHurr//dF7dWrV/H7779j4MCBMDc3lxuXpaUlAgMDER0drdwb0oEogejievbsiZEjRwIAzp07h/LychVH1DE0eurCZLQtAODJhXuovl+q2oAIIYS8UPz9/aGtrY3ExMRG2/Lz85GRkYGgoCAVRKYaR44cgZWVFRwdHVvVrra2Vqn9CYVCxMbGYvbs2QgNDYW7uzu2bdsGAwODFu/kJCQkoKqqCjExMfDy8sKUKVPw7rvvIiEhAYWFhWy9HTt2YMKECViyZAlGjBiBDRs2wMnJCTt37mTrZGRk4MKFC/jPf/6D8ePHw9/fH5999hmysrKQkpLC1gsJCcFPP/2ETz75BBYWFk3GNmXKFCQlJaG4uFip96WjUALRDbz00kswNzdHXV0dkpOTFRrSwzAMyktrOiC69iNwNIXeoJ4Nz7VIzEJ9ZeuWtCWEEEKUpaWlBX9/f5w+fRoSiURmW1JSEng8HsaPH4/Hjx9j1apV8Pf3h7OzM8aMGYNt27bJHS7TFlJSUjB58mQ4OTnB09MTmzZtklnyXSQSISoqCr6+vnB0dISnpycWLFiAiooKhbY35cSJEwgMDGy2Tnp6OgYOHIi0tDQsWbIEbm5ueOedd5Q6zqtXr6KyshLjxo1jy/h8PgICApCamtps29TUVLi7u8PAwIAtGzduHCQSCfug3ry8POTm5sr0DwDjx4/Hr7/+yn5+qampEAgE8PDwYOvY2NjAwcFBJg4uV7FL7iFDhsDAwAAnT55UqL6qUALRDXC5XIwdOxZqamrIy8tDRkZGs/XrhPXYui4Fn350FrVdeBIyh8NBz9G24PfQhrhahJIrBaoOiRBCiJIYhoFIJFLZH2Xm002cOBGPHz9Genq6THliYiJGjhwJY2NjlJSUwMDAAKtWrcJXX32FuXPn4vjx41i/fn1bvXWss2fPIiIiAv369cPOnTsxd+5cJCQk4N1332XrxMbGIiEhAfPmzcPevXvx/vvvo2fPnuwFcUvb5bl37x4KCgrg5uamUJwbN25E7969sXPnTsyePRsAIJFIUF9f3+wfsVjM9pGdnQ2g4WL9aba2tnjw4EGzdzays7MbtRMIBDAxMWH7lf5tbW3dqH+RSMQOk8rOzoa1tTU4HI5MPRsbG7aP1uByuRg8eDAuXbrU6rYdSU3VAZC2YWhoCF9fX5w5cwZpaWmwsrKCsbGx3Lp8DTXwNdRQUy3Co4Iy9O3Xo4OjbTtcdR7MJjmg/K9CGHu8GONMCSGku2EYBgkJCXjw4IHKYjA3N8e0adMaXQg2x8PDA0ZGRkhKSoK7uzsA4Pbt27h9+zbmzJkDoGFy9IoVK9g2bm5u0NLSwsqVK7Fu3TpoabXdfMSYmBi4uLjgk08+AQB4e3tDS0sL69atQ1ZWFuzt7ZGZmQlPT0/MmDGDbff0nYOWtsuTmZkJoOFYFeHj44PIyEjweDy2bPXq1Th+/Hiz7SwsLHDu3DkAQHl5Ofh8PjQ0NGTqCAQCMAyDsrIyaGpqyu2nvLwcAoGgUbm+vj7KyhqWuZf+/Ww96Wvp9vLycujp6cnt66+//mr2eJoyYMAAHDp0SKm2HYUSiG7E2dkZ//zzD3Jzc3H69GlMnz5d5ofzaWaW+igrqcGDvK6dQAAA30gLPbz7qjoMQgghLxg1NTWMHTsWSUlJWLduHfh8PpKSkqClpYWAgAAADcnRgQMH8M033yA/P19mOFFeXh7s7OzaJJaqqircunVLJlkBGobcrFu3Dn/88Qfs7e0xcOBA7NmzB9HR0fDx8YGjo6PM8JqWtstTVFQELpcLQ0NDhWL19PRsVLZo0SKZpEUePp+vUP9dnaGhIUpKSiASiaCurq7qcOSiBKIb4XA4CAwMxIEDB1BYWIj09HR2gvWzzHsb4O/MR13+gXLPYsQSPLl0H/pOvaBuIP+bB0IIIZ0Lh8PBtGnTZFat6WhqamqtuvsgFRQUhMOHDyMtLQ3+/v5ITEyEn58fdHR0AAAHDhxAVFQU5s6di+HDh0MgECAzMxMbNmyQSSaeV0VFBRiGaTT6QE9PD3w+n/3G/O233waXy8Xx48cRExMDIyMjzJgxA+Hh4eBwOC1ul0coFLbq/ZM3QsLc3By9evVqtt3T/QsEAtTV1UEoFMrchSgvLweHw4G+vn6T/QgEArlzOsrKyth20r8rKipgYmIi0//T2wUCAR49etRsX60lTZSEQiElEKRj6Orqwt/fH0lJSbh8+TJsbGzk/kBKHyj3ML+0gyNsX0W/5KAs4yGqc0pg+cZgcNVomg8hhHQFHA6n014sNcfNzQ0WFhZISkqCsbEx8vPzsWbNGnZ7cnIy/Pz8EBkZyZbdvXu3zePQ09MDh8NptHpPRUUF6urq2ItZPp+PxYsXY/Hixbh37x6+++47REdHw9LSEi+//HKL2+XR19eXezHfFHmJRmuHMEnnMOTk5GDAgAFsnezsbJibmzc5fEna9tn5CRUVFSgqKmL7lf797HyJ7OxsqKuro3fv3my9X3/9FQzDyBxXTk6O0neXysvLoa6uDl1dXaXadwS6uuqGBgwYAHt7ezAMg1OnTkEkajxRWppAFP9b3aUnUj/LcJgleFpqED6uQtG5tv8FTQghhDyNw+EgKCgI586dwzfffAMDAwN4eXmx22traxslRu2xwo6Ojg4cHByQnJwsU3769GkADav7PMvKygrLli2DgYGB3Am/LW2Xkk40fp4HoC1atAhHjx5t9s8XX3zB1ndzc4Ouri57fEDDClIpKSnw9vZudl/e3t64dOmSzNL3ycnJ4HK57GpKvXv3Rt++fRu9n6dOnYK7uzt7l8Db2xtlZWX49ddf2To5OTm4efNmi3E0paCgoNHk7c6G7kB0U/7+/sjPz0dJSQnS0tLg5+cns11bhw8DIy2UFtfgYX4ZrPt37XkQUup6GjCdYI8HR2+g/HohtMwFEDiaqjosQggh3VhQUBBiY2Nx7NgxBAcHyyQMI0eORFxcHA4ePIi+ffvihx9+wL1795TeV1FRUaOLWgDw9fXFokWLEB4ejuXLl2PSpEnIycnB9u3bERgYyE5wXrhwIQYNGoSBAwdCS0sLP//8M8rKyjBixAiFtsvj7OwMNTU1/PXXX7C1tVXquCwtLWFpaalwfQ0NDYSFhSE6OhpGRkaw+z/27js8zvLM+/73vqfPSKPRqHfZlmzLttwLrhiwDaaEQMAYCJCQQhIn2ZRnN3l3s9nspmwe8mxJIclmk9ACGNMxYLDBYBsbsA3Ylrtc1XvX9PL+MdJIY0mukmYknZ/j0KEp19xzSh6P7t9cbeJEnnnmGVpaWsIT2AF2797NF77wBX7xi1+Ee1DWrl3Lk08+ybp163jooYeora3l4YcfZu3ataSl9ZwzfOtb3+L//J//Q25uLgsWLOCNN97gwIED/O1vfwu3mTVrFkuWLOEf//Ef+cEPfoDBYOC//uu/mDRpUsRO5U1NTezevTt8ubOzM/zvePXVV0dMpj948GC/gS+WSIAYpUwmE9dffz0vvvgin376KRMmTCAvLy+izYx5OXjcPuKsF+5uHEks+YnYF+fStLOMurdPYkiLw5BiiXZZQgghRqmJEycyadIkjh07xi233BJx37p162hubuY3v/kNEFrR6Ec/+hFf+9rXLuu5Dh061O/eCdu2bQtvYvbII4/wjW98A5vNxpo1ayKGT82ePZtNmzbx6KOP4vf7GTduHP/v//2/8JzJC93fH7PZzNKlS9m+fTu33nrrZf1cl+MrX/kKwWCQv/71rzQ1NVFUVMRf/vKX8PAiCE1i9/v9EXt1JCQk8Pjjj/PTn/6UdevWYbFYuOOOO/jud78bcfybb74Zp9PJ//7v//KnP/2JcePG8bvf/Y5Zs2ZFtPvv//5v/v3f/50f//jH+Hw+lixZwo9+9CO02p7T7NLS0j7/bt3X33nnnXB4amxs5NChQ3zve98bnF/SEFGCl7Pw8RjQvSRZcXFxVJ7f4XBw5MgRioqKMJvNl32ct99+m/379xMXF8cDDzxw3jGBo0kwGKTqhUM4zrSgSzSS8/mZaAyjPy8P1utGjC3yuhGX43JfNy6Xi9OnTzNu3Lgx8zdpLNi6dSvf//732bVr13mXpvX7/bhcLoxG44ArRY5lTz31FI899hibN2++rEn953Oh/3uXcu4rcyBGuauvvhqbzUZHRwfvvvtutMsZNoqikH7jJLTxBnwdXjwNjmiXJIQQQoxa11xzDePGjeO5556LdikjViAQ4IknnjjvilexQgLEKKfT6bjhhhtQFIXDhw9TWloacb/T4eHU8Xo87ugtnTdUNGYdGbcWkXvfDExZfTeMEUIIIcTgUBSFn/zkJ4O6Md5YU1dXx2233cZnPvOZaJdyQRIgxoCsrCzmzZsHwJYtW+js7Azf9+f/fp+//c9HVJxtiVJ1Q8uYHofe3tO1LiP2hBBCiKExffp07rzzzmiXMWKlp6fzta997YIb98WC2K9QDIpFixaRkpKC0+lk8+bN4RPp0bofRH8cZS2U/20/PsfoWbZWCCGEEGK4SYAYIzQaDatXr0aj0XDq1CkOHjwI9A4Qo2tH6nMFA0Hq3zmFu7aD2jeOEQxIT4QQQgghxOWQADGGpKSkhDdI6V7XOSPbBoz+AKGoCuk3T0LRqjjOtND0YXm0SxJCiDFNhpQKMbwG8/+cBIgxZs6cOWRlZeH1ennzzTdJz4oHoLnRgdPhiXJ1Q8uQYiF1ZQEATbvK6DzTHOWKhBBi7OneZM3hkNXxhBhO3f/nzt0Z/XKM/oXxRQRVVbnhhht44oknqKio4PCREhKTzDQ3OqiuaGX8xJRolzikrFNTcVa10ba/hprXj5F73yx0o2wjPSGEiGUajQabzUZdXR0Q2oQs1pesFIPH7/fjdrsBZB+IYRIMBnE4HNTV1WGz2Qbl9y4BYgyy2WwsX76cLVu28P7775OaPpvmRqgqH/0BAiDlmvG4a9px13ZS89pRsu8qRtFIZ5wQQgyX9PR0gHCIEGNHIBDA5/Oh1WpHxGpDo4nNZgv/37tSEiDGqOLiYk6cOMHp06dx+E+y+vblYyI8AKhalYxbiih78lN0CUaCgSCKfAgihBDDRlEUMjIySE1NxeuVlfHGEqfTyalTp8jNzZU9I4aRTqcb1B4fCRBjlKIorFq1iscff5zWtiY8VJGUMi7aZQ0bnc1I7v2z0FoN0nUuhBBRotFoZBjLGBMIBAAwGAwYjcYoVyMul/QdjWFxcXGsXLkSgI8++oiqqqooVzS8dAnGcHgIBoP4ZX8IIYQQQogLkgAxxk2cOJGioiKCwSAvv7SRqvKmaJc07PwuH9UvH6FiQwkBrz/a5QghhBBCxDQJEIJrr70WjWrA6epg69b3ol3OsAv6Ariq2/E0OKjbclLWJhdCCCGEOA8JEAKj0cjE8XMAqG04zZkzZ6Jb0DDTxulJv3kSKNB+uI62ktpolySEEEIIEbMkQAgAiqYW4u20AfDWW2/hcrmiW9AwM+faSFqSB0D9Oydx1XZEuSIhhBBCiNgkAUIAkJGVgKcthYBPR0dHB1u3bo12ScMucX42lgl2gv4gNa8exe/yRbskIYQQQoiYE3MB4uTJk3zxi19k5syZLF68mIcffhiPx3NRj62treUHP/gBV111FdOnT2f16tW8+uqrQ1zx6GA06bAnxeNuzgAUjhw5wrFjx6Jd1rBSFIW01RPRWg14W13UbT4R7ZKEEEIIIWJOTO0D0draygMPPEB+fj6//e1vqa2t5Ze//CUul4sf//jH531sXV0dd911F+PGjeOnP/0pcXFxlJaWXnT4EJCZk0BTQyeZqYVU1R3n7bffJisri7i4uGiXNmw0Ri0Znymi5vWjJM7LinY5QgghhBAxJ6YCxPr16+ns7OR3v/sdNpsNAL/fz7/+67/y0EMPkZaWNuBjf/WrX5Gens6f//zn8KY0CxcuHI6yR42M7AQOflqF6ksjNbWFuro6Nm/ezG233TamNlszpseR98U5KOrY+ZmFEEIIIS5WTA1h2r59OwsXLgyHB4DVq1cTCATYuXPngI/r6Ohg06ZN3HPPPbKj5RUomp7B5x+6is/cNZPVq1ej0Wg4ffo0JSUl0S5t2PUOD+66Dnyd0pMlhBBCCAExFiBOnTrF+PHjI26zWq2kpKRw6tSpAR936NAhvF4vWq2Wz3/+80ydOpXFixfzq1/9Cq9Xdhe+WDa7mfETkzGZ9SQnJ7NkyRIA3nvvPVpaWqJbXJR0lDZQ/vQBal47RjAg+0MIIYQQQsTUEKa2tjasVmuf2xMSEmhtbR3wcQ0NDQD86Ec/Ys2aNXzzm9/kwIED/OY3v0FVVb7//e9fVj3BYBCHw3FZj71STqcz4ns0FBUVUVpaSlVVFa+//jq33norqhpTmXPI+c2hn9dZ3krNeydIuCq250XEwutGjDzyuhGXQ1434nLI6yZ2BYPBix6yHlMB4nIFAgEAFi1axA9/+EMArrrqKjo7O/nrX//KunXrMBqNl3xcr9fLkSNHBrXWSzXcm7o113uoqXBhtenIGmciJyeH2tpaampq2LJlC7m5ucNaTyzQTtRgPhyg45Na6rzN+JJjf5jcWNsMUAwOed2IyyGvG3E55HUTm/R6/UW1i6kAYbVaaW9v73N7a2srCQkJ530chEJDbwsXLuSPf/wjZ8+eZdKkSZdcj06no6Cg4JIfNxicTidnzpwhPz8fk8k0bM+7t6mMU0dOUFCUTFFREQAmk4n33nuPM2fOMHfuXJKSkoatnphQBC2acjpL6rEc95M6oxCt1RDtqvoVrdeNGNnkdSMuh7xuxOWQ103sOnHi4pevj6kAMX78+D5zHdrb26mvr+8zN6K3C53ku93uy6pHURTMZvNlPXawmEymYa0hb3wKcIK66o7w886ePZuysjJOnTrFu+++yz333INWG1MvnSFnWlFIRYMLV3U7LVvOkn33dFRt7A7nGu7XjRgd5HUjLoe8bsTlkNdN7LmUFTdj6gxo2bJl7Nq1i7a2tvBtb775Jqqqsnjx4gEfl5WVxcSJE9m1a1fE7bt27cJoNEatF2EkSs9KAAXaWlx0toeCl6IorFq1CpPJRH19PR988EGUqxx+ikYl/ZZJqEYt7toO2g7URLskIYQQQoioiKkAsXbtWiwWC+vWreP999/nhRde4OGHH2bt2rURe0A88MADrFy5MuKx3/3ud9m6dSs///nP2blzJ3/84x/561//yhe+8AVJuJfAYNSSnBLaOK6qomfiusViCf/O9+zZQ2VlZVTqiyad1Uj6TZOwL8whYWZGtMsRQgghhIiKmAoQCQkJPP7442g0GtatW8d//Md/cMcdd4QnRncLBAL4/f6I26699lr+8z//kw8++ICHHnqIDRs28K1vfYvvfOc7w/gTjA4Z2aH5JtUVLRG3FxYWMmXKFILBIG+++eaY3OXbMi6RpMV5ssmcEEIIIcasmBvIPmHCBB577LHztnnyySf7vf3GG2/kxhtvHIKqxpaMnARKPqmkurzv0rnXXHMN5eXltLS0sH37dlasWBGFCmNDwBeg+aNyEudlo+pjf2UmIYQQQojBEFM9ECI2dPdAtDT13QPDaDRy/fXXA7B//35Onz49rLXFkpqNR2n6oJzazaUEg7LJnBBCCCHGBgkQoo+sXBvf/qdr+er3l/V7f15eHrNmzQLgrbfeGrObwSTOzwYFOo420LpPJlULIYQQYmyQACH60Go12Ozm8y7ntXTpUhITE+ns7OSdd94ZxupihynLSvLV4wCof/cUruq+e5gIIYQQQow2EiDEZdHpdNx4440oisKxY8c4evRotEuKCtucTCyFSRAIUv3qUfxOb7RLEkIIIYQYUhIgRL8qy1p49tE9vPbcgQHbpKenh3f/fuedd+jo6Biu8mKGoiik3VCIzmbE1+6m5o3jMh9CCCGEEKOaBAjRL78vwLGDtZQerj1vuwULFpCWlobL5eKtt94akyfPGoOWjM9MRtGquKrb8Ta7ol2SEEIIIcSQkQAh+pWeZUVRoL3NTXvbwCfEGo2G1atXo9FoOHPmDAcODNxjMZoZUuNIv3kSuffNRG83RbscIYQQQoghIwFC9Etv0JKcGtqRurqi734QvSUlJbF06VIA3nvvPZqbm4e8vlgUV5CELsEY7TKEEEIIIYaUBAgxoIwcG0C/G8qda/bs2eTk5ODz+di0aROBQGCIq4ttnSebqHrpMEH/2P49CCGEEGL0kQAhBtS9oVxVRcsF2yqKwg033IBer6e6upo9e/YMcXWxy+/yUfPGMTpPNtGw42y0yxFCCCGEGFQSIMSAMrsCxIWGMHWzWq1ce+21AOzatYu6urohqy2WaYxa0m4oBKBlbyUdpQ1RrkgIIYQQYvBIgBADSs9KQKfXkJBowuP2XdRjpkyZQkFBAYFAgE2bNuHzXdzjRpu4wmRsc7MAqN1UiqdlbO7WLYQQQojRRwKEGJBOr+EHP7+BL317CXqD9qIeoygKK1euxGQy0dDQwK5du4a4ytiVvDQPY5aVgMdPzatHCXj90S5JCCGEEOKKSYAQ56WqyiU/xmw2s2rVKgD27NlDRUXFYJc1IigalYybJ6Ex6XDXdVK/9VS0SxJCCCGEuGISIMRF8V/iakIFBQVMnToVgDfffBOPxzMUZcU8bbyB9JsngQIoCsHA2NtoTwghhBCjyxUFiKqqKvbu3Rtx29GjR/mHf/gHvvOd7/D2229fUXEi+pobO/njr7bxm59vveRdpq+55hri4+NpbW1l27ZtQ1Rh7DPn2ci9fxZpqwpQLqNHRwghhBAillxRgPjZz37G7373u/D1hoYG7r//frZs2cLevXv51re+xebNm6+4SBE9cfFG6mvbaW91nXdH6v4YDAZWr14NwIEDBzh1auwO4TGkWMKXg4GgzIcQQgghxIh1RQHiwIEDLFq0KHz95ZdfxuVy8corr7B9+3YWLlzIX//61ysuUkSPTq8hJT0euLgN5c6Vk5PDnDlzAHjrrbdwOByDWt9I4+v0UPnCIWrfOH7JPTpCCCGEELHgigJEa2srSUlJ4evvvfce8+bNIzc3F1VVWbly5Zj+1Hm06NlQ7tIDBMDixYux2+04HA7eeeedMX3i7Gtz4yxvpaO0kZZPqqJdjhBCCCHEJbuiAGG326mqCp0EtbW1sW/fPpYuXRq+3+/3j9l9AEaTzGwbcPEbyp1Lp9Nx4403oqoqx48f5+jRo4NY3chizIgn5ZpxADRsO4Ozsi3KFQkhhBBCXJorChCLFi3iySef5NFHH+Uf/uEfCAaDXHfddeH7T5w4QUZGxhUXKaIrI6drR+rylsvuPUhLS+Oqq64C4J133qG9vX3Q6htpEmZmEDc5GQJBajYexefwRrskIYQQQoiLdkUB4vvf/z7jx4/n//7f/8vOnTv5h3/4B3JycgDweDxs2rSJhQsXDkqhInrSMq0oqkJnh4f21kubSN3bggULSE9Px+1289Zbb43ZoUyKopC2qgCd3YSvw0Pt68dkeVchhBBCjBgXt73wAJKTk1m/fj3t7e0YDAb0en34vkAgwOOPP056evoVFymiS6fTMGlqGjqdBp/v0vaD6E1VVVavXs2TTz7J2bNn2bdvH7NmzRrESkcOVa8l4zOTKf/bfhxnW2j6sJykRbnRLksIIYQQ4oIGZSO5+Pj4iPAAYDQamTx5MjabbTCeQkTZmi/M5bZ7Z2FPtly48XnY7fbwPJnt27fT1NQ0GOWNSIZkC6mrCtDZjMQVJl34AUIIIYQQMeCKAsQHH3zAn//854jbnn/+eZYvX86iRYv4xS9+gd8v692LSLNmzSI3Nxefz8ebb75JIHD5vRojnXVKKrlfmB2xT4QQQgghRCy7ogDx29/+NmJFnWPHjvEv//Iv2O125s+fz5NPPslf/vKXKy5SxIZAIEh9bfsVz11QFIXrr78eg8FAdXU1u3fvHqQKRyZV2/Pf0FXbQdA/dgOVEEIIIWLfFQWIkydPMm3atPD1V155hbi4OJ566in++7//mzvvvJNXXnnliosU0ef3B/h/P97MHx7eRlvL5U+k7ma1Wrn22muBUE9WbW3tFR9zpGs9UEP5U/tp2HY62qUIIYQQQgzoigKE0+kkLi4ufH3Hjh0sWbIEk8kEQHFxcXifCDGyaTQqCYmhf9fqipZBOWZRURGFhYUEAgE2bdo05vcM0Vj0EAjS8kk17Ufro12OEEIIIUS/rihAZGRkUFJSAsDZs2cpLS1lyZIl4ftbW1v7TK4WI1fmFe5IfS5FUVixYgVms5nGxkbef//9QTnuSBU3wU7igmwAat86gafJEeWKhBBCCCH6uqIAccstt7Bhwwa+9rWv8aUvfYmEhISIjeQOHTpEfn7+ldYoYkTPhnKDEyAAzGYzq1atAuDjjz+mvLx80I49EiUtzsOUk0DQ66f6laMEPLIIgRBCCCFiyxUFiK997Wt89atfpaamhoyMDB555BGsVisALS0t7N69OzzOXYx8Gdk2AKorWgd1E7gJEyaE59K8+eabuN3uQTv2SKOoCuk3T0Jj0eFpdFD39okxu+GeEEIIIWLTFW0kp9Vq+e53v8t3v/vdPvfZbDZ27tx5JYcXMSYtIx5VVXB0emhtdmKzmwft2MuXL6esrIy2tjbee+89rr/++kE79kijtejJuHkyFRtKaD9cT/zkFCzj7dEuSwghhBACGKSN5AA6Ozs5efIkJ0+epLOzc7AOK2KIVqchNSMeCPVCDCaDwcDq1asBOHjwICdPnhzU4480ppwEkpflk7wsH/O4xGiXI4QQQggRdkU9EAAHDhzgV7/6FZ988kl4QzBVVZkzZw5///d/T3Fx8RUXKWLHzPk5ODo9JKfGXbjxJcrOzmbu3Lns3buXzZs388ADD2A2D14vx0iTOC872iUIIYQQQvRxRQFi//793Hfffeh0Ou644w4mTJgAhPaHeP311/n85z/Pk08+yfTp0welWBF985eMG9LjL168mNOnT9PY2MiWLVv4zGc+g6IoQ/qcI0HA46f1QA22OZny+xBCCCFEVF1RgPiv//ov0tLSePrpp0lJSYm471vf+hZ33303//Vf/8Wjjz56RUWKsUOr1bJ69WqefvppTpw4wZEjR5gyZUq0y4qqYCBIxfoDuOs6CQaC2OdLz4QQQgghoueK5kDs37+fu+66q094AEhOTmbNmjXs27fvSp5CxKD2NhfHDtXi6PAMyfHT0tJYuHAhAO+88w5tbW1D8jwjhaIqJMzIAKBxxxkcg7iMrhBCCCHEpbqiAKGqKn7/wOvUBwIBVHXQ5mmLGLH+L3t49q97OHuqccieY/78+WRkZODxeHjrrbfG/FKm1ulpxE9JgSDUvHYUX+fQhDchhBBCiAu5orP7WbNm8dRTT1FZWdnnvqqqKp5++mlmz559JU8hYlBG947U5S1D9hyqqnLDDTeg1WopKyvj008/HbLnGgkURSF1RQH6ZDP+Ti81rx0jGBjboUoIIYQQ0XFFcyC+973vce+997J69WpWrlwZ3nX69OnTvPPOO6iqyve///3BqFPEkO4AMdhLuZ7Lbrdz9dVX884777Bjxw7y8vJISkoa0ueMZapeQ8ZnJlP25H6c5a007jxL8tL8aJclhBBCiDHminogpkyZwnPPPcfSpUvZunUrjzzyCI888gjvvvsuS5cu5ZlnniExUdawH216eiAGd0fq/syYMYO8vDx8Ph+bNm0675C5sUBvN5N2fQEAbQdr8bt8Ua5ICCGEEGPNFe8DUVBQwCOPPEIgEKCpqQkIfXKsqip/+MMf+M1vfsORI0euuFARO1Iz4tFoVFxOLy1NThKThm6vBkVRuP7663n88cepra1l9+7d4QnWY1X85BT8Th9xhUlojFf8X1gIIYQQ4pIM2gxnVVVJTk4mOTlZJk6Pclpt7x2pW4b8+eLj47nuuusA+PDDD6mpqRny54x1tlkZaOP00S5DCCGEEGOQnOmLy5KZ0zOMaThMnjyZiRMnEggE2LRpE16vd1iedyRoP1pP/Xuno12GEEIIIcYIGf8gLsv0uTnkjLOTk28fludTFIUVK1ZQWVlJU1MTO3bs4Nprrx2W545lnkYHNa8dA8CQakGbHxflioQQQggx2kkPhLgsOfmJTJ+TPaTzH85lMplYtWoVAJ9++ik7duwY8/tD6JPM2BfmAFC3+QTeJmeUKxJCCCHEaHfJPRCHDh266LZ1dXWXenghzmv8+PEsW7aM7du3s3v3bpxOJytWrBjT827sC3NxVbXjONtC05unoFiJdklCCCGEGMUuOUB87nOfQ1Eu7gQlGAxedFsx8lRXtHL2ZCPZ+Ylk5w3fcr3z5s3DYDDw9ttvU1JSgsvl4sYbb0SrHZsj8hRVIe2mSZQ/8Sm+FjemoyrBKWO7Z0YIIYQQQ+eSz7j+/d//fSjqECPQJx+W8fEHZ1l0zYRhDRAA06dPx2g08sYbb1BaWspLL73Erbfeil4/Nlcm0pp1pN8ymYpnD6CrD9D4+gnMa6ajqBLghRBCCDG4LjlA3HbbbUNRhxiBejaUa4nK80+cOBGDwcArr7xCWVkZzz33HLfddhtm8/DNy4glpiwr9hsm0PjWSQxZ8RIehBBCCDEkxu7AcXHFupdyra4Y+h2pB5KXl8eaNWswGo3U1NTw7LPP0t7eHpVaYoEpP4GOeQbiZqeFbwsGZDiTEEIIIQaPBAhx2VLS49FoVdwuH82NjqjVkZ6eztq1a4mLi6OpqYlnnnkmvCv6WBQ0qeG5RwGPn/Kn99N2sDbKVQkhhBBitJAAIS6bRqOSlmkFoHqYNpQbSFJSEnfffTeJiYm0t7ezfv16amvlpLl1fzXumg5q3yylYceZMb/srRBCCCGunAQIcUUyu+dBVLREtxDAarWydu1a0tLScDqdbNiwgbKysmiXFVW2uVkkXhXaJ6L5owpqXj1KwOuPclVCCCGEGMkkQIgr0j2Ruroiuj0Q3cxmM2vWrCEnJwePx8OLL75IaWlptMuKGkVRSF6SR9rqQlAVOkobqVhfgq/DE+3ShBBCCDFCSYAQV2TilDS++K3F3P2l+dEuJUyv13P77bdTUFCA3+9n48aNHDx4MNplRZV1ahrZa6ahmrS4azsof2of7vrOaJclhBBCiBFIAoS4IpZ4Azn5iej0mmiXEkGr1XLLLbcwbdo0gsEgb731Fnv27Il2WVFlyk4g594Z6OwmggFQDWNz4z0hhBBCXBk5gxCjlqqqrFq1CqPRyN69e9m+fTtOp5OlS5eO2R3S9TYTOffMwNfuRmc1RLscIYQQQoxA0gMhrlj5mWbeeKGE3e+fjnYpfSiKwtVXX83SpUsB2LNnD1u2bCEQCES5sujRGLUYUizh6+3HG6jdfIKgf+z+ToQQQghx8SRAiCvWWNfB3l1nOXKgOtqlDGj+/PmsWrUKRVEoKSnhtddew+fzRbusqPM7vdRuKqXtQA2VLxzC75LfiRBCCCHOTwKEuGI9KzG1xfSux8XFxdxyyy1oNBpKS0t56aWX8HjG9mpEGpOOjJsnoeg0OMtaKX96P55mZ7TLEkIIIUQMkwAhrlhKWhxarYrH7aOpMbZX9iksLOT2229Hp9NRVlbGc889h8MRvV20Y4Flgp2ce6ajjTfgbXJS/tR+nFHeGFAIIYQQsUsChLhiqkYlLSu0I3XVCDjxzM3NZc2aNRiNRmpqanj22Wdpa2uLdllRZUixkPP5GRjS4wi4fFQ8d5C2g7KTtxBCCCH6kgAhBkVmtg2InQ3lLiQ9PZ21a9cSFxdHU1MT69evp7GxMdplRZXWoif7rmLiJiVDICj7RAghhBCiXxIgxKDongdRVd4S3UIuQVJSEnfffTeJiYm0t7fz7LPPUlNTE+2yokrVaUi/eRJpqyeSfPW4aJcjhBBCiBgkAUIMisycUIBwOrwEg7E7kfpcVquVtWvXkpaWhtPpZMOGDZSVlUW7rKhSFAXr1FQUNbRXRsAXoO7tk/g63FGuTAghhBCxQAKEGBTJafF8/ycr+frfXz3iNmkzm82sWbOG3NxcvF4vL774IqWlpdEuK2Y0bj9D675qyp/aj7uuI9rlCCGEECLKJECIQaGqCpb4kbuzsV6v57bbbqOgoAC/38/GjRspKSmJdlkxwTY7E53dhK/dQ/kzB+g4MbbnigghhBBjnQQIIbpotVpuueUWpk2bRjAYZPPmzezZsyfaZUWdzmYk554ZmPNsBL0Bql8+QvOeihE1VE0IIYQQg0cChBg01RWt/O1/PuLZR0fuSbeqqqxatYp58+YBsH37drZv3z7mT5Y1Ri2Zt08hYUY6AA3bzlC3+QRBfyDKlQkhhBBiuEmAEINGo1U5dbyeU8cbCMTwjtQXoigKy5YtY+nSpQDs2bOHLVu2EAiM7ZNlRaOSsmICKdeOBwU6jjfgbZeJ1UIIIcRYo412AWL0SE6NQ6fX4PX4aazvICUtPtolXZH58+djMpnYsmULJSUluFwubrzxRrTasfvfRlGU0JwImxFFo6K3maJdkhBCCCGGmfRAiEGjqgrpmaEdqUfKhnIXUlxczC233IJGo6G0tJSXXnoJj8cT7bKizjLejjnPFr7uKGvBUdYStXqEEEIIMXwkQIhBldG1H0R1+egIEACFhYXcfvvt6HQ6ysrK2LBhAw6HI9plxQxPs5PqV45Q+fwhWktqo12OEEIIIYaYBAgxqDKzbQBUjZIeiG65ubmsWbMGk8lEbW0t69evp62tLdplxQRtnB5zfiIEgtS9VUrD9jNjftK5EEIIMZpJgBCDKiM71ANRU9k6oidS9yc9PZ277rqL+Ph4mpubWb9+PY2NsieCqtOQfvMk7FflANC8u4LqV48S8PijXJkQQgghhoIECDGoklLjiE8wkpVrw+kYfXMFkpKSWLt2LXa7nfb2dp599llqamqiXVbUKYpC0pI80m6ciKJR6CxtpOLZEnwdskqTEEIIMdpIgBCDSlUVvvPP13H/1xdiiRu5O1Ofj9Vq5a677iItLQ2n08mGDRsoKyuLdlkxwTollaw1xWhMWty1HTTvqYx2SUIIIYQYZBIgxKBTFCXaJQw5s9nMmjVryM3Nxev18uKLL1JaWhrtsmKCKctKzr0zsU5LI2lpfrTLEUIIIcQgkwAhhozH7Yt2CUNKr9dz2223UVhYiN/vZ+PGjZSUlES7rJigsxlJu6EQVRt6iwkGg3Qcb5DJ1UIIIcQoEHMB4uTJk3zxi19k5syZLF68mIcffviS191/7LHHmDRpEg899NAQVSnOp73NxW9/sZX/+MmWUTeR+lxarZabb76ZadOmEQwG2bx5M3v27Il2WTGncfsZql89St3mEwT9Y3tHbyGEEGKki6kA0draygMPPIDX6+W3v/0t3/3ud9mwYQO//OUvL/oY9fX1PPLIIyQlJQ1hpeJ8LHEGOjvceD1+Guo6ol3OkFNVlVWrVjFv3jwAtm/fzvbt2+XT9l608QZQoK2klsrnD+F3eqNdkhBCCCEuU0wFiPXr19PZ2cnvfvc7li5dyh133MHf//3fs379emprL26Dql/96ldce+21TJgwYYirFQNRVYX0rO4N5VqiW8wwURSFZcuWsWzZMgD27NnD5s2bCQTk03YA2+xMMj87BUWnwVneSvnTB/A0O6NdlhBCCCEuQ0wFiO3bt7Nw4UJsNlv4ttWrVxMIBNi5c+cFH793717efvttvv/97w9hleJidO8HUT3KNpS7kHnz5rFq1SoUReHgwYNs3LgRn290zwW5WJYJdnLumY423oC32Un5U/txlLVEuywhhBBCXKKYChCnTp1i/PjxEbdZrVZSUlI4derUeR/r9/v56U9/yte+9jVSU1OHskxxETK7AkRV+dgKEADFxcXccsstaDQaTpw4wUsvvXTJ83hGK0OKhZzPz8CYEU/A5aP65SP4XRKwhBBCiJFEG+0Cemtra8Nqtfa5PSEhgdbW85+IPv300zidTr7whS8MWj3BYBCHwzFox7sUTqcz4vtIk5gc2gOipqqVjvYOVE1MZdUhl5WVxU033cSmTZsoKytj/fr13HTTTZhMpiF93hHxulHAfssEmt89i3GcDXfAA6Nw08GRZES8bkTMkdeNuBzyuoldwWDwopfij6kAcbkaGxv5zW9+w//9v/8XvV4/aMf1er0cOXJk0I53Oc6cORPV579cwWAQrVbB5w2wZ/dBrDZdtEuKiuLiYkpKSqivr+fZZ59lxowZGI3GIX/eEfG6yQa8NXAktJO32hkgYFRAM/r3EYlVI+J1I2KOvG7E5ZDXTWy62PPomAoQVquV9vb2Pre3traSkJAw4ON+/etfM2nSJObOnUtbWxsAPp8Pn89HW1sbZrMZrfbSf1SdTkdBQcElP24wOJ1Ozpw5Q35+/pB/aj1UKmaGeh0KC3Kwp1iiXE30TJw4kddee42Ojg5KSkq45ZZbSExMHJLnGqmvG3+Hh7oXjqExa0laPQFN3OB9ECAubKS+bkR0yetGXA553cSuEydOXHTbmAoQ48eP7zPXob29nfr6+j5zI3o7ffo0e/bsCS+j2du8efP43//93/DqOJdCURTMZvMlP24wmUymqNdwuT579+xolxATzGYzd999Ny+88AJNTU28/PLLfO5znyM9PX3InnOkvW5cbX6UAHjrndS/eJzM26ZgTIuLdlljzkh73YjYIK8bcTnkdRN7Lnb4EsRYgFi2bBl//OMfI+ZCvPnmm6iqyuLFiwd83D/+4z+Gex66/eIXv8BoNPK9732PSZMmDWndQlyI1Wrlrrvu4qWXXqKmpoYNGzZw6623kpeXF+3SYoIxPZ6ce2dQ9dJhPI0OKp45QPpNk4grlP1chBBCiFgTUzNb165di8ViYd26dbz//vu88MILPPzww6xdu5a0tLRwuwceeICVK1eGrxcVFbFgwYKIL6vVis1mY8GCBRHLworh5fP5qSpvISC7D2M2m7nzzjvJzc3F6/Xy0ksvUVpaGu2yYobOZiT7numY820EfQGqXzlC8+4K2ZBPCCGEiDExFSASEhJ4/PHH0Wg0rFu3jv/4j//gjjvu4Ic//GFEu0AggN/vj1KV4mIFg0H++9/e4c///T71taN/R+qLodfrue222ygsLMTv97Nx40ZKSkqiXVbM0Bi0ZN4+lYSZGQA0bD9D677qKFclhBBCiN5iaggTwIQJE3jsscfO2+bJJ5+84HEupo0YWoqikJIez9mTjVRXtJKW2XeJ3rFIq9Vy88038/bbb1NSUsLmzZtxOp3Mnz8/2qXFBEVVSF0xAb3dRNvBWqxTZV8XIYQQIpbEVA+EGH0ywhvKtUS3kBijqiorV64MT/zfsWMH27Ztk+E6vdhmZ5Jz7wxUfehzjmAwiK9T9osQQgghok0ChBhSmTmhAFFdMfZ2pL4QRVFYtmxZeIWwvXv3snnzZgIBmS/STem1AWHLnkrOPvoJjrKW6BUkhBBCCAkQYmh190DUVLXhl4nU/Zo3bx7XX389iqJw8OBBNm7ciM/ni3ZZMSUYCNJxopGAy0fl84doLamJdklCCCHEmCUBQgwpe5IFg1GL3xeQidTnMW3aNG655RY0Gg0nTpzgxRdfxO12R7usmKGoCll3TiNucjIEgtS9dYKGbadlyJcQQggRBRIgxJBSVCXcC1Et8yDOq7CwkNtvvx29Xk95eTnPPfccDocj2mXFDFWnIf2mSdgX5gDQvKeS6leOEPDIimxCCCHEcJIAIYbczPk5XHfTZLLzE6NdSszLzc1lzZo1mEwmamtrWb9+fZ9NEscyRVFIWpxH2k0TUTQKnSeaqNhQQjAgPRFCCCHEcJEAIYbc9DnZLL62gJS0+GiXMiKkpaWxdu1a4uPjaW5u5plnnqGxsTHaZcUUa1EqWWuK0Zh0WKeloahKtEsSQgghxgwJEELEILvdztq1a7Hb7XR0dLB+/XpqamTicG+mLCt5D87G1rXpHEDAK8OZhBBCiKEmAUIMi+ZGB4c+raK12RntUkYMq9XKXXfdRXp6Oi6Xiw0bNnD27NlolxVTNCZd+LLf6aXsyX00fVQhk6uFEEKIISQBQgyLjRv288LfPuHksfpolzKimM1m7rzzTnJzc/F6vbz00kuUlpZGu6yY1H60AW+Tk8YdZ6h7q5SgLBsshBBCDAkJEGJYZObYANlQ7nLo9Xpuu+02CgsL8fv9bNy4kQMHDkS7rJhjm5VByrXjQYG2g3VUPncQv9Mb7bKEEEKIUUcChBgW4aVcK1qiW8gIpdVqufnmmykuLiYYDLJlyxZ2794d7bJijm12Jpm3TUHVa3BWtFH+9H48TTJsTgghhBhMEiDEsMjMCQWI2qp2/D4ZWnI5VFVl5cqVzJ8/H4AdO3awbds2Ge9/Dst4O9l3T0drNeBtdlH+9H5c1e3RLksIIYQYNSRAiGFhs5sxmnT4/QHqauRk7nIpisLSpUtZtmwZAHv37mXz5s0EAhLKejOkWMi5dwbGjHhUvQat1RDtkoQQQohRQwKEGBaKosgwpkE0b948rr/+ehRF4eDBg2zcuBGfzxftsmKK1qIn665istcUo7XoAQj6A/jd8nsSQgghroQECDFsugNEVblMpB4M06ZN45ZbbkGj0XDixAlefPFFPB5PtMuKKapWRWczhq+37Kvm7J8/pmVftazSJIQQQlwmCRBi2BTPyeLOB+awbGVhtEsZNQoLC7n99tvR6/WUl5fz6quvSogYQDAYpONYA36nl/q3T3L28U/pONEoc0iEEEKISyQBQgybtAwrRdMzsNpM0S5lVMnNzWXNmjWYTCbq6+v55JNPZMO5fiiKQvZdxaRcNwGNSYu3yUn1y0eofLZEJlkLIYQQl0AChBCjQFpaGmvXriU+Ph6Xy8Ubb7zByy+/TGurDBfrTdGo2GZlkPfluSQuyEbRqqHlXp/aT/OeimiXJ4QQQowIEiDEsKoqb2HH26WyI/UQsNvtrFmzhpycHFRV5eTJkzz22GN88MEHMsH6HBqDluSl+eQ9OIf4KamgKpjzE6NdlhBCCDEiSIAQw+rIgWre3XSMIweqo13KqKTX65kwYQJ33nknOTk5+Hw+du3axeOPP87p06ejXV7M0VkNpN84kfwvz8WQYgnf3rDjDM0fV8pEayGEEKIfEiDEsMrMsQGhnggxdOx2O3feeSc33XQTFouFlpYWXnzxRV555RUZ1tQPXa99IjzNTpp3V9Dw7mnOPvoJ7ccaZKK1EEII0YsECDGsupdyratpx+fzR7ma0U1RFCZPnsyDDz7InDlzUFWVEydO8Nhjj/Hhhx/KsKYB6BKMpK4sQGPW4W1xUbPxKBXPHMBZ2Rbt0oQQQoiYIAFCDKuERBMms46AP0idrHwzLPR6PcuXL+e+++4LD2vauXOnDGsagKIqJExPJ//Lc7EvzEHRqriq2ql45gDVrxzB1+6OdolCCCFEVEmAEMMqtCO1DZAN5YZbcnIyd955JzfeeGOfYU1tbfLp+rlUvYakxXnkf3kO1uI0ABxnW1A08rYphBBibNNGuwAx9mTmJHDqeD3VFS1AXrTLGVMURaGoqIjx48fzwQcf8Mknn3DixAnOnDnDggULmDt3LlqtvC30po0zkHZ9IbbZmXianWjMOqBrY7qjDVgKk1C1EiqEEEKMHfJXTwy77nkQtVXyqXe0GAwGli9fzv333092dnZ4WNMTTzzBmTNnol1eTDKkWIifmBy+7jjdTM3rxzj7149pO1InE62FEEKMGfJRoxh24wqT+er3l5GSFhftUsa85ORk1qxZw9GjR9m2bRvNzc288MILFBYWsnz5cqxWa7RLjFnBQBBtnB5fm5va14/TsreK5OXjMOckRLs0IYQQYkhJgBDDzmjSkW7SRbsM0aW/YU2lpaWcPn2aq666ijlz5siwpn7EFSRhzrPR8nEVTR9V4K7toPLZEiwT7CQvy0efZI52iUIIIcSQkCFMQgigZ1jTfffdR1ZWFj6fj/fff1+GNZ2HqtNgvyqH/C/PIWFGOijQebKJ6lePypAmIYQQo5YECBEVFWebeXX9frZtPh7tUsQ5UlJSuOuuu1i9ejVmszk8rGnjxo2yWtMAtBY9qSsLyPvCbCwT7CQtyUVRFACC/gABr+x5IoQQYvSQACGioqPNxb495RwtqYl2KaIfiqIwZcoUHnzwQWbPno2iKBw/fpxHH32Ujz76CL9fToj7o08yk3nbFOIKeyZbt+6v4exfPqbtYC3BgPRKCCGEGPkkQIio6N4Lor6mHa98OhuzDAYD11xzTZ9hTY8//jhnz56NdnkxLxgM0naoDl+Hh9o3Syl7ch+dZ5qjXZYQQghxRSRAiKiw2oyY4/QEAkFZznUE6G9Y0/PPP8/GjRtpb5cdxQeiKArZd08neVk+qkGDp76TqucPUfn8Idz1ndEuTwghhLgsEiBEVCiKQmbXfhDVFbIj9UjQPazpi1/8IrNmzYoY1rR7924Z1jQAVauSOD+b/C/PxTY7E1QFx5lmyp74lJZPqqJdnhBCCHHJJECIqOkexlRdLgFiJDEajVx77bV8/vOfJzMzE6/Xy44dO3jiiSdkWNN5aEw6Uq4dT94XZxM3MQkAk+wZIYQQYgSSACGiJjOnuweiJbqFiMuSmprK2rVrueGGGzCbzTQ1NfH888/z2muvybCm89Anmsj4TBF5X5qDIcUSvr1x51laD9TIRGshhBAxT3aHElGTkZ0ACgSCEAgEUVUl2iWJS6QoClOnTmXChAns2rWLffv2cezYMU6dOsXChQuZPXs2Go0m2mXGJL3NFL7saXLS9GE5BKHl4yqSr87HPC4xvBSsEEIIEUskQIioiU8w8oOfXY/BKLtSj3Tdw5qmTZvGO++8Q1VVFdu3b+fgwYNcd9115ObmRrvEmKZLMJC8fBxNH5TjaXRQ9eJhTLkJJF89DmNaXLTLE0IIISLIECYRNYqiSHgYZbqHNV1//fWYTCaampp47rnnZFjTBSgalcQ5WaGJ1nOzUDQKzrJWyp/cR80bx/F1eqJdohBCCBEmAUIIMagURWHatGk8+OCDzJw5E0VROHbsGI8++ih79+6V1ZrOQ2PUkrJ8HHkPziFucgoAnScaZSiTEEKImCJDmERU1dW088YLJQT8QR789uJolyMGkdFo5LrrrgsPa6qurmbbtm0cPHiQa6+9VoY1nYcuwUjGzZNwzcnE2+pCYw711AWDQTpLG7FMsKNo5PMfIYQQ0SF/gURUGU1ayk41UVnWjNcjn0yPRmlpadx9993hYU2NjY0899xzvP7663R0dES7vJhmzIgnvqsnAsBxpoXqV49y9rFP6ShtJBiUFZuEEEIMPwkQIqrirUbi4g0Eg1AjO1KPWr2HNc2YMQNFUTh69Ch//etfZVjTJQh4/GhMOrzNTqpfOULFsyW4qmVuiRBCiOElAUJElaIooeVcgerylugWI4ac0WhkxYoV3HvvvWRkZOD1etm2bRtPPvkk5eXl0S4v5sVPSibvy3NIvCoHRaviqmij/Kn9VL92FG+LK9rlCSGEGCMkQIioywhvKCc7Uo8V3cOaVq1aFR7WtGHDBt544w0Z1nQBGoOW5CV55H1pDvFTUwHoONpA1cuHZUiTEEKIYSGTqEXUZWTbAKiSADGmKIpCcXExBQUF7Ny5k/3793PkyBFOnjzJokWLmDVrFqoqn3EMRBdvIH31RBLnZFK/7Qy2Genh1ZqC/gDBIKha+f0JIYQYfPLXRURdZtcQpobadjxuX5SrEcPNZDKFhzWlp6fj8Xh47733ePLJJ6moqIh2eTHPkBpH1h1TsRQmhW9r3V9D2aOf0H6sQXolhBBCDDoJECLq4hOMpGdZKZicisvpjXY5IkrS09O55557WLlyJUajkYaGBp599lneeOMNOjs7o11eTFMUpaf3IRik9UAN3lYXNRuPUvH0AZyVskCBEEKIwSNDmERM+Or3lkW7BBEDFEVh+vTpFBYW8v7773PgwIHwsKbFixczc+ZMGdZ0AYqikHPPDJr3VtK8pwJXdTsVzxzAUphE8rJ89ImmaJcohBBihJO/xDEq4PcTCMjSlmJsMplMrFy5MmJY07vvvivDmi6SqteQtCiX/C/NxTo9DRToLG3k7KOf0Lq/JtrlCSGEGOEkQMQgv9/H87//V/ZveQZn59ha472zwx3tEkQMGWhY06ZNm2RY00XQxulJW1VI7gOzMI9LhEAQY2Z8tMsSQggxwkmAiEGqqkGr0+Nqb2Hrc/+L3zf65wW4XV7+81+38B8/2SITqUWE7mFNDz74INOnTwfg8OHD/PWvf+WTTz4hEAhEucLYZ0i2kPW5qeR9cTaGFEv49qYPy2k7XCcTrYUQQlwSCRAxSFEUrr3jy2h0emrLT/Ley0+M+j/wBqMOBSAINTLhU/Sje1jTPffcQ1paWnhY09/+9jcqKyujXd6IoE8yhy97mpw07iqj9o3j1D9/FG2jn2BgdL/PCCGEGBwSIGKULTmdwgXXoygKRz5+n/07N0e7pCGXkWMDoLqiJap1iNiWkZERMaypvr6e9evX8+abb8qwpkugjdeTtDgXVa/BW+/EfMBDzRMHqX/vNO66jlH/oYUQQojLJwEihiWm5zF/5e0A7HzjWc4eK4lyRUMro2s/CNlQTlyIqqpMnz6dL37xixQXFwNw6NAhHn30UT799FMZ1nQRVJ0G+4Ic8r40B8v0FIJaCDi8tOytpOyJfXQca4h2iUIIIWKUBIgYN2X+NRTNXUowGOStZ/5Ac111tEsaMpk5oQBRLQFCXCSz2cyqVavCw5rcbjdbt26VYU2XQGvRY1uSQ/tiI/bV44mbmIRq0IQmXXfpKG2gtaQGv0vmJwkhhJAAEfMURWH5rfeRkV+Ix+3ktSf+G5ejI9plDYmMbBsADXUduOVERVyC7mFNK1as6DOsyeFwRLu8kUFVMI2zkfGZIsZ9fQEaQ882QU0fVlD31glO/+Ejql89SseJRoJ+6eURQoixSgLECKDR6lh97zeJtyXR2ljHW8/8gYB/9O0RERdvwJpgDE2krpJeCHFpVFVlxowZfYY1/fWvf5VhTZdI1fb8aQgGg8RNTEKfZCboD9JxvIHql49w6g+7qdtyAmeVLHoghBBjjQSIEcIcZ+Wm+7+NTm+g/MRh3n99fbRLGhLT52Yzf2k+ZrM+2qWIEap7WNPdd99NampqeFjTU089RXl5uUwOvkSKomBfkEPuF2aRc99MbHMy0Vh0BFw+WvfX0LxHhooJIcRYo71wExErkjNyWbHmK2z62+848MHb2NMymbbgmmiXNaiuvXFytEsQo0RmZib33nsvBw4c4P3336euro4NGzZgt9spLi5mypQpmM3mCx9IAKEgYUyLw5gWR/LV43CUtdB+uI64SSnhNp5mJzWvH8M6JZW4ScloLfJBgBBCjEYSIEaYCVPncNWq2/lw84tsf/UpbCkZZI+Xk24h+qOqKjNnzmTixIns2rWLQ4cO0dTUxLZt29ixYwcFBQUUFxeTl5eHoijRLnfEUFQFS34ilvzEiNvbD9fhrumgvqaD+ndPYc5PxDo1FcsEO6pOE6VqhRBCDDYJECPQnOU301hTSemBj3jzqUe4c90/k2BPjXZZg8bj9lFd2Upmtg2dXk46xJUzm82sWLGCpUuXcuzYMUpKSqipqeH48eMcP34cq9XKtGnTmDp1KlarNdrljlgJMzPQmHS0dQUJx+lmHKebUXQa4iYmkbwsX3olhBBiFJAAMQIpisJ1dzxIa2MtdZVneP2J33DH1/4JvdEU7dIGxR9+tY3WZif3f2Mh+ROSol2OGEUMBgPTp09n+vTp1NXVcfDgQQ4fPkxbWxu7du3igw8+ID8/n+LiYsaPH49GIwH2UmgtemyzM7HNzsTT5KD9cD1th+vwtbnpLG0kdcWEcFu/04tq1ErPjxBCjEASIEYorU7Pjfd9mw2P/BtNtZVsfvZ/uPG+b6OqI39efHqWldZmJ9UVrRIgxJBJTU3l2muvZenSpZw4cYKSkhLKy8s5ffo0p0+fxmw2M3XqVKZNm4bdbo92uSOO3m4maUke9sW5uKra8ba4wsOYgsEg5c8cQFEU4qekEl+Ugs5qiHLFQgghLpYEiBEsLiGRm+77Fi/+6d85c3Q/H25+gUU33Bntsq5YZo6NYwdrqS5viXYpYgzQ6XQUFRVRVFREc3MzBw8e5NChQ3R2drJnzx727NlDdnY2xcXFFBYWotPpol3yiKIoCqYsK6asnqFhvlY3vlYXQX+Qxh1naNxxBlNOAvFTUoibmByxB4UQQojYI+/SI1xazniu/dyDbHn2T3yy7Q2S0rKYNGtRtMu6IhnZsiO1iI7ExESWLl3KokWLOH36NCUlJZw+fZqKigoqKirYunUrkydPpri4mLS0tGiXO2LpbEbGfX0BHaUNtB+qw1nRhrO8FWd5K/VvnyR5+ThsszKjXaYQQogBSIAYBSbNXEhTbSUfv/c6W198lISkNNJzJ1z4gTGqO0A01nfidnkxGOUTXzG8NBoNBQUFFBQU0N7ezqFDhzh48CCtra3s37+f/fv3k5qaSnFxMUVFRRgMMvzmUmmMWhKK00koTsfb5qL9SD3th+vxNDrQJfbM5/K2uPA5PBgz4mW+hBBCxAgJEKPEVStvp6muitOHP+WNJ3/Dmm/+C3EJI3PctiXOQEKiqWceREFytEsSY1h8fDxXXXUVCxYsoKysjJKSEk6cOEFdXR3vvPMO27ZtY+LEiRQXF5OVlSUnuZdBZzViX5BD4vxsPPWd6JMt4fta9lXTsrcSXYKR+CkpxBelorePjgUjhBBipJIAMUooqsrKNV/hhT/+gsaaCl5/4jfc/tD/h04/Mj8ZzchOkAAhYoqiKOTl5ZGXl4fT6eTIkSMcOHCAxsZGDh8+zOHDh0lMTAxvUmexWC58UBFBURQMqXHn3AaKToO31UXTB+U0fVCOIT1ONqsTQogoGvlL9ogwvcHETff/HUZLHPVVZ3nn+b8QDAajXdZlmTk/hxs+O5XCIhlnLmKPyWRi9uzZPPDAA9xzzz0UFxej0+lobm5m+/bt/OlPf+LVV1/l9OnTBAKBaJc7oiVfPY7xX59P+k2TMI9LBIXQZnVbT1H25L4R+x4nhBAjmfRAjDLWxGRW3/tNXvnLrzhRsoektGzmXfeZaJd1ySZOkeAgYp+iKGRkZJCRkcHy5cs5evQoBw8epLq6mtLSUkpLS4mPj2fatGlMmzZNNqm7TKpeQ3xRCvFFKfg6PXQca6DtcB2mzJ55EcFgkIZ3T2GZkIQpJwFFlaFkQggxVCRAjEJZ4yZx9a338e6Lj/HR2y+RmJZJwbS50S5LiFFNr9eHN6mrr68Pb1LX3t7OBx98ELFJ3YQJE2STusvUe7O6YKCn98FZ3krLJ9W0fFKNxqInvigF65QU9CkWmZcihBCDTALEKDV13tU01VSyf9cW3t7wvyTYU0nJzI12WZekobaDirPNZOclkpwWd+EHCBEjUlJSuOaaayI2qSsrK+PMmTOcOXMGk8nElClTKC4uJilJNku8XL17GbQWPQkz0mk/1oC/00PL3kpa9laiTzITPyUF67Q0mS8hhBCDRALEKLb4xrtoqqui/MQhXn/i16xZ92PM8QnRLuuivfvmUY4cqGHFzUUSIMSIpNVqmTx5MpMnT6alpSW8SV1HRwcff/wxH3/8MZmZmRQXFzNp0iTZpO4K6JPMpK4sIOXa8XSebqb9cB2dJ5vwNDpo3HEWU06CBAghhBgkEiBGMVWj4fp7vs7zv/8ZLQ01vPG333LbV36ARjsyTlIysm0cOVAjG8qJUcFms7FkyZKITepOnTpFVVUVVVVVvPvuuxGb1Mmwm8ujaFTiCpKIK0jC7/LRUdqAs6wVY0Z8uE391pP4OjzET0nFMi4RRSPriQghxKWQADHKGU0Wbrr/2zz3+59SU3aSd19+gus+9+CIODnp3lCuqrwluoUIMYhUVWXChAlMmDCBjo6O8CZ1LS0tHDhwgAMHDpCSkhLepM5oNEa75BGr92Z13YL+AG2H6gi4/XQcb0Q1aomflEz8lFSMmbJZnRBCXAwJEGNAYkoGN9z9DTY+9p8c/fh9ktKymLX0hmiXdUHdAaK50YHL6cVoGhk9J0JcrLi4OBYsWMD8+fOpqKigpKSE48ePU19fz9atWyM2qcvOzpaT28GgKmTfVUzb4Xraj9Tj7/TQur+G1v016BKM2OaEJmgLIYQYmASIMSJ34jSW3HQ3O157ml2bNpCYkkH+5BnRLuu8zBY9NruZliYH1RWtjCuUDeXE6KQoCjk5OeTk5HDNNddw5MgRSkpKaGho4MiRIxw5cgSbzca0adOYOnUqcXEyJ+hydW9Wl5IaR/KyfJzlrbQdqqOjtBFvqwufwxtuG/QH8Lt8MndCCCHOIQFiDJm+aAWNtRUc3rOdzev/hzu+8SPsqbH9SVtGdgItTQ6qylskQIgxoXuTulmzZlFbW0tJSQlHjhyhpaWF999/n507dzJ+/HiKi4sZN24cqirj9y+XoiqY82yY82wEPH46TzZFzJXoPNNM9ctHMOUkYMpOwJRlxZgRj6qXJXiFEGNbzAWIkydP8rOf/YxPP/0Ui8XCrbfeyne+8x30+oE/Aaqrq+Oxxx5j586dlJWVER8fz7x58/je975HVlbWMFYf2xRF4erP3EdLfQ1VZ47z+uO/5o5v/DMmS+x+mpmZk8CRA9UykVqMOYqikJ6eTnp6OldffTXHjx+npKSEqqoqTp48ycmTJ4mLiwtvUpeQMHJWWItF3ZvV9eaqbIcgOMtacZZ1vQcpYEiNw5RtxTYnC53VEIVqhRAiumIqQLS2tvLAAw+Qn5/Pb3/7W2pra/nlL3+Jy+Xixz/+8YCPO3ToEFu2bOFzn/scM2bMoLm5mT/84Q/ceeedvPbaa9jt9mH8KWKbRqtl9b3r2PD7n9LaVMdbz/yeW774PTSamHophE2ZkUFqhpXMbDk5EmOXXq8PB4XGxkZKSkrCy8F++OGHfPjhh+Tm5lJcXExBQQFabWz+fx5pkpflY52WhqOsBVdlG87KNnxtbty1HbhrO7DN7fmAqvNUE75OD6YsK7pEk8xXEUKMajH1V2b9+vV0dnbyu9/9DpvNBoDf7+df//Vfeeihh0hLS+v3cXPmzGHTpk0RfzRnz57N8uXLefnll3nwwQeHo/wRwxRn5ab7v80Lf/gFFSeP8P5rz3D1rfdFu6x+JSZZSEyyRLsMIWJGUlISy5cvZ8mSJZw8eZKSkhLOnj1LWVkZZWVlGI3G8CZ1ycky7O9K6e0m9HYTzMwAwNvmxlXVhru+E118T+9Dy75qHKeaAdCYdBiz4sPDngypFlkqVggxqsTUO9r27dtZuHBhODwArF69mkAgwM6dOwd8nNVq7fOJW3p6Ona7nbq6uqEqd0RLTs9h5V1fAUWh5MOtlHy4NdolCSEugVarZdKkSdxxxx18+ctf5qqrriIuLg6Xy8Unn3zC448/ztNPP01JSQkejyfa5Y4aOquB+MkpJC/Nj7jdlGXFlG1F0Sj4nV46TzTR8N5pyp/az+k/7iYYCIbb9r4shBAjUUz1QJw6dYrPfe5zEbdZrVZSUlI4derUJR3r9OnTNDY2MmHChMEscVQZP2U2V626nQ/feoHtG58iMSWD7AlF0S6rj8qyZo4dqiU908qUGbE96VuIaEhISGDx4sUsXLiQs2fPUlJSwsmTJ6murqa6ujpik7r09HQZXjME7AtysC/IIeAL4K7rwFURGvLkrGxDn2xBUXt+52VPfIqiKhizrOHgoY2TuRRCiJEjpgJEW1sbVqu1z+0JCQm0tl78JNpgMMjPfvYzUlNTuemmmy67nmAwiMPhuOzHXwmn0xnxfagUzbuGusqznDq4l01PPcItD/49VnvKhR84jEqP1PD+2yeZODWF/EJbtMuJacP1uhGxKy0tjbS0NBYtWsSxY8c4cuQIra2tlJSUUFJSgt1up6ioiIkTJ4Y3qZPXzSCzaTHY7Bim2UkIBgm4/eG/JQGXD09D6LK7rpPWT6sB0Fj16NPjMOUnYCpIjFrpl0JeN+JyyOsmdgWDwYv+gCmmAsRg+e1vf8uHH37In//8Z8xm82Ufx+v1cuTIkUGs7NKdOXNmyJ8juXAutZVldDbX8foTv2batXei1cXOuudunxuA8jNNUf/3GCmG43UjYp/RaGTmzJm0trZSXV1NfX09TU1N7Ny5k127dpGSkkJGRgY2mw1FUeR1M0yURUY0rX40LQG0rQHUjiD+Ng/OtiZa21pxeWtCDQNB9BU+/Akq/ngV1NjsOZLXjbgc8rqJTedb9bS3mAoQVquV9vb2Pre3trZe9BKFGzZs4JFHHuHnP/85CxcuvKJ6dDodBQUFV3SMy+V0Ojlz5gz5+fmYTKYhf77xeTm8+peHcbQ3U3N4F9eteShm1pd35Xv5aOsOnJ1+8vMKMJllR+qBDPfrRowsbreb0tJSjhw5QkNDA3V1ddTV1REXF0dSUhKTJ08mNzdXVnEaZgGPH09tJ57qDvRpFox5ob93nnoH9duOhhppFPSpFvQZFgwZcejTLaiG6P47yfuNuBzyuoldJ06cuOi2MfVXYvz48X3mOrS3t1NfX8/48eMv+PgtW7bwk5/8hG9/+9vccccdV1yPoihX1IMxGEwm07DUYDabuen+v+PF//l3yksPsn/HGyxevWbIn/dimM2QmGSmudFBS6OHpGRZ0vVChut1I0YWs9nM/PnzmT9/fsQmdR0dHXR0dHD27Fk0Gg2ZmZnhnbHT09MlUAw1M2CLh0mRN6vGAJbCJFwVbfidXjzVHXiqO+igFoCU6yZgmxVaHepShh4MNnm/EZdDXjex51LeQ2Lqr8KyZcv44x//GDEX4s0330RVVRYvXnzex3700Ud873vf484772TdunXDUe6ok5Y9juvu+BKb1/+RT7dvwp6aSdGcJdEuC4DMHBvNjQ6qK1qZMCm25mgIMRJ1z5W4+uqrOXjwIAcPHqSjowOHw0F5eTnl5eVAaLWncwOFRiM7MQ8HY1ocmbcWEQwG8Ta7cFa2hvej8Da70Cf1fHrbWdpI/bunMWVbw5Oz9clmmTAvhBgSMRUg1q5dy5NPPsm6det46KGHqK2t5eGHH2bt2rURe0A88MADVFVVsWXLFiC0e/W6devIz8/n1ltvZd++feG2drud3Nzc4f5RRqyJMxbQVFvB3ndf492XHseWnE5GXnSGcfWWkZ3AoX1VVFe0RLsUIUYVnU7HpEmTCAQCTJ48GbfbTXl5OWVlZVRUVOBwOMJ7TEAoUGRlZZGTk0Nubi5paWkxM9xxtFIUJbwfRUJxOgC+Tk/EECZnZRu+djftR+ppP1IPgGrQYMwMrfJknZaG1hI7c9uEECNbTAWIhIQEHn/8cX7605+ybt06LBYLd9xxB9/97ncj2gUCAfx+f/j6/v37aW9vp729nbvvvjui7W233cYvf/nLYal/tFiw4jaaaqs4dfgTNv3tt9y57sfE25KiWlNGTmjYUmN9Z1TrEGI0UxQFu92O3W5nxowZBINBmpqaKCsrC/dKuFwuzp49y9mzZ4HQhLvuQJGTk0NqaqoEimFwbhhIWpyHZbw9vHSsq6ottPrT6WYcp5uJ79Vz6yxvxe/xYcq0ojHJnDIhxKVTgsGg7GjTj5KSEgCKi4uj8vwOh4MjR45QVFQUlTGCHreLF/74CxpryknJzOX2h/4RnT5665R7vX7aWpzYkyLXUxeRov26ESPTxb5ugsEgDQ0N4TBRUVGBy+WKaGMwGMjKyiI3N5fs7GxSU1NlGE0UBANB3HWdOCtb8dQ7SL2+IPzvUPXyYTpPNAGgTzKHhjxlh4Y9aa2Gi/73kvcbcTnkdRO7LuXcN6Z6IETs0BuM3HT/t3nukX+jvqqMt5/7Mzfc/XWUKH2yqNNpSEqJi8pzCyFCFEUhJSWFlJQUZs+eTSAQoL6+PiJQuN1uTp06FV4Qw2g0kp2dHe6hSE5OlkAxDBRVwZgehzG97/umPtGEx27C2+TE0+jA0+ig7UBo6VhdgpG8L82RD2qEEOclAUIMyJqYzOrPf5OX//wwJw/uZffWV1mw4rPRLksIESNUVQ1Pxp47dy6BQIC6uro+PRQnTpwILw9oMpnCgSI3Nxe73S6BYpglXz2O5KvH4XN4cVW14axow1XZhqu2A41FFxEeKtYfQNGqmLJCk7ONGfGoOplEL8RYJwFCnFdm/kSWf/Z+tr7wKHveeYWktCwKiudFpZbKshY+3HYKS7yeGz47LSo1CCEGpqoq6enppKenM2/ePPx+P7W1teFAUVlZidPppLS0lNLSUiC0tGx370ROTg6JiYkSKIaJ1qwjriCJuILQHLeA14+/0xu+3+/24axoA8BxpiV0o6pgTIvDmGVFkyFr+AsxVkmAEBc0Ze4yGmsq2b9zM28/92es9hRSs/KHvQ6vx8ehfVUkJJokQAgxAnTvKZGZmcmCBQvw+/3U1NSEA0VVVRUOh4Njx45x7NgxAOLi4iJ6KBISEiRQDBNVp0G1aSKu594/E2dFW3hytr/Dg6u6HVd1O6aJdsgKtQ34AtRtLkVvN6NLDK0YpUs0oWplQr0Qo5EECHFRFq9eQ3NdFWWlB3njyd9w57p/wRI/vBu6pWeFnq+12Ymjw4M5TpYkFGIk0Wg0ZGVlkZWVxVVXXYXP56Ompia8ylN1dTUdHR0cPXqUo0dDOzDHx8dH9FAkJMhGksNFURUMqXEYUuOwzc4kGAzia3OHw4Q23QQeJwDeFifth+v7HEOXYERnNxE/JQVrUSoQmowPl7ZplRAitkiAEBdF1Wi4/u6v89wffkpLfQ1vPPlbbvvKD9Dqhm8JQKNJR1KKhcb6TqoqWiiYnDpszy2EGHxarZbs7Gyys7MB8Hq9VFdXh3soqquraW9v5/Dhwxw+fBgAq9UaESi6Nx0VQ09RlFAgSDBinZKKw+GAI6HJ1xqjlqSleXganXiaHHibnAQ8frytLrytLkxZPf9O3mYX5U/tC/VWdO1vEfoyo7MZUTTSayFErJMAIS6awWTm5vv/juce+Sm15Sd596XHWHHnl4f1U6SM7AQa6zuprmiVACHEKKPT6cjNzQ1v/un1eqmsrAwHitraWtra2jh06BCHDh0CwGazRQSKuDhZrS0atHEG7AtywteDwSB+hxdPowNvsxNjRnz4Pk+Tg4DbHx4KFUGB5OXjSJwTGhvld/vw1Heit5vRmGXPCiFihQQIcUlsyelcf8832PjYf3Ls010kpWcze9nqYXv+jBwbBz+torqiddieUwgRHTqdjvz8fPLz8wHweDx9AkVLSwstLS3h9csTExMjAoXFYoniTzB2KYqC1qIPbXiXa4u4z5yfSO4Ds/A0OfE2OfA0OcNfQa8/YpM8V2UbVS+Gep9UkxZ9oimi58KYES87bAsRBRIgxCXLLZzK0pvuZvvGp9j15nMkpmQwrmjmsDx3ZnZo/POxQ7UEg0EZQyvEGKLX6xk3bhzjxo0DwO12U1lZGZ5DUVdXR3NzM83NzRw4cACApKQksrOzwxvbycZV0adqVQwpFgwpkeEuGAzi7/CgGnomcgd9AbRWA742NwGnD5ezHVdVT69F2uqJWKeGeqPddR20H20ITeJOCgUNjVFOc4QYCvI/S1yW4oXX0VhbwaHd29j87P9wx9d/RFJa1pA/b0Z2ApZ4A0XF6RIehBjjDAYD48ePZ/z48QC4XC4qKirCPRT19fU0NjbS2NjI/v37AUhOTg73TmRnZ2MyyVKksUJRFLTxhojb4iYmEzcxmYDXj7fZ2au3IjTPQp/cEwidFW00766IeLzGpEOXFOqtsM3K7BNahBCXRwKEuCyKorDsls/TXF9D1eljvP7Er7nzGz/GZBna8cd6g5YvrFuIXt/z0q0qb2HrG0dZuqKQvAlJQ/r8QojYZTQaKSgooKCgAACn00lFRUW4h6KxsZGGhgYaGhr49NNPAUhJSSE3N5ecnByysrIwGo3R/BHEAFSdJrwi1ED0yWYSZmaEw4Wvw4Pf6cVf4cVV0YZ1alq4bWtJLS17K0PLzXZN4O6+rDHIqZEQFyL/S8Rl02i1rL53Hc898lPamup58+lH+MyD30ejGdqXVVJK5B+Q9985wanjDZw63kDueDvLVhYyrjBZeiiEGONMJhOFhYUUFhYC4HA4wr0T5eXlNDU1UV9fT319PR9//DGKopCamhrRQ6HXy/j6kcKca8Pca75FwOPD0+TC0+zA2+hEn9TTW+Fp6MTT6MDT6OhzHI1FR9bnpobDiq/dTTAQRGs1yN8VIbpIgBBXxGSJ56b7v83zf/g5laeOsmPj0yz/7P3DWsOqz0zBEmdg3+5yyk418bf/+YisPBtLVxRSWJQqb/hCCCC06/WkSZOYNGkSAB0dHZSXl4eHPTU3N1NbW0ttbS179+5FURTS09PJzMwkKSkp/GUwGC7wTCIWqHotxvQ4jOl9ey0S52Vhzk/E0zWJ29s1LMrf6cXf6UXTa2J2yydVNO+pRNGq6BKNod6KRFPX0CgzhmSzLD0rxhwJEOKKJaVns2rtQ7z+5G84+NG7JKVlU7zw2mF7fpvdzE13FLN0RQG73jvJJx+UUXm2hfV/2cPEKWms/dK8YatFCDFyxMXFUVRURFFREQDt7e0RPRStra1UV1dTXV3d53G9A0X3lwx/Gjm0cQa0cQYs4xIjbve7fXibnBFLxga8ARSNQtAXwFPvwFMf2Wsx7qF54bkbHScb8bW6Q8OibCY0cXrZjVuMShIgxKAYVzSThdd/jg/efJ7trz2FLSWdnIIpw1qD1Wbihs9OY8m1BXyw7RR7d51l3MTk8P2BQGj3U1WVHgkhRF/x8fFMmTKFKVNC711tbW3h1Z26J2N3dHSEv86ePRvxeIvF0m+wkInaI4fGoEXTa88KgNQVE0i5djzeNhfero3yPF0Tun3tbjRxPb0VbQfr6CxtjHi8atKijdOjtRjIuHUyqi60ypS7vjM0NCpOj8ask95yMaJIgBCDZvayG2mqqeTYvg948+nfc+c3foQtOX3Y64izGll5yxQWX1OArtdygIf2VbF983GWrCikeFYmqnQ5CyHOw2q1MnXqVKZOnRq+zeVy0dTUFA4U3V/t7e10dnbS2dlJWVlZxHHMZnO/wUKWlB05FFVBbwv1Klgm2AdsZ8q2QjAYChdtboK+AAGnD48z1LOh9OqNaNxV1hM21K59M+K6vuL1JC0bF+698Du8KFoVVa/p72mFGHYSIMSgURSFa27/Ii2NtdSWn+L1J37DHd/4EQZjdP5ImuMiJz/u3XmGxvpOXnlmH9s3H2fxtQXMmJuNRrqXhRAXyWg0kpmZSWZmZsTtbre732DR1taGw+EIT+DuzWQyDRgs5NPokSlxTlZ4F+1gMEjA5cPX4cHX4SHg9kX8u6o6DRqzDr/DC4EgvnY3vnY3AIpGIfma8eG2tZtL6TzRhGrQoLXo0cQbesJGnJ6EGRkoXb3rskeSGA4SIMSg0up03Pj5b7HhkX+jub6at575Izc/8B1UNfon6fd8ZT57dp7lw22naG508NpzB9jxdimLrpnArPk5aHXyyY4Q4vIYDAYyMjLIyMiIuN3j8fQbLFpbW8PLzFZURO5dYDQa+w0WFotFTgxHEEVR0Jh0aEy6fvefSL9xIgBBfwBfpxdfhzu09GyHh4DXH/FvHXD5Qt/dfjxuJzQ5e55Hq5Iws+d1V/PqUZzV7b0CRq+wEW/AlJsgryNxxSRAiEFnsdq46f5v8+L//Dtlx0vY9eYGlty4NtplYTDqWHJdAfOX5PPxh2V88O5JWpudbHrxIKdLG1jzhbnRLlEIMcro9XrS09NJT48czun1evsNFi0tLbhcLiorK6msrIx4jMFg6DdYxMXFyQnhCKZoVHRWAzrrwKt7Za+dTsDjw9fu6erRcId7NghE9jh42934u4KI+9zn0qpM+LuF4et1b5/E0+gIBwxNd+CI14eGVMnStWIAEiDEkEjNyue6O77EW8/8gX073iIpLZuiOUuiXRYQ2oxu4dXjmbsoj08/KmPX1pPMvio3fL+765Meg1H+ewghhoZOpyMtLY20tLSI271eL83Nzf0GC7fbTVVVFVVVVRGP0ev1/QaL+Ph4OfkbRVS9Fn2SNmI/i/5k3jYlNByqw9Pz1RUqUJSI14Sruh13bUe/xzk3bDR9UIa33ROeo9G7Z0M1auW1NsbIGZIYMoXT59NUW8mera/y7kuPY0tKIyO/MNplhel0GuYvGcecq/JQNT1vfB+8d5I9O8+wYNk45i8Zh9GkO89RhBBi8Oh0OlJTU0lNTY243efz9Rssmpub8Xg8/S43q9frsdvtEaEiOTlZgsUop7WEeg8uRsp14/G2uvD307Oh6jQRr5OOE00Dhg3VpGXCuqvC11tLagi4/RHDqGRJ29FFAoQYUvOvu5XG2kpOHfqYN576HXd+45+xJiZf+IHDqPck6mAwyMlj9TgdXt578zgfvHeK+UvGsWDZOMwX+YYshBCDTavVkpKSQkpKSsTtfr//vMGipqaGmpqaiMfodDrsdjvJyckRASMhQcbGjzWmTCumTGu/9wWDwYjriXOz8DQ5Ino2/B1u/E4fmnN67Fv31fQbNlSjFk2CAXqt8t52sBa/24fGqENj1KKatKHLJi2qQRueHC5iiwQIMaQUVWXlmq/wwh/raKgu540nf8PtD/0jekNsbrikKApf/NZiDu+rYsfbpdTXdrDj7VI+2nGKuYvyuerq8cTFyy60QojYoNFoSE5OJjk58oMZv99PS0tLOFA0NDTQ1NREU1MTXq83vON2b1qttk+PRXewiIWFMMTwOjdMxhel9Nsu4AsQcPsibosrsKNLNOHv1aMR9AUIuHwoBg3Q83pq+bQKd21nv8c+t2ej4f2z+NpcoaDRFTI0Rl0odJh0GNP67jouhoYECDHkdHoDN973bZ77/U9pqC7n7ef+zOp7voESo3+QVFVh2uwsps7M5OjBGnZsKaWmqo1d757E5fRy853To12iEEKcl0ajCQeA3gKBQESw6P5qamrC5/NRV1dHXV1dxGO0Wi2JiYnhIVDdIUOvl15ZAapWRdVGvhbsC3MjrgeDQQJuP752N84OB81NPXulWMaHwkbA6cPv8uJ3+gi4fAQ8fjSGyNNUx6km3HUDhA2jlgnf7Akb1RuP4qnvRDV19WwYtV2rYoW+J0zvWdjA7+zaZ0NWY7xoEiDEsLAmJnPj57/JS//7MKcOfczud15hwcrbol3WeSmqQtH0DCYXp1N6pI733znBomsmhO9vaXKgKAoJibLLrBBiZFBVFbvdjt1up7CwZ05aIBCgtbV1wGBRX19PfX19xLE0Gg0Gg4ETJ06QkJBAfHx8+CsuLo74+HgMBlnFR3QtaWvUojFq8VsUaOq5L2lxXr+PCfoDBDz+iNsSF+Tga3Phd/rwu3wEnF78rlDwUPWRp7Tert3Cwcm5VKM2IkBUbzyKs6wVRatGhA3VqEVr1pG6siDc1lXdTtAf6NUDokUZgxvTSoAQwyYjr5BrbnuAd57/C3u2voo9LZPC6QuiXdYFKYrCxClpTJwSuVrK268d4ejBGmbMzWbxtQXYk/uu8y2EECOBqqokJiaSmJhIQUHPyVIgEKCtrS1iGFR3uPD5fOFN8s7dy6KbVqvtN1j0/m4ymSRkiD4UjYrGFHliHj/p4udQpt80CV+nJ9yj0dO74e1zwh9wh4JK0BcILY3b4Qnfp5q0EQGiYccZnGWtEY9X9ZpQ6DDryLl3Rvj13H60Hl+npys86UbV/A4JEGJYFc1ZQmNtBft2vMXbz/2FhKQ0UrPyo13WJfP7AzgdXgL+IJ9+VM6+PRUUz8pkyXWFJMsYTCHEKKGqKjabDZvNxoQJPT2wwWCQ2tpaDh48SFJSEh6Ph/b29vBXR0cHTqczvHpUc3PzgM+h0WgGDBjdl2V3bnGp9EnmCy552y3n8zMIePyhoNE1lCp02QuRc8nRxunRJRrDwQQIPdbj77MBYOuBmj5hI0xVKPjuonD7po/K8TQ6SLluQp+hW7Eo9isUo86iG9bQXFvF2eMlvP7Eb1iz7sdYrLZol3VJNBqV+752FeWnm9jxdiknjtZz4ONKDnxSydQZmSxdUUBqRv8rWwghxEinKApWq5XExEQmTZqE2dz3RM3r9dLZ2dknWPT+7nA4whO+W1paBnw+jUZDXFzcgAEjPj4es9ksk73FZVEUBY1Bi8agRZdw/rbpN04KXw4GggTcXcOpXD4C3sghV+Y8Gxqzrmt+RyiQ9MzviFwm13GmBWd5a0RvRyyTACGGnaqqrLr7azz/+5/RXF/NG0/+ltu++gO0upE3IS9nnJ17vrKAqvIWdmwp5dihWg7tqyI9yyoBQggxpul0unDvxUB8Pl9EyDg3YHR0dNDR0YHf76e1tZXW1gE+zSX0t8VisQwYMOLj47FYLBIyxKBRVKVrYnb/+0XZF+T0e3t/8ztsczKxjE9EGSF7ZUiAEFFhMJq56f6/47nf/5TailNsffFRVq756ojtos7MsXHXg/OorWrjw+2hJV+7lZ1qQtUoZOclRq9AIYSIQVqtloSEBBISBv7Y1+/3h0PGuQGj+6uzs5NAIBC+PhBFUfqEjN4BoztkaDSyGo8YOv3N74grSBqgdWySACGixpacxg33fINXH/0Pju/7kKT0bOZcfVO0y7oiaZlWbl07M3w9GAzy5ksHqalqY1xhMstWFpI3YWS9SQghRDRpNBqsVitW68C9uoFAgM7OzgEDRndPRiAQCF8+H4vFMmDA6B5KpdXKKZQYu+TVL6Iqp2AKy26+l22vPskHb72APTWLcUUzo13WoPF6/KRnJ1BX087p0gZOlzaQO97O0hWFjJ+YPGJ7XIQQIpaoqho+wc/IyOi3TTAYxOFw9BkudW7PRnePR2dnZ5/N9nozmUwDBozungydTifv82JUkgAhoq544bU01lZw8KN32bz+f7jj6/9EUnp2tMsaFHqDls/cNYNlKwvZufUk+3aXU3aqiaf+9BFZuTZW3jKF3PH2aJcphBCjXvfwJYvFQnp6er9tgsEgTqfzvHMy2tvb8fl8OJ1OnE5nn433etNoNBiNRkwmU/j7QJd7f5fQIWKdBAgRE5becg/N9dVUnjrKa0/8mjXf+GdMcaNnErLNbuamO4pZurKAD949xccfnKWyrAWnw3PhBwshhBgWiqJgNpsxm82kpaX12yYYDOJyuQack9F92ev1RvRmXEoNBoNhwIAxUAiReRtiOEmAEDFBo9Fywz3reO73/0ZbUz2bnv49tz74f9CMsjGm1gQT1392KouvK+DA3gomTu35A7Vvdzk6nYaiGRmoI3hzGSGEGM0URQmfuKempvbbJhgMRvRSOJ1OXC5XxPf+Lns8nnBAcblc590/41x6vX7AwDHQZa1WK70d4rKMrrMzMaKZLHHcdP/f8fwffkbV6WNse/VvXHPbA6PyzS0u3sCia3o2ZfK4fWzZeBinw0vSWxaWrCikeFYmqmZkLOcmhBCih6Io6HQ6dDrdeSd/n8vv9w8YMM4XQgA8Hg8ej+e8S92eS6vVnjdw9HebwWAYlX+XxaWRACFiSlJaFtev/RqvPfFrDu/ZRlJ6NjMWrYh2WcNiwbJxfLjtNI31nbzyzD62bz7O4msLmDE3G80IWRdaCCHE5dNoNOF5Gheru8fiYns5ur/7/X58Pt9FrUrVm6Iol9TL0X1d9t8YXSRAiJiTP3kGi264k12bNvD+68+QmJJBbuHUaJc1pPQGLctWTmTB0vHs3XWGD7adornRwWvPHWDH26XcsmY64yemRLtMIYQQMab3kKqLFQwG8Xq95+3l6C+QeL3e8ERzp9N5SXUaDAaMRiMGgwGv10tZWRlmsxm9Xt/ny2Aw9Hu7LJ0bO+RfQsSkWUtvoLGmgmOf7uKtp3/PHd/4ZxJT+l81YzQxGLUsvraAeYvz+eTDMna9d5LWFifxCcZolyaEEGKUUBQlfFJ+vk38zuXz+c4bMPq7rXuIldvtxu12h491KfM7uqmqesGQcbFhRCadXxkJECImKYrCNbd9gZaGWmrLT/L6k7/mzq//MwaTOdqlDQu9QctVV49n7qI8Tp9oJCUtPnzf5lcPY4nTM3dRPgaj/BcWQggxPLRabXgjvYsVCATCk8KdTictLS2cOXOG5ORkoGfuhsfjwe124/V6cbvdEbd7vd4+xxqMn0Wn010wiBgMhgu2G4vDs+TsQ8QsrU7Hjfd9i+ce+Vda6mt465k/cPMD30EdQ58aaHUaCot6Vvlobuzkox2nCQaC7Hr3JAuWjWP+knEYTbooVimEEEL0T1XV8NK4AImJibhcLoqKisK3XUggEMDr9UYEjd4Bo7+vgdr4fD4g1JvSvVLWldJqtVccRLrvHykT1CVAiJhmiU/gpvv/jhf++AvKSg+yc9MGlt58d7TLipoEm4nP3DWD998upbG+k/fePM4H751i3pJ8rlo2HkbG+44QQghx0VRVxWAwYDAYrvhYgUDgooJG796Q/npFPB4Pfr8f6Akjl7Lfx0Byc3O54447Yj5ISIAQMS8lM48Vd36ZN5/+Pft3biYpLYsp85ZFu6yoUDUqM+ZmUzw7i8P7q9jx9gnqa9p5/+0TfLT9NJ+9pzjaJQohhBAxS1VVjEYjRuOVzy30+XzhIVYX6hW50P2BQACAtrY2AoFAzM/RkAAhRoSC4nnMu+5W9rzzCu+98gQWq43sCVNG3UZzF0tVFabNymLqjEyOHaph+5ZSmhsdpGXGc/pMLQAvPPkJFWebscTpscQbiIszYI43ENd1feqMTBTZsE4IIYS4LFqtdlBWhgoGg/j9ftxu94jZVXxsnn2JEWn+tZ+hqbaSkwf3svGx/0JRFOIS7FjtKVjtKSTYU7AmpoSvmyzxMd8FeKUUVWFycQaTpqXT3OjAaOr5eVuaHbQ2O2lt7ju+U6tVmTozM3z9ucf3Un6mmbg4A5b4rq84PXFdl4tnZYXDRjAYHPW/VyGEEGK4KIoyaGFkuIycSsWYp6gqK+78MsFgkLLjJfi8HtpbGmlvaaTy1NE+7XV6QzhMWBO7Akb4ejJanT4KP8XQUBQFe7IFh8MRvu2O++bQ3uais91NZ4ebjnYPjg43He3u8GO6tbW46Ghz09Hm7nNsrU6leHZW+PqGR/dSfrZX2Ojq0bDEGYiLNzBjbnZP2AgEpZdDCCGEGGUkQIgRRac3cOPnvxnayKajjdametq6vlqbuy/X0dHWgtfjprGmgsaain6PZbHazgkXqeGQYY5PGPGfsickmkhIvLiNhe764lza20LhoidwhC5DZNhob3fj6PDg6PBATXvEcXR6DTPn54SvP/voXspON/UMo+oKGqHLembNzw0HjIA/gKoZe0vhCSGEECONBAgxIimKgjk+AXN8Ahl5BX3u9/u8tDU39goXdT2Xm+rxul10trXQ2dZC9ZnSPo/XaHV9ei5692Do9Fe+EkQsibMaibNe3ISyu780L9Rb0RU0QoHDEw4bvXW0u3A5vbicXhrrI1en0Ok1zL4qL3z92Uf3cvZUY6+A0RU2usLH3EV54SDj8/nRaNQRH/KEEEKIkUgChBiVNFodiSnp/e5eHQwGcTk6aWuqo625PrIXo6mejpZG/D4vzXVVNNdV9Xt8c5yV+HOGRXVftlgTR/WmMqGTegNpF9H23q8uoKOtO2h46OgOHP2Ejc4ONx63H4/bQXOjI+I+vUHDvMX54esbHvuYM6UNEUOoes/fmL8kPxwuvF4/Wq2EDSGEEGKwSIAQY46iKJgscZgscaTljO9zv9/vo6O1KRwoeoeLtqZ63M5OHB1tODraqC0/2efxqkaLNTE5PNeiJ1yEhkjpjRc3rGg0MJn1mMx6Uoi/YNv7vnZV17ApT8QQqs4OT5+2ne1ufL5Av5PE9QYtC5aOC19/7rG9nDregCVOj8miR6/Xojdo0Bu0GIxabl07M9z22KFaOttd6PVadIaudvqe71abUYKIEEKIMU8ChBDn0Gi0JNhTSbCnktPP/W6no5+ei9AQqfbmRgJ+Hy0NNbQ01PR7fKM5Lhwq4hOTSbCnhq/HJdjH1E7bvRmMOgxGHUkpF277hXWLzhlCFZok3n/PhodAIEh7m5v2cyaJhwJEz/U975/m1PGG/p9UgX/+1U3hq6+u38/pEw3o9Rp0Bi0Ggxa9PhRM9AYNN3x2GhptqCfqzIkG2ltd6A1adL0CTHd7k3nk7D4qhBBCSIAQ4hIZTGZSTHmkZOb1uS8QCIR7L9qa6/v0Yjg723E5OnA5OqirON3n8YqqEm9LOmfFqJ4hUgaTRU40Cc2fSEwyk5hkvmDbL35rUbhXw+X0hoZJeXx43H4gGNE2J9+OTqfB4/Hjdvvwun14PH48bh9wzmTyNle/S+R2u/H2nk399u46y+H91QO2/cHPr8dg1AHw9mtHOH64tieM9AocOr2G5ddPwmAMvXVXlbfQ2uzsCi2RPSYGgxaNDN0SQggxBCRAxLAKZw1JnSlkGjLQa3TRLkdcBFVVQ8OWEpOBoj73e9xO2poawgGjd7hoa67H7/OFr/dHbzSFh0NF7n2RTLwteYh/upFJq9Vc9IpUV18/8aKPe+PninF0ds/b6AkaHo8fn9cfsXxtaoa1T3jxuH143D58vgA6fc9bcWuzk4bajoFrXNVT4ycflvHJh2UDtv27H10X/rl3bj3B4f3VvUJGTw+IogaITw6EH1dd0UpjXQcarYpWp6LVatBqVbS60PfEZDNabainzO8PoCgKqizXK4QQY4YEiBjl9ft4qvI1nqp8DYB4QxxJJht2cyJJJhtFKQUsyZsfbu/2eTBoR8++BqOV3mAiOSOH5Iy+g6OCgQCd7a09Q6KaGyLmXzjaW/C4nNRXlVFf1fekUVEUzFYbikbPmY9tmEwWdHoDOoMRvcGITm/suWwwRF7vdVmr08un1hfhYntAAJatLAQK+70v4A9EnHwvv2Eicxbm9gSSc0KHXt8zxM1mN5OdnxjRU+Lx+PF6/EBo8nm35kYH1RWtA9Z4za09Y8dKPqnkw22nBmz7tb+/mtT00LyWHW+Xsn1zKaqqhMJGV8jQajVodSq33zuL1AwrAEdLaij5pLIrlPS00Wo1aLQq0+dkYbOHfqeN9R3UVbeHQkxE29BzxCcY0elCP18wGOpJktetEEIMDwkQMarT68CuS6DD78AT8NLu7qDd3cGZltCeBh6/NxwgPH4v973wd1j0ZpJMiSSZbdh7fc+3ZTHe3ne4jYgtiqoSl5BIXEIimeP6fhLu9bhpb27o6bVobqCtqS583ef10NnaDEBHU+0VFKKg0xv6ho6uMKLTd4cQY0S7yNtMoZDSdd9oXpXqSp2790VSShxJKXEX9dgl1xWw5Lq+yxgHAkG8Hn9EgFiwbByTpqX122Pi6HCh03nDbe3JZsYVJuPz+vH5AqGv7stePzpdT81+XyD8nKFj+yNr6TVKrL62nSMHBh7KlV+QFA4QpUfq2PzK4QHb3vOV+RRMTgXg04/Kef35A73CS68go9Ow4pYixhWEeujKTjexd+eZiPu1vXpaCqekkpIWCkftrS6qylvQaFU0mtCXqlG6LitYbSaMJl349+Dx+CLaSaARQoxWEiBiVIIhntneGxmfl4XRosGrduKmE4e/g05/GxOScsNtm50tAHR6HHR6HJS1VkYca3n+Qr6x4H4gFDZ+uPnfe4WMUI9GkjkRu8lGssWOWTd2VgkaSXR6A/a0LOxpWX3u695Yr666nNJjR0lPTUEhgMftwutx43W78Hpcoetdt/VcdoXbEQxCMBi63e0atNq1On0oiOiNEcGivyASGVhM/YYXjVbeus5HVZXwPIluKWnx4RPjczkcDo4cORK+PndRPnMX5V/Uc129aiILl0/A5/Pj854bNgLYe/XSTJiUgtGow+ePDCT+rpAS32svkrh4A7nj7RHH8vl6Lnf3PkDo5D0YBG+v3pfeet/W3NDJwU/7X54ZIMFmCv+eKs4289zjHw/Y9pY105m1IPRefKq0gWf+vDviflVV0GhVVFVh5S1F4X1PqspbeOWZfaGgoVXRhNupaLQKM+flUDQ9A4CWJgc7t57ouk8NH1PTFWRyxtnJHWcHwOX0cvxwLZqu46i9Q4+qYrUZwwEt4A/Q1uo6JxSFjquoEn6EEOcnf4VjlMcbYOPuZtjd3M+9CguLfSz6QuhaijmJzJrb0RjdaAweFL2LoNaJX+PAo3SiuBLCj2x2tlDRVk1FW/+fAi4ft5BvzO8JG3/e+wx2sy2iZyPZnIhFb5Y/MDGke2O9VI2OxnY3E4qKMJsvbnhNt2AwiM/riQgWXrcbj9vZ63Lf0OF1d1929bocuj0QCJ24+bwefF4PTtoG5edVNdqe0BERRgwRwaR3+NAbTGh1ejRaLRqNLvRd2/t76LJWq0Oj0aJIr8lF0eo0aHUXt3JYZo6NzBzbRbWdNiuLabP6huX+zFyQw+Tp6T0hozvI+ELhJCOr5z0wI8fGqs9MCd/v8wa6Akzosq1X4NEbtGTl2vD7AvgDQfy+AIFAIHxd32vuSsAf4FyBQJBAV3gJ9Lrb5fRRf555Lnnjk8KXO9rdfPzBwPNcrl41MRwg2lqcvPz0vgHbLlw+npW3TAGgtcXFb3+xdcC285fkc8Nt0wBwdHj4n//cHg4Y3aEDBdxuJ231Z7jmhtBxfV4/Lzz5CapGQVVDgUdRQ3NkVFUhK9cWDl3BYJCtm46F7wt/aUKPS0wyM3FKz44zh7qCX+jYvY+rYrboSO/171xXHXqvUVU13L77MVqtisncM+Q34A9IaBLiEkmAiFH+QICJmUY0eiNubxCny4fT7cPh8uLxBTAZev7pPL4AJ8u6N97SAJaur5D0aelwdeiy1RCP99g8gjonit7V9eVGY3CB3kXpKRd0Ta1ocrbw3pkP+q1Pq2hZmLGIby29O1SD38uW0vdJi7eTZLaTZLIRb4iTN+QRROkauqTTGyA+4cIPuIBgMEjA7wuHDk8/AaP37V63u9fl7tu7AorHidftxu8LDbMJ+H24nT7czs4LVHH5VFXTK2B0hQ5dKFycGz66Q8e5YST8XaNDq9OhdrXRnttG0/dx2vBjJcxciE6nieiROJ/U9Pjw/I0LmTAphQmTLmJdYWDi1DT+6eEbu0JGsCtkBAj4g/j9AcyWnhPWjGwr933tKvz+AH5/kIA/gN/f0zYz1xZuG281cvX1E8P3hdp1Py5IepY13Far0zB+YjL+rraBc45viTOE2waDQbQ6Fb8/SDAQuRoZRM4n8fn9tLcO3CPZ0tSzGpnPF+DYoYGHUHrcvnCACASC7HznxIBtJ05NiwgQLz+zD38/QQ1Cw9/u//rC8PXHHvkAl9Pbb9usPBtf+vaS8PVf/3wr7a0uFCUUOBSVcPhJy4zngW8sCrd96k8f0driDAUSRQkNVesKMQk2I7d/fna47eZXD9PS5AgvMhA6fiioGM06rr91arjtR9tP0dLsDN+vdB1fUUOv78XX9gxVPHKgmrYWZ0+7rsd0h6Tpc7PDbSvONtPR5u5qR08tXcfPHWcPL/jQ3OjA5fRG3B/6XYSOb7WZwvO1PG4f/q75W72PqSihn1P+9o9+EiBilNmo457lyRT180myzx/A1+tNVKtR+ZcvX9UVMEJBw+ny4nCHLo/L7DkZ1Co67JocnA4vjiYfvnP+cNin9rxZGzR6/JUTCWqd4aCh6F0oOg++oI+DJ5phaahtk6OZx/dviPwhgipavxkdZsabp/Avt90Vqj/gZ/2O3Zg08SSZEjCbdJgNWkwGLWajDotJh9UiE8JHOkVR0Gh1mLQ6TJaLO0gWd5AAACgeSURBVGG7EL/fFxE+BhqK1W+viNuFz+vB7/fh83oJ+H34fV58Pi9+nw+/3xcawtUlEPAT8Pjx9t3HbtipGk2/vSZarRZV0xNywsFEo+0VdnQRQai7jarRhkOKzx+gpbaSapMGs9mCqtGgqGroeVXtOdc1KBpNKGB1fZeA0/V67/p0/kJMZj3jCi9u1bSERFPEylvnY0+28PmHrrrotv/4yxsBCAaCEWHH7w+i1fb8HBaLga98d2k4wPh9QQKBAI5OJ2Vl5UyZ1tNTpNWp3HRHcaj3JRAKJ4HwVyBiGJ0CzF+aT8Af7PrAoXfbIBnZkR9k5E1ICgWjrmP1Pva5CxpY4vRoNErE8ULHD6Cec3LbHaCCwdCqYvgBQn9j3S5fRNumhk6aGx30pzPZEnH99PF6aqvb+20bbzVEBIiD+6qoPNvSb1uDURsRIPbuOsvp0v73q1E1kQHi/XdOcPw8ge6fHr4RDaHfx7ubjp53eN8//Oz68Jyft14+xKe7ywds+50fX4c1ITQc+u3XjrB319lwiFIU8Pv9bNM3oaoKD3xjUfjf78Ntp/jkw7JwCAl/73rsbffOCs8RO7C3gk93l/dq2xV4ui6vuLmI5LRQ29IjtRzYWxlqe07QUVWFBUvHkdL1wUL56abQstu97u/9HNNmZ4Zfx3XVbRw/XBdRp9qr7fhJyeF6W5ocnDnR2G8NigIZ2baLXpgjFkiAGIG0GhVtrz9SWo3K3KK08zyih06r4S//tBIIfQLl9QXCocPh8qLv9SlegsHKvbNu7nW/D4fbS0enm05fO+PSEyOOHWhOA52zK2i4QQng03bgo4Oqlp76GjobebXmiVANAYWg10jQYyToMRD0GEnT5fHHdaHdvQLBAP/0h510OHxdASMUMnRdf9xSE83ce8Pk8LGfevMorR3unv+UdP1HBRLiDKxZ0fOH+NUdJ2luO6dt139ks1HLZ6/uecPeurecxtbQJz6hD2BCn+aAgl6ncuOinp2Pdx+uobHFCb3bdr1pqqrCtXN75q8cPNlAY9cnXxFvJoTaL5iaEf7E51RlK01trog2qqJ0vcnBlHE9wx6qGztx1UTuUdD7b2Zhjg1d1zKcNY2dtPSzAVu3cVkJGLpeF/XNTprazj1uz4Fz0+Mxdg3raGpz0dh6Tlt62malxoV70lra3TT0adsjI9mC2Rg6Ie4MBGh0+IA40MSBGRQz6Al9pSaasXT9ketwekP/FucK/dpISjCF2zpcXhpbHPj9XoJ+Xyhg+L0E/X4Cfh8mHWg14Pd5cbncdHQ48ftDwSPo9+H3+Qj4Q8FEVQIoQT9+nw+v14vb5e46no+Azxv63n09/PheYcbno/ceFQG/n4B/6MPMkR2X+UBFiQwUGhVV1Z5zPXS/ompQewWQvtdVVI32nOuh+9Xuthd1Xe26rj3n+nkef+7tYyQYKaqCVtUMeEag0ap9TuYhNHfGS0NkL4hWw5yFF7doh6pRueGz0y66zs8/tOCi26774TUD3hcMRn5w9o0fLA+Hi3MDh6qJDBufu282Ho8/on13kDl3KN/SlYU4OkMbWQaDhNsFg0F0+si2M+Zmkzc+iWCwb1uNNvJ1mF+QhNmiD4Wu7ucPhr73Xj4aICnFQnZeYvj+3m2Dwcj3b6NJR5zV0Ou5CdcQDAQj2gb7dlpF6B3SvL320unN7XJ3HavnYB3tbhrqBh7e5/P2fHja0uzg7MnGAdsuWdHzN7yhrpND+wYOR1NmZIYDRG11Ox/t6LtPU7fMnIRwgKipbGPrG0cHbHt7r8BTXdHKq8/uH7DtzXdOJ7HX/NZYJwFiDFMUBb1Og16nwRZv6HO/qircfk3/S0+eKz0+lSe/+KNwEGl3uqhrb6a+s5kmRzNZ1vRwW4fXiR4znqADRQ2iGJxg6DnJCzp6VqCp62jgpP0ZAnGGrpBhBIeBoE8PQYX0hpxwgOj0ONhSuouWNg/BoAJBFbq/BxRS4uzhAOEL+Nm09zCVdZ0Eu9sFutsrJFpNEQFi067THD3b33wUsJh0EQFi4/ZT7Cvtfx8HzTkB4uVtJ/noUP87VgO89PAtqF2n0i+8W8r2TysHbPvMz26k+8/MqzvOsvXjgds+9uNVJHV9OvTqjlNs3DHwkp3/88PryOx6A9z0wWmee6d0wLa/+f7ycI/Xlt1n+dumgd9YH/7mUoq6xm6/90kFf3n14IBt/+2rC5k1KbTizq4DVTzy/MBvwv/0xflcNS00AXXv4Rr+4+lPBmz7/XvnsHx26NO6/aX1/OKxPQO2/eadM7j+qnwAPjlWx08e7294nwbQ8JVbp/GZZRMAOHSqkR8+8v6Ax71vdVH4dXmyooXv/Nc2IIhCEBU/KoHw1+qrsrlpUS5+n4/axnZ+/fQeFAKoSgBNRFs/U8bZmF2YhM/npbPTyTu7z4TvU5WedioBbBYNFn0AvU6HPxCgrrEDhUBXDaHv4etKP2cOXcPVAv6+JwkjWRAIBtWuf42eL1BQVJVEqykUcBSVhjY3Hm+gT7sgoeEtk/KTuobHqJysbKPN4Q3f3/OloqCwfG4uihKaVP3p8Xpqm10Rx+t9/DUrJndtGKjy/v5qTlW1Ddj2odtnYDLqUVSVzR+Wsf9EY59au79++MB8EuJDK6i9sv0UO/ZX920XVPAHg/zLl5PJz0pGURRe2HqC194/3asdoHS9MykKv1y3hPyuZX1f3naCDW8fP+e33nPi+ZOvXMXE3NAHVZs+OMNTbx45p2VP23+4fy7FE0K9Olv3lvPYa4ci2/Y6r/72XbOYMzn0odauA1X86eWSASqAr942nYVdPQufHK3jd8/vG7Dt/TdOCS/WfOhUI/95Tg293b2y58Os0vJm/vDuwO+rt189IXz5bE0bj3048MntTYt7/hbVNHayfn/FgG1XzM8Lf0DV3Obi+SPV3W9h9Pxkoe/LZmaFF2bodHp5ubQGbLqIn7/7rWHOpFTMXcPlvL4Az5dUosZ3tw0FE7/Ph1arpSjfjtXWs3jC85+Wo8Rpz3n2kNz0eGz2nkVeXtxbTsASeRrb3T410UyivefT/Jf3nMVr7r9tgkWPPbmn7au7z+I0afq0AzAbtBHzpF758AwOY6+2vd4e9VoVa6/9h17ddQaXXo04HsHQ8VVVIc7a9zwslkmAEIPG1DUMyW41AvEU0f/Y4fH2PP5213/gC/hpcbXS5Gih0dlMo6OZRkcLMzN6unUbnS2gBFENLjD0HYM7ztarZ8PRhCN1D/rU/uvLMs+LaNuY+QbGzP7bpummhy+3utqoSX+FhFQFBTXyK6iSova8YXt8HtpSd5ER74eu+0PfFUDFHOgZthAIBvDZS8mZ6oJg6HjdAYagBo3fFPFGo09oI3ecLxR4AirBoIISVAgEVQhoQr0dXW9e8RYNmSmhNzkFpc+nRb33HbBa9GQk9XS9B8/Znbl32ziTjrReb8rnnkpqerU1G3Sk9HrzPLcGrbanrcmgISnBOGBbXa9P4Ax6DXar4aLa6nQabF1/yLp/rq6FpkL39+rJ02jUUG9EsOc3EGoX7PrZetqqCqHeuq62vdsFg0T+xSF08nKhT+zOeQRBFPyo9F5TSBuXQmpWPgABcwcNwa7Jtf0ce3rWeBasDO2G3dDi5LcfbB7w2a4rzGLGRIWioiICaLn7nzcN2PbqmVl8956ZBPx+3B4v9//LG32DhhK6Pn2CnQduLAoNB/P7+cmfdhIMBCLadD8mN9XCTYvyuj7V9fHMW0fw+Xx9jq0SJDFex5xJKeHj7j1cjdfnQ+3VpjvwGHUK6UmmUC9OwE9jS2e4ht7t+gtGoZ7A/sfcE4SOXr1mOkA30LDvIFSd7hlyogfON4DpyN6zEcfNHrgpu9/uWepWBfou6ttj24uRAfl8n/2/9fi7EdfnDtRQgbf/8kbETYvObdP1qw0G4fXfvtA1fEPFH4D5/u7/b6Gw0TukbHviLXZpNSiKgsPtp9jtDbcj2B18Qo/Z+9L7HDGF9rBp7fBQ6HT0OhYRlw9v2Uf1bhOKotDY4iK7sxWC9Nv+5K7jOEtD8/nqmp0kt9VHPG/vx1TsK+fDhkQURaGmyUFcS0VPvb0eA1BzpI59nqPhtsaW0xG/h57v0HCqlSOmqnANmtbjETWEf78oNJc7OXW4HUVRaWpzEWg53c8xQ49tq/VRdTrUHdvW6cHT0v2hU98aOhoVGmsSQFFwunw4WiJ/Dz3/zAqdDgOO9hZQwO+HlraWcJtzj50XsBDwe/EGQh8+1LV1dg2t7tXb0XXZrsRjMPZsqlvd4cLh6v9DC0OKBUuvD0ZrOt00OfrvaR/fa3UygBqnh2pn/929mRYdaRk9PW51Li9nXP23tRuM4QUOABo8Xo56+p+XYzHpIub7jARK8Ny+PAFASUnoE4ni4uKoPH/TyZOc+OBDcnLzMFrMKBoNilYb8V3ValA0WpSI76HLavjyxU0sjFWBQIAWd9s5IaMZh9eFP+BnfvZM5maFTvar2+v4y8fr8Qf9+AJ+/F1fvoAPX9DPygnLuHnSdQBUtdfyw83/Hm537knzTROv44FZdwChsPGNjf80YI0rxi/hq/PuBaDD3cmDL/+fAdsuyZvPt6/6IgBev5d7n//2gG3nZs3gH5Z8LXx97YZ1BIL9n8wUp03mn5f/XXg5zl+feRKXr+fNUlG6Io+iMil5Aj++5jvh+777xr/S5m5HVdTwuE0VFVVRyEnI5IfL1oXb/nzbb2lyNPdpqygKqZYkvrPoy+G2f9zzN+o7G0JBS+makNdVg9UQx9fm3xduu+HgRmo6GkKfv4aPGzrJMGkN3N/1bwGw+cQ2atrru47Z+9gqGlXDHVNvDLf9sPwT6s6pIfw7QeH6wqtRuz4d3V9zmNqOerr/cCko/3979x7dRJn3Afw7SdqmtzQtlGIpdwXKSi3YVRG3XMSjvPv6iity8SggF6vSs259z7KwZ5FFOcJyjp7lIqyyIIgX4PUgHFBYxdcXVkDfVRS8VIGGcmmhlF7StGluM8/7x0wmSdNg6Cskhe/nHEzmmWee/BIn6fzmeZ6ZoLOWEkb1uQMmo3rO5YfaEzjnuBDSpS8hcBWXO/KGIlG7saOt/hSqHdoY2eDMQqh/uAu7D0ZKopponWqsQnVTjboOQss8AGhD1gZ3uwkWs5rsVTfV4HTD+aBX16pqr3FT1z7I0uaeXGiuQ2XdOUBLJtU4hV43x5yF2rNVyM/PhwseVFxoc5W2oLB7ZdyAHKsVAGB3OXCiJqinSwp8dgDQPT0buZnqGeRmTwuOnw8dQhB8Vjg7tQt6ZKl/bJ3eVhw/XxVIuvR2VV1SMpHXRbt0qc+N4+erQ77BwcfxVnM6enZVD9c9shcnaqohB8398scqhEBaghl5WZkQigKP140TNdXweX0QQtEzTyHUsfdmYyK6Wy0QigKf7ENVXS28sqwNDVHUG2AIRZ2sbDCimyVdT3hq7A3wyjKg+NtT1EchYIQBWWkpEIoCWZbR0NIEn1fWhraocfjrS5CQbk7ShpgoaHI2Q5bVdqDF6W8fAMwmEyAUKEKBy+2GLMvhdbVloyTp2/pkWZtzoARiCGqX6KqS/L932rfX//sgqckctGG+BsmgLUpQhAj6wZH0NtTeRAkmo1FbLcEnK4Hf0pC2/SM3TPqyx+evC/21Af9wZAOSkxL0Nlwen3aCSdLaDn4/EtJS1ISnR9+BuP2eB6/IR/dTLufYlz0QcUjx+fDjvD9B8XgQeWBJ9CSjATAY1LM+RrULXdKW0WY5UNcAySgF1mnPYQgq08r1tg0GQN8mUA/+qzMYg+q1qaPGErSt5I9FGypgNCBLktrcy9cIHPsG9cfVoS9JkPC0lK31BwZ98f1f1At1aLjwX4AkIRnA8py79DqKAGQokCEgCwGTE2g8tF17HYHnbhgBWYiQOjLUf5kuwP6v9wFI8CgyHu06BDIAn1AgCwEFiv68p1ug6bB6JtgrFAxP76W2qbXtrycLBTkuDxxH1bOAQgBdTSnwQYEcVMf/T2ppQvN3n8Lt8SDhXBVEm6Ekwh+vUOBurofj6P9APeUm4HA2wCG3f2YmRVb0eCEEqi5W4qKv/QmEzY6LsP/vLn35h7NHUO1tfyxrptGMRl8a/N3ZX9R8jkpv+5d4TZUS8O8OWY/hn/Vf40ePvd26CTBgdE1gTOxHjeX4xtv+0DMAKDpVqScVexzH8aWnPmLdfNuPMEtqQr672YZD7vaHqQFAz2NHYTEkaHUrsc8deRLj89ZbkG1Ue18+ajmND12Rb7T2p4wh6GFSz5R95DyL91sjD1Oba/kFRII69Oy/W89hmzPypUBLU25C1yYvmi58hYNyHTa3VEas+1T6AJgS1aTgM3ctNjZH/pWamXYjkpLUuTmH3fVY2xx5mMZjqX1hNqvdh996GvGK48eIdSem9MboZHVY5HFvE15uKo9Yd3xKT9ybrHY1nvK1YKk98lC5f0vugftT1PP953yteNF+NGLdu83d8VCKOhyxSXbjJfuRiHV/lZiNKS3qvIBmxYvVTZHr3paQhemuPoAQ8AgFL7vb1JWgDTEBCk0ZmJ3cR10QAqXiWHudUQCAwcY0zDH3hf888bMt38ON9hOA/oYUlJn76MvznT/CoR8mhZ6UukE2Ym5SXxiNJggIvOCqQL3iVY+NRNAxkgCypQSUJfVRczEIrHKdRq1w6+uDt7HAiKeTekNovxFvuqtQo3j0jwBCO/cvgCQYMDOpp5pIAdjluYBziktvN/jRBGByQg/4f3v2e+tQLVxh9fyx/EdCN/VgVQCHfY04p7j1dSFxAyg2ZalDTgXwo9yMC4o75DPwxwwAhcYMJGgnCE7Lrern0Oa1/XUHGNPgHwBUI7tR3+Zz0J8DyDMkIwEGAAKNihd2xRvSZnDdLlKiXrdFyGhRfO22CQGkSiZtsrWAVyhwayez2n5e6mesHeCLtqfmfiZ6u9qj3gMTKvyOMJFdzvSydmbVRRR5Nkc4/1/AC2dsKBpzP4zG+D5Ej+/orlOSBCRn++Cxw3+MF/LYXpm+rh1CVgAto+70/HmB9ssW/Fxfj6Cydpb17S5jObgdA7QhLNqyF0BF0Pqewe34/xMU48mgNscEnYRo77UrsU9ffvKS76kGp6WvAQAmCfhPCfqhgZCCupglwIBanP1nYGzuTJNB3a0kffdST4ADMOIiqg4FxihPSjTCawh0Vfu3UQAkoA7n/vW6Xnec2QS3QRsOIEkhbZuEQM3hN/W6v0pJwFCjQY9Vj0cCjAKoPRq4wldBWiJ6m4zq60pSSNwGAHXfbtfr9rckIS3RFBIDEPiq1H+3S583kmdJggjqHg/tlgfs338Af+rULd2MguRAXT9F+9wcP+yBdoNmZKab8YuURAR/RUXQ/uD8cS/qZXVNWroZA9ISg7r7Q7mOfYJ6rWFzehL6pmvDvrTqStCC+8Q+1GsTDk2pSehlTQ6NITjuis+Q4JHhAIDURORmpmhxhsfhPfkZ6rVhA3JKInK6pIbV8fNV/gv12qU0fckJ6NY1Lez9+8mnv0JDi/pn3J1sQtfswNV62v52iTNH0dCsDsdpNRuR1S0jsLJN26LqOzQ0qTeDcyYakXlDRkibwW2Lcz+gofErAEBzggGWXGvE94bzJ9DYqCYYDqOE9LzMiFWlC5VorFe/c04DkN4zMKyh7f9rQ+1ZNNapc4e8EpDWqwsiMTrPw14bSMpSel+irqsW9ppAspfcKytkyGEwk6seTecr9eWknpnwRbi6lNnjRuvxg/pyQp4Viab2e74NXie8JwJ1pR5WQJt4rO+bWkiy7IHxWODGfHJuBtxJwd+5oNhlGaknA3OdPN0tcCS3fyU/kyLQ9VQgMfTkpKM+JfLY89yTP+i/EZ9lp+N8WuS6vSsrkKTtUN91TcPpdHPEug+dOo00rSesMisVtozIN3Add7oKWdpvxN7MFByzRr5Sz6izF5Cjfe/3ZSTj+6zI38+ZVfXI0+5TcjDDjENZaRHrPnauEf207/0X6Ul4v2vkK+tNPm/HQO17/3VaEnZkp4eM4dSTDQl48IIDg1s8EAB+SEnEe93Sw379/EnKvRebMUT7jbCZTXivmyXQnp+WeBU3tKDQ4YYAUJVowns5gWFHwUkSANze2IqhzWoieyHBGBZD8OjGWxwu3Kq122iUsL2bJSTRCn6NQc1u3KpdoKTFCOzIbhtDoH5/p1tvNyPJgPa/bfGFQ5giiPUQpouf7UJt+RfIsGbAFHLX3bCvlv5MCPXQy3+lBSiB5+qyol3dQQlar4Sub1tfadueol+RAYoCIbfTjgheVgBZ6M/1MiW0DO2sJyIiIrqemLt3xbDVq2MyBJ1DmK4BKQVj4Ey4Ab07cEfha4EQAkKW1X8+n/Yoq5fX9MlaMqToSU4gaQmMEdYftaQHweuFUHtmgscU+8cs+xMcEbytv82gJKdt+5d6vbbtBydhYe9DDnlP/teM9D6Cl2WfD62trTAnmbVLUIrAmRY9JxOBy+YFnz/QnqvvJVA3UB5e1n6bQSuCt9UbEEGrw8uC64mQstDXFG3LguNv856imcF8yXMpHT3PcontOnzu5krESUREccGUnhk6QSxOMYGguCRJEiSTCTCZgKTOdWmzWPJPoh50nSae1DEtLS0oLy9v98aVMRFviVC8xRMnnK2tgf0mOfIQHKJg/r9T+YMGITkefm/ijGQ0doo7eTOBICK6zul3cZWk+PjDFQ8x0E8KvaBG577iH109/itESiYTDCYehnZWnWGeBhERERERxQkmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFLW4SyAqKirw+OOPo7CwECNGjMCyZcvg8Xh+cjshBF577TWMGjUKBQUFmDRpEr7++usrHzARERER0XUkrhIIu92OadOmwev1YuXKlSgrK8PWrVuxdOnSn9x27dq1WLFiBaZPn45XX30V2dnZmDFjBs6cOXMVIiciIiIiuj6YYh1AsM2bN6OlpQWrVq2C1WoFAMiyjEWLFqGkpAQ5OTntbud2u/Hqq69ixowZmD59OgDg1ltvxX333Yd169bhz3/+89V5A0RERERE17i46oHYv38/hg8fricPADBu3DgoioIDBw5E3O7w4cNobm7GuHHj9LLExETcc8892L9//5UMmYiIiIjouhJXCYTNZkO/fv1CyiwWC7Kzs2Gz2S65HYCwbfv374/q6mq4XK6fP1giIiIioutQXA1hampqgsViCSvPyMiA3W6/5HaJiYlISkoKKbdYLBBCwG63w2w2X3Y8Qgg4nc7L3u7n0NraGvJIFA3uN9QR3G+oI7jfUEdwv4lfQghIkhRV3bhKIOKJ1+uFEALl5eUxjaOysjKmr0+dE/cb6gjuN9QR3G+oI7jfxKdOmUBYLBY4HI6wcrvdjoyMjEtu5/F44Ha7Q3ohmpqaIEnSJbeNxP8BJiQkXPa2RERERESdidfr7ZwJRL9+/cLmOjgcDtTW1obNb2i7HQCcPHkSgwYN0sttNhtyc3M7NHxp6NChl70NEREREdG1Lq4mURcXF+PgwYNoamrSy/bs2QODwYARI0ZE3G7YsGFIS0vD7t279TKv14sPP/wQxcXFVzRmIiIiIqLrSVz1QEyePBmbNm3CnDlzUFJSgpqaGixbtgyTJ08OuQfEtGnTUF1djY8++ggAkJSUhJKSEqxcuRJZWVkYMGAA3nnnHTQ2NmLmzJmxejtERERERNecuEogMjIysHHjRrzwwguYM2cOUlNTMWHCBJSVlYXUUxQFsiyHlM2ePRtCCKxfvx719fXIz8/HunXr0LNnz6v5FoiIiIiIrmmSEELEOggiIiIiIuoc4moOBBERERERxTcmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEEREREREFDUmEHGmoqICjz/+OAoLCzFixAgsW7YMHo8n1mFRHNu9ezeeeuopFBcXo7CwEA888ADeffdd8BYvdDlaWlpQXFyMgQMH4ptvvol1OBTn3nvvPYwfPx5DhgzB7bffjlmzZsHlcsU6LIpjH3/8MR5++GEMHToUd911F5555hmcOXMm1mFRB8XVnaivd3a7HdOmTUOfPn2wcuVK1NTUYOnSpXC5XHjuuediHR7FqQ0bNqBHjx6YN28eMjMzcfDgQSxYsADnz59HaWlprMOjTmL16tWQZTnWYVAnsGbNGqxduxZPPvkkCgsL0dDQgEOHDnH/oYg+//xzlJaWYvz48SgrK0NjYyOWL1+OGTNmYOfOnTCbzbEOkS4TE4g4snnzZrS0tGDVqlWwWq0AAFmWsWjRIpSUlCAnJye2AVJcWrNmDbKysvTl4cOHo7GxEa+//jqefvppGAzsaKRLq6iowNtvv40//OEPWLhwYazDoThms9mwatUqrF69GiNHjtTL77333hhGRfHu/fffR25uLl588UVIkgQAyMrKwrRp0/Dtt9+iqKgoxhHS5eKRRRzZv38/hg8fricPADBu3DgoioIDBw7ELjCKa8HJg19+fj6am5vhdDpjEBF1NosXL8bkyZPRt2/fWIdCcW7btm3Iy8sLSR6IforP50NqaqqePABAeno6AHC4bSfFBCKO2Gw29OvXL6TMYrEgOzsbNpstRlFRZ/Tll18iJycHaWlpsQ6F4tyePXtw7NgxzJkzJ9ahUCdw5MgRDBgwAKtXr8bw4cNx8803Y/LkyThy5EisQ6M49pvf/AYVFRV466234HA4cObMGbz88ssYPHgwhg0bFuvwqAOYQMSRpqYmWCyWsPKMjAzY7fYYRESd0RdffIEPPvgAM2bMiHUoFOdaW1uxdOlSlJWVMdmkqNTW1uLTTz/Fjh07sHDhQrzyyiuQJAkzZsxAXV1drMOjOFVUVIRVq1bhpZdeQlFREcaOHYu6ujqsXbsWRqMx1uFRBzCBILqGnD9/HmVlZbj99tsxderUWIdDcW7NmjXo0qULHnrooViHQp2EEAJOpxPLly/Hfffdh5EjR2LNmjUQQuDNN9+MdXgUpw4fPoy5c+di4sSJ2LhxI5YvXw5FUfDEE0/w6l2dFCdRxxGLxQKHwxFWbrfbkZGREYOIqDNpamrC7NmzYbVasXLlSk6epkuqqqrC+vXr8corr+i/O/45M06nEy0tLUhNTY1liBSHLBYLrFYrBg0apJdZrVYMHjwYJ06ciGFkFM8WL16MO+64A/PmzdPLCgsLMWrUKOzYsQOTJk2KYXTUEUwg4ki/fv3C5jo4HA7U1taGzY0gCuZyuVBSUgKHw4EtW7bok9OIIjl79iy8Xi+eeOKJsHVTp07FLbfcgq1bt8YgMopnN954I06fPt3uOrfbfZWjoc6ioqICd999d0hZ9+7dkZmZGXF/ovjGBCKOFBcX429/+1vIXIg9e/bAYDBgxIgRMY6O4pXP58Pvfvc72Gw2vPXWW7zcL0UlPz8fb7zxRkhZeXk5lixZgkWLFmHIkCExiozi2ejRo7Ft2zaUl5cjPz8fANDQ0IDvvvsO06dPj21wFLdyc3Px/fffh5RVVVWhoaEBPXr0iFFU9P8hCV4/K27Y7Xb8+te/Rt++fVFSUqLfSO7+++/njeQoogULFmDr1q2YN28ehg4dGrJu8ODBSExMjFFk1Nl8/vnnmDp1Kt59910mENQuRVEwceJE2O12lJWVISkpCa+99hoqKyuxa9cuZGdnxzpEikMbN27Eiy++iMceewxjxoxBY2Mj1qxZg/r6euzatQuZmZmxDpEuExOIOFNRUYEXXngBX331FVJTU/HAAw+grKyMB4EU0ZgxY1BVVdXuuo8//hh5eXlXOSLqrJhAUDTq6+uxZMkSfPLJJ/B6vSgqKsL8+fNx4403xjo0ilNCCGzevBnvvPMOzpw5g9TUVBQWFqKsrAz9+/ePdXjUAUwgiIiIiIgoarxMCxERERERRY0JBBERERERRY0JBBERERERRY0JBBERERERRY0JBBERERERRY0JBBERERERRY0JBBERERERRY0JBBERERERRY0JBBERXVO2bduGgQMH4ptvvol1KERE1yRTrAMgIqLOZ9u2bZg/f37E9Vu2bEFhYeHVC4iIiK4aJhBERNRhv/3tb5GXlxdW3qtXrxhEQ0REVwMTCCIi6rDi4mIMGTIk1mEQEdFVxDkQRER0RZw9exYDBw7EunXrsGHDBowePRoFBQV49NFHcezYsbD6hw4dwiOPPILCwkIUFRXhqaeeQkVFRVi9mpoa/PGPf8Rdd92Fm2++GWPGjMHChQvh8XhC6nk8HixZsgR33HEHCgsLMWfOHNTX11+x90tEdL1gDwQREXVYc3Nz2EG5JEnIzMzUl7dv346WlhY88sgjcLvd2LRpE6ZNm4adO3eia9euAICDBw9i9uzZyMvLQ2lpKVwuF958801MmTIF27Zt04dJ1dTUYMKECXA4HJg4cSL69euHmpoa/OMf/4DL5UJiYqL+uosXL4bFYkFpaSmqqqqwceNGPP/88/jrX/965T8YIqJrGBMIIiLqsOnTp4eVJSYmhlwB6fTp0/jwww+Rk5MDQB329PDDD2Pt2rX6ROxly5YhIyMDW7ZsgdVqBQCMHTsWDz74IFauXIm//OUvAICXX34ZFy9exNatW0OGTj3zzDMQQoTEYbVasX79ekiSBABQFAWbNm2Cw+FAenr6z/YZEBFdb5hAEBFRhz333HPo27dvSJnBEDo6duzYsXryAAAFBQW45ZZbsG/fPsyfPx8XLlxAeXk5Zs2apScPADBo0CDceeed2LdvHwA1Adi7dy9Gjx7d7rwLf6LgN3HixJCyoqIibNiwAVVVVRg0aFCH3zMR0fWOCQQREXVYQUHBT06i7t27d1hZnz59sHv3bgBAdXU1AIQlIgDQv39/fPrpp3A6nXA6nWhubsZNN90UVWy5ubkhyxaLBQDQ1NQU1fZERNQ+TqImIqJrUtueEL+2Q52IiOjysAeCiIiuqFOnToWVVVZWokePHgACPQUnT54Mq2ez2ZCZmYmUlBSYzWakpaXh+PHjVzZgIiK6JPZAEBHRFbV3717U1NToy0ePHsWRI0dQXFwMAOjWrRvy8/Oxffv2kOFFx44dw4EDBzBy5EgAao/C2LFj8cknn4RM0vZjzwIR0dXBHggiIuqw/fv3w2azhZUPGzZMn8Dcq1cvTJkyBVOmTIHH48Ebb7wBq9WKWbNm6fXnzp2L2bNnY9KkSZgwYYJ+Gdf09HSUlpbq9Z599lkcOHAAjz32GCZOnIj+/fujtrYWe/bswdtvv63PcyAioiuHCQQREXXYihUr2i1fsmQJbrvtNgDA+PHjYTAYsHHjRtTV1aGgoAALFixAt27d9Pp33nkn/v73v2PFihVYsWIFTCYTfvnLX+L3v/89evbsqdfLycnB1q1bsXz5cuzcuRPNzc3IyclBcXExzGbzlX2zREQEAJAE+3yJiOgKOHv2LO6++27MnTsXM2fOjHU4RET0M+EcCCIiIiIiihoTCCIiIiIiihoTCCIiIiIiihrnQBARERERUdTYA0FERERERFFjAkFERERERFFjAkFERERERFFjAkFERERERFFjAkFERERERFFjAkFERERERFFjAkFERERERFFjAkFERERERFFjAkFERERERFH7P/NwHH5SWRXXAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAqrc41cC2O9"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "Is it the lowest validation loss that one can achieve? Probably not. So here are your TODOs:\n",
        "\n",
        "#### TODO:\n",
        "- Train the model for 100 epochs and plot the learning curves. Use learning rate of 0.01.\n",
        "\n",
        "#### TODO from now on, for all subsequent tasks:\n",
        "- Print the overall best validation loss and the epoch at which it occurred of.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z0n7hif7C2O9"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(Y_train_scaled.shape[1], activation='linear')\n",
        "])\n",
        "\n",
        "# compile\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "# train (100 epoch)\n",
        "history = model.fit(\n",
        "    X_train_scaled, Y_train_scaled,\n",
        "    validation_data=(X_val_scaled, Y_val_scaled),\n",
        "    batch_size=512,\n",
        "    epochs=100,\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the validation loss ve training loss lists\n",
        "val_losses = history.history['val_loss']\n",
        "train_losses = history.history['loss']\n",
        "\n",
        "# find the epoch that includes the lowest val_los index\n",
        "best_epoch = np.argmin(val_losses)\n",
        "\n",
        "print(f\"Best epoch (0-indexed): {best_epoch}\")\n",
        "print(f\"Validation loss at best epoch: {val_losses[best_epoch]}\")\n",
        "print(f\"Training loss at best epoch: {train_losses[best_epoch]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0hYSF2d1VlR",
        "outputId": "bd60f52a-cebc-485f-c940-99ac66dc7c55"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best epoch (0-indexed): 99\n",
            "Validation loss at best epoch: 0.0439310148358345\n",
            "Training loss at best epoch: 0.04798450320959091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_bEGGrC2O9"
      },
      "source": [
        "**TODO Your answer here**\n",
        "\n",
        "Answer the following questions in written form:\n",
        "\n",
        "1. Does the training loss decrease after each epoch? Why does it? // Why does it not?\n",
        "1. Does the validation loss decrease after each epoch? Why does it? // Why does it not? (For your answer to be sufficient, you should describe fluctuations and discuss the overall minimum of the curve.)\n",
        "1. At which epoch was your model best? I.e. if you had saved your model after each training epoch, which one would you use to make predictions to unseen samples (e.g. from the test set)? Why? (For your answer to be sufficient: Also discuss what this means in terms of overfitting)\n",
        "\n",
        "**TODO Your answer here**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxGMPr9DC2O9"
      },
      "source": [
        "<span style='color:red'>**Your answer:**</span>\n",
        "\n",
        "1. The training loss generally decreases over the epochs, indicating that the model is successfully minimizing the error on the training set. This behavior is expected because the optimizer (SGD in this case) updates the model weights in a direction that reduces the loss function. However, the decrease is not necessarily strict at every epoch due to the stochastic nature of gradient descent and the use of mini-batches (batch size = 512). Small fluctuations are normal, but the overall trend is downward.\n",
        "\n",
        "2.   The validation loss also shows a decreasing trend, but it may not decrease consistently at every epoch. Minor fluctuations can occur due to the model’s generalization capability and the natural variability of the validation set. In this experiment, the lowest validation loss was achieved at epoch 99, with a value of approximately 0.0439. This indicates that the model continues to improve in terms of generalization up to the final epoch, and no significant overfitting is observed throughout training. Overall, the curve suggests stable and gradual improvement.\n",
        "\n",
        "3. The model performed best at epoch 99, which had the lowest validation loss of 0.0439. If I had saved the model after each epoch, I would select the one from epoch 99 for making predictions on unseen data. This is because the validation loss reflects the model’s ability to generalize to data it has not seen before. Since the loss continued to decrease and did not increase toward the end of training, there is no evidence of overfitting. Therefore, the final model is not only well-fitted to the training data but also generalizes effectively.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HINT**: *These results prove that a correctly configured model (activation: ReLU, 1 hidden layer, 20 neurons, lr=0.01) can give stable, generalizable and effective results when trained with appropriate batch size (512) and sufficient epochs (100). This also shows that the learning rate selection (lr=0.01) was the right choice.*"
      ],
      "metadata": {
        "id": "0AF2tM5D3JNe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BramMlgC2O-"
      },
      "source": [
        "# Subtask 2.6\n",
        "## Save and restore model checkpoints\n",
        "\n",
        "Training that model for 100 epochs took quite a bit of time, right? Wouldn't it be a pity if it would get deleted out of memory, e.g. because your Colab session terminates (this can even happen automatically)? We would have to train it again to make predictions! To prevent this, we would like to save a check-point of the already optimized model's weights to disk. Then, we could just load our model weights at any time and use our model again without retraining. As you will see in a bit, this will be very handy for early stopping, too!\n",
        "\n",
        "#### TODO\n",
        "- Save a checkpoint of the `model` trained above (i.e. the model's parameters) to disk.\n",
        "- Initialize a new model, `model2` with the same architecture as used for the `model` you stored. Do *not* train `model2`.\n",
        "- Evaluate `model2` on the validation dataset.\n",
        "- Now, overwrite the initialized, untrained weights of `model2` with the weights you saved into the checkpoint of `model`.\n",
        "- Evaluate `model2`'s validation loss again. It should be of the exact same value as `model`'s validation loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9XGHSA3C2O-"
      },
      "source": [
        "*Hints:*\n",
        " - Read https://www.tensorflow.org/tutorials/keras/save_and_load\n",
        " - Use `model.save_weights`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_hBJsTnAC2O-"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"model_checkpoint.weights.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(Y_train_scaled.shape[1], activation='linear')\n",
        "])\n",
        "\n",
        "#same optimizer and loss\n",
        "model2.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "               loss=tf.keras.losses.MeanSquaredError())"
      ],
      "metadata": {
        "id": "J7dC5EDHKbP9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_before = model2.evaluate(X_val_scaled, Y_val_scaled, verbose=0)\n",
        "print(f\"Validation loss (before loading weights): {val_loss_before:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WukUsguqKpux",
        "outputId": "13ceccee-9eef-48a9-edac-30224137c337"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss (before loading weights): 0.723943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.load_weights(\"model_checkpoint.weights.h5\")\n",
        "\n",
        "val_loss_after = model2.evaluate(X_val_scaled, Y_val_scaled, verbose=0)\n",
        "print(f\"Validation loss (after loading weights): {val_loss_after:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vg-ItMvKtcT",
        "outputId": "eb535f0b-94b4-4c97-e4a5-a26a2a9382ec"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss (after loading weights): 0.043931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9NY8eSC2O_"
      },
      "source": [
        "# Subtask 2.7\n",
        "## Early stopping\n",
        "So the model you ended up with after 100 epochs was not the best one. That has two implications for us: (1) We would not have had to train for that many epochs and could have saved some computing time. (2) We do not have the best model to apply our model to make actual predictions for unseen samples. If we would constantly assess our model's validation performance during training, we could stop optimization as soon as our model's performance does not increase anymore. This is called *early stopping*.\n",
        "\n",
        "### TODO\n",
        "- Implement Early Stopping using `tf.keras.callbacks.EarlyStopping`. Use patience = 5, which means that if the validation loss is not improved after 5 epochs, the training process should be stopped.\n",
        "- Save the model checkpoint after each epoch if the validation loss is improved. Use `tf.keras.callbacks.ModelCheckpoint`.\n",
        "- Train the model for 100 epochs with a learning rate of 0.01.\n",
        "- Load the model checkpoint.\n",
        "- Evaluate the model on the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5oZ3VpgcC2O_"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t--gs2KHC2O_",
        "outputId": "682757d3-962c-40fc-f87f-da884a916f78",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1794\n",
            "Epoch 1: val_loss improved from inf to 0.05048, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1793 - val_loss: 0.0505\n",
            "Epoch 2/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0551\n",
            "Epoch 2: val_loss improved from 0.05048 to 0.04617, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0550 - val_loss: 0.0462\n",
            "Epoch 3/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0518\n",
            "Epoch 3: val_loss improved from 0.04617 to 0.04551, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0518 - val_loss: 0.0455\n",
            "Epoch 4/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0512\n",
            "Epoch 4: val_loss improved from 0.04551 to 0.04527, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0512 - val_loss: 0.0453\n",
            "Epoch 5/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0510\n",
            "Epoch 5: val_loss improved from 0.04527 to 0.04512, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0510 - val_loss: 0.0451\n",
            "Epoch 6/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0509\n",
            "Epoch 6: val_loss improved from 0.04512 to 0.04504, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0508 - val_loss: 0.0450\n",
            "Epoch 7/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507\n",
            "Epoch 7: val_loss improved from 0.04504 to 0.04498, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0507 - val_loss: 0.0450\n",
            "Epoch 8/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507\n",
            "Epoch 8: val_loss improved from 0.04498 to 0.04493, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0507 - val_loss: 0.0449\n",
            "Epoch 9/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507\n",
            "Epoch 9: val_loss improved from 0.04493 to 0.04489, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0507 - val_loss: 0.0449\n",
            "Epoch 10/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506\n",
            "Epoch 10: val_loss improved from 0.04489 to 0.04485, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0506 - val_loss: 0.0449\n",
            "Epoch 11/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506\n",
            "Epoch 11: val_loss improved from 0.04485 to 0.04482, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0506 - val_loss: 0.0448\n",
            "Epoch 12/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506\n",
            "Epoch 12: val_loss improved from 0.04482 to 0.04479, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0505 - val_loss: 0.0448\n",
            "Epoch 13/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0505\n",
            "Epoch 13: val_loss improved from 0.04479 to 0.04476, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0505 - val_loss: 0.0448\n",
            "Epoch 14/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0505\n",
            "Epoch 14: val_loss improved from 0.04476 to 0.04473, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0505 - val_loss: 0.0447\n",
            "Epoch 15/100\n",
            "\u001b[1m677/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0505\n",
            "Epoch 15: val_loss improved from 0.04473 to 0.04471, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0504 - val_loss: 0.0447\n",
            "Epoch 16/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0504\n",
            "Epoch 16: val_loss improved from 0.04471 to 0.04468, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0447\n",
            "Epoch 17/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0504\n",
            "Epoch 17: val_loss improved from 0.04468 to 0.04466, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0447\n",
            "Epoch 18/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 18: val_loss improved from 0.04466 to 0.04464, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0503 - val_loss: 0.0446\n",
            "Epoch 19/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 19: val_loss improved from 0.04464 to 0.04462, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0503 - val_loss: 0.0446\n",
            "Epoch 20/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 20: val_loss improved from 0.04462 to 0.04460, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0503 - val_loss: 0.0446\n",
            "Epoch 21/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0503\n",
            "Epoch 21: val_loss improved from 0.04460 to 0.04458, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0503 - val_loss: 0.0446\n",
            "Epoch 22/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 22: val_loss improved from 0.04458 to 0.04456, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0503 - val_loss: 0.0446\n",
            "Epoch 23/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 23: val_loss improved from 0.04456 to 0.04454, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 24/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 24: val_loss improved from 0.04454 to 0.04451, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 25/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 25: val_loss improved from 0.04451 to 0.04450, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 26/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 26: val_loss improved from 0.04450 to 0.04448, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 27/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0502\n",
            "Epoch 27: val_loss improved from 0.04448 to 0.04446, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 28/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 28: val_loss improved from 0.04446 to 0.04445, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 29/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 29: val_loss improved from 0.04445 to 0.04443, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 30/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 30: val_loss improved from 0.04443 to 0.04442, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 31/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 31: val_loss improved from 0.04442 to 0.04440, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 32/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 32: val_loss improved from 0.04440 to 0.04439, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 33/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 33: val_loss improved from 0.04439 to 0.04438, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 34/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 34: val_loss improved from 0.04438 to 0.04436, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 35/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 35: val_loss improved from 0.04436 to 0.04435, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 36/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 36: val_loss improved from 0.04435 to 0.04434, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 37/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0500\n",
            "Epoch 37: val_loss improved from 0.04434 to 0.04433, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 38/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0500\n",
            "Epoch 38: val_loss improved from 0.04433 to 0.04431, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 39/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 39: val_loss improved from 0.04431 to 0.04430, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 40/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 40: val_loss improved from 0.04430 to 0.04429, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 41/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 41: val_loss improved from 0.04429 to 0.04427, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 42/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 42: val_loss improved from 0.04427 to 0.04426, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 43/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 43: val_loss improved from 0.04426 to 0.04425, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 44/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 44: val_loss improved from 0.04425 to 0.04424, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 45/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 45: val_loss improved from 0.04424 to 0.04423, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 46/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 46: val_loss improved from 0.04423 to 0.04422, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 47/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 47: val_loss improved from 0.04422 to 0.04421, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 48/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 48: val_loss improved from 0.04421 to 0.04420, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 49/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 49: val_loss improved from 0.04420 to 0.04419, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 50/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 50: val_loss improved from 0.04419 to 0.04417, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0498 - val_loss: 0.0442\n",
            "Epoch 51/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 51: val_loss improved from 0.04417 to 0.04416, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0442\n",
            "Epoch 52/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0498\n",
            "Epoch 52: val_loss improved from 0.04416 to 0.04415, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0442\n",
            "Epoch 53/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 53: val_loss improved from 0.04415 to 0.04414, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0498 - val_loss: 0.0441\n",
            "Epoch 54/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 54: val_loss improved from 0.04414 to 0.04414, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0498 - val_loss: 0.0441\n",
            "Epoch 55/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 55: val_loss improved from 0.04414 to 0.04412, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0441\n",
            "Epoch 56/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 56: val_loss improved from 0.04412 to 0.04411, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 57/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 57: val_loss improved from 0.04411 to 0.04411, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 58/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0497\n",
            "Epoch 58: val_loss improved from 0.04411 to 0.04410, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 59/100\n",
            "\u001b[1m678/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 59: val_loss improved from 0.04410 to 0.04410, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 60/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 60: val_loss improved from 0.04410 to 0.04408, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 61/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 61: val_loss improved from 0.04408 to 0.04407, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 62/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 62: val_loss improved from 0.04407 to 0.04406, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 63/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 63: val_loss improved from 0.04406 to 0.04405, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0441\n",
            "Epoch 64/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0497\n",
            "Epoch 64: val_loss improved from 0.04405 to 0.04404, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0440\n",
            "Epoch 65/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 65: val_loss improved from 0.04404 to 0.04402, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0440\n",
            "Epoch 66/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 66: val_loss improved from 0.04402 to 0.04401, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0440\n",
            "Epoch 67/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 67: val_loss improved from 0.04401 to 0.04400, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0496 - val_loss: 0.0440\n",
            "Epoch 68/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 68: val_loss improved from 0.04400 to 0.04398, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 69/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 69: val_loss improved from 0.04398 to 0.04398, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 70/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 70: val_loss did not improve from 0.04398\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 71/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 71: val_loss improved from 0.04398 to 0.04398, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 72/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 72: val_loss improved from 0.04398 to 0.04397, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 73/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 73: val_loss improved from 0.04397 to 0.04396, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 74/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 74: val_loss improved from 0.04396 to 0.04395, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0495 - val_loss: 0.0439\n",
            "Epoch 75/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0494\n",
            "Epoch 75: val_loss improved from 0.04395 to 0.04392, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0439\n",
            "Epoch 76/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 76: val_loss improved from 0.04392 to 0.04385, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0494 - val_loss: 0.0438\n",
            "Epoch 77/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 77: val_loss improved from 0.04385 to 0.04378, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0493 - val_loss: 0.0438\n",
            "Epoch 78/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 78: val_loss improved from 0.04378 to 0.04377, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0493 - val_loss: 0.0438\n",
            "Epoch 79/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 79: val_loss did not improve from 0.04377\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0492 - val_loss: 0.0438\n",
            "Epoch 80/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 80: val_loss improved from 0.04377 to 0.04377, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0438\n",
            "Epoch 81/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 81: val_loss did not improve from 0.04377\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0438\n",
            "Epoch 82/100\n",
            "\u001b[1m678/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 82: val_loss improved from 0.04377 to 0.04376, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0492 - val_loss: 0.0438\n",
            "Epoch 83/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 83: val_loss improved from 0.04376 to 0.04375, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0492 - val_loss: 0.0437\n",
            "Epoch 84/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 84: val_loss improved from 0.04375 to 0.04374, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0492 - val_loss: 0.0437\n",
            "Epoch 85/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 85: val_loss improved from 0.04374 to 0.04372, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0491 - val_loss: 0.0437\n",
            "Epoch 86/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0491\n",
            "Epoch 86: val_loss improved from 0.04372 to 0.04371, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0491 - val_loss: 0.0437\n",
            "Epoch 87/100\n",
            "\u001b[1m678/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 87: val_loss improved from 0.04371 to 0.04371, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0491 - val_loss: 0.0437\n",
            "Epoch 88/100\n",
            "\u001b[1m677/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 88: val_loss improved from 0.04371 to 0.04370, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0491 - val_loss: 0.0437\n",
            "Epoch 89/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 89: val_loss improved from 0.04370 to 0.04368, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0437\n",
            "Epoch 90/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 90: val_loss improved from 0.04368 to 0.04367, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0490 - val_loss: 0.0437\n",
            "Epoch 91/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 91: val_loss improved from 0.04367 to 0.04366, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0437\n",
            "Epoch 92/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 92: val_loss improved from 0.04366 to 0.04365, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0437\n",
            "Epoch 93/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 93: val_loss improved from 0.04365 to 0.04364, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0490 - val_loss: 0.0436\n",
            "Epoch 94/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 94: val_loss improved from 0.04364 to 0.04362, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0490 - val_loss: 0.0436\n",
            "Epoch 95/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 95: val_loss improved from 0.04362 to 0.04361, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0489 - val_loss: 0.0436\n",
            "Epoch 96/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 96: val_loss improved from 0.04361 to 0.04361, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0436\n",
            "Epoch 97/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 97: val_loss improved from 0.04361 to 0.04359, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0436\n",
            "Epoch 98/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 98: val_loss improved from 0.04359 to 0.04357, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0436\n",
            "Epoch 99/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 99: val_loss did not improve from 0.04357\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0489 - val_loss: 0.0436\n",
            "Epoch 100/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 100: val_loss improved from 0.04357 to 0.04356, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0436\n",
            "Validation loss (best saved model): 0.043558\n"
          ]
        }
      ],
      "source": [
        "## TODO load the model checkpoint and evaluate on validation data\n",
        "\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"best_model_earlystop.weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# create new model with same architecture\n",
        "# Recreate model3 with the corrected MLP function\n",
        "model3 = MLP(N_hidden_layers=2, N_neurons=20, activation='relu')\n",
        "\n",
        "\n",
        "# compiling\n",
        "model3.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "               loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "# start the train with 100 epoch\n",
        "history_es = model3.fit(\n",
        "    X_train_scaled, Y_train_scaled,\n",
        "    epochs=100,\n",
        "    batch_size=512,\n",
        "    validation_data=(X_val_scaled, Y_val_scaled),\n",
        "    callbacks=[early_stopping_cb, checkpoint_cb],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# EarlyStopping with restore_best_weights=True will automatically restore the weights\n",
        "# model3.load_weights(\"best_model_earlystop.weights.h5\") # No longer needed\n",
        "\n",
        "val_loss_best = model3.evaluate(X_val_scaled, Y_val_scaled, verbose=0)\n",
        "print(f\"Validation loss (best saved model): {val_loss_best:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFDUpq5yC2PA"
      },
      "source": [
        "#### TODO\n",
        "- Compare the training you just did with the one of the same model trained for 100 epochs. Did you reach best model performance? If so: why? If not: why not?\n",
        "- What is the purpose of `patience`, and why do we need that?\n",
        "- Do the same training as in the previous cell, starting training from scratch, but try different values for `patience` now. Did you end up with a model resulting in the best validation accuracy you have seen so far, but without training the full 100 epochs?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. When comparing the training with early stopping to the full 100 epochs training, the early stopping model often achieves similar or even better performance without wasting time on unnecessary epochs. In this case, the model reached its best validation loss before the 100 epochs ended, indicating that continuing to train beyond that point would likely lead to overfitting or minimal improvements. Early stopping effectively detects when the model stops improving and halts the training, so the best model performance is preserved. However, if early stopping stops too soon, it might miss the true optimal performance if the model was still improving slowly, which means patience settings must be carefully chosen.\n",
        "\n",
        "2. Patience is a parameter that defines how many epochs the training should continue after the last observed improvement in validation loss before stopping. It is important because validation loss can fluctuate slightly from epoch to epoch due to the stochastic nature of training and mini-batches. Without patience, training could stop prematurely during a temporary plateau or minor increase in validation loss, thus missing further possible improvements. By allowing a few more epochs to wait, patience helps ensure the model has a fair chance to improve before training ends, avoiding premature stopping.\n",
        "\n",
        "3. When experimenting with different patience values, you often find a trade-off between training time and model performance. A smaller patience value might stop training early, potentially before the best model is reached, while a larger patience allows more epochs but might waste time after the model has stopped improving. By carefully tuning patience, it is possible to stop training soon after the best validation accuracy is achieved, saving computational resources while maintaining high performance. This approach often yields a model with optimal validation accuracy without completing all 100 epochs, demonstrating the efficiency of early stopping with a well-chosen patience parameter."
      ],
      "metadata": {
        "id": "BcDiiqNj5hJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "e57PjrVlC2PA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9d9ed109-fa90-4a99-805f-df9b25247536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with patience=2\n",
            "Epoch 1/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1677\n",
            "Epoch 1: val_loss improved from inf to 0.05059, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.1677 - val_loss: 0.0506\n",
            "Epoch 2/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0581\n",
            "Epoch 2: val_loss improved from 0.05059 to 0.04687, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0581 - val_loss: 0.0469\n",
            "Epoch 3/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0535\n",
            "Epoch 3: val_loss improved from 0.04687 to 0.04581, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0534 - val_loss: 0.0458\n",
            "Epoch 4/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0519\n",
            "Epoch 4: val_loss improved from 0.04581 to 0.04537, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0519 - val_loss: 0.0454\n",
            "Epoch 5/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0513\n",
            "Epoch 5: val_loss improved from 0.04537 to 0.04519, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0513 - val_loss: 0.0452\n",
            "Epoch 6/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0510\n",
            "Epoch 6: val_loss improved from 0.04519 to 0.04510, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0510 - val_loss: 0.0451\n",
            "Epoch 7/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0509\n",
            "Epoch 7: val_loss improved from 0.04510 to 0.04504, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0509 - val_loss: 0.0450\n",
            "Epoch 8/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0508\n",
            "Epoch 8: val_loss improved from 0.04504 to 0.04497, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0508 - val_loss: 0.0450\n",
            "Epoch 9/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0508\n",
            "Epoch 9: val_loss improved from 0.04497 to 0.04491, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0508 - val_loss: 0.0449\n",
            "Epoch 10/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507\n",
            "Epoch 10: val_loss improved from 0.04491 to 0.04485, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0507 - val_loss: 0.0449\n",
            "Epoch 11/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506\n",
            "Epoch 11: val_loss improved from 0.04485 to 0.04476, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0506 - val_loss: 0.0448\n",
            "Epoch 12/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506\n",
            "Epoch 12: val_loss improved from 0.04476 to 0.04472, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0506 - val_loss: 0.0447\n",
            "Epoch 13/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0505\n",
            "Epoch 13: val_loss improved from 0.04472 to 0.04468, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0505 - val_loss: 0.0447\n",
            "Epoch 14/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0504\n",
            "Epoch 14: val_loss improved from 0.04468 to 0.04465, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0447\n",
            "Epoch 15/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0504\n",
            "Epoch 15: val_loss improved from 0.04465 to 0.04464, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0446\n",
            "Epoch 16/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0504\n",
            "Epoch 16: val_loss improved from 0.04464 to 0.04463, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0446\n",
            "Epoch 17/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0504\n",
            "Epoch 17: val_loss improved from 0.04463 to 0.04459, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0503 - val_loss: 0.0446\n",
            "Epoch 18/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 18: val_loss improved from 0.04459 to 0.04444, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0503 - val_loss: 0.0444\n",
            "Epoch 19/100\n",
            "\u001b[1m677/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 19: val_loss improved from 0.04444 to 0.04430, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0502 - val_loss: 0.0443\n",
            "Epoch 20/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 20: val_loss improved from 0.04430 to 0.04421, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 21/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0500\n",
            "Epoch 21: val_loss improved from 0.04421 to 0.04420, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 22/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 22: val_loss did not improve from 0.04420\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0498 - val_loss: 0.0442\n",
            "Epoch 23/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0499\n",
            "Epoch 23: val_loss did not improve from 0.04420\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0442\n",
            "Validation loss with patience=2: 0.044202\n",
            "\n",
            "Training with patience=5\n",
            "Epoch 1/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2717\n",
            "Epoch 1: val_loss improved from inf to 0.04848, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.2715 - val_loss: 0.0485\n",
            "Epoch 2/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0551\n",
            "Epoch 2: val_loss improved from 0.04848 to 0.04599, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0551 - val_loss: 0.0460\n",
            "Epoch 3/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0518\n",
            "Epoch 3: val_loss improved from 0.04599 to 0.04536, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0517 - val_loss: 0.0454\n",
            "Epoch 4/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0509\n",
            "Epoch 4: val_loss improved from 0.04536 to 0.04513, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0509 - val_loss: 0.0451\n",
            "Epoch 5/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506\n",
            "Epoch 5: val_loss improved from 0.04513 to 0.04500, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0506 - val_loss: 0.0450\n",
            "Epoch 6/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0505\n",
            "Epoch 6: val_loss improved from 0.04500 to 0.04492, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0505 - val_loss: 0.0449\n",
            "Epoch 7/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0504\n",
            "Epoch 7: val_loss improved from 0.04492 to 0.04484, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0504 - val_loss: 0.0448\n",
            "Epoch 8/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 8: val_loss improved from 0.04484 to 0.04478, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0503 - val_loss: 0.0448\n",
            "Epoch 9/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 9: val_loss improved from 0.04478 to 0.04470, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0502 - val_loss: 0.0447\n",
            "Epoch 10/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 10: val_loss improved from 0.04470 to 0.04466, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0502 - val_loss: 0.0447\n",
            "Epoch 11/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0501\n",
            "Epoch 11: val_loss improved from 0.04466 to 0.04463, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0446\n",
            "Epoch 12/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 12: val_loss improved from 0.04463 to 0.04461, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0501 - val_loss: 0.0446\n",
            "Epoch 13/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 13: val_loss improved from 0.04461 to 0.04459, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0501 - val_loss: 0.0446\n",
            "Epoch 14/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 14: val_loss improved from 0.04459 to 0.04458, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0501 - val_loss: 0.0446\n",
            "Epoch 15/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 15: val_loss improved from 0.04458 to 0.04456, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0500 - val_loss: 0.0446\n",
            "Epoch 16/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 16: val_loss improved from 0.04456 to 0.04454, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0445\n",
            "Epoch 17/100\n",
            "\u001b[1m678/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 17: val_loss improved from 0.04454 to 0.04453, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0445\n",
            "Epoch 18/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 18: val_loss improved from 0.04453 to 0.04451, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0500 - val_loss: 0.0445\n",
            "Epoch 19/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 19: val_loss improved from 0.04451 to 0.04448, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0445\n",
            "Epoch 20/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 20: val_loss improved from 0.04448 to 0.04446, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0445\n",
            "Epoch 21/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 21: val_loss improved from 0.04446 to 0.04443, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0444\n",
            "Epoch 22/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 22: val_loss improved from 0.04443 to 0.04441, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0444\n",
            "Epoch 23/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 23: val_loss improved from 0.04441 to 0.04440, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0499 - val_loss: 0.0444\n",
            "Epoch 24/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 24: val_loss improved from 0.04440 to 0.04438, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0498 - val_loss: 0.0444\n",
            "Epoch 25/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 25: val_loss improved from 0.04438 to 0.04437, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0498 - val_loss: 0.0444\n",
            "Epoch 26/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 26: val_loss improved from 0.04437 to 0.04436, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0498 - val_loss: 0.0444\n",
            "Epoch 27/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0498\n",
            "Epoch 27: val_loss improved from 0.04436 to 0.04434, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0498 - val_loss: 0.0443\n",
            "Epoch 28/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 28: val_loss improved from 0.04434 to 0.04433, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0443\n",
            "Epoch 29/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 29: val_loss improved from 0.04433 to 0.04431, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0443\n",
            "Epoch 30/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 30: val_loss improved from 0.04431 to 0.04429, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0497 - val_loss: 0.0443\n",
            "Epoch 31/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 31: val_loss improved from 0.04429 to 0.04426, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0443\n",
            "Epoch 32/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 32: val_loss improved from 0.04426 to 0.04424, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0442\n",
            "Epoch 33/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0497\n",
            "Epoch 33: val_loss improved from 0.04424 to 0.04423, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0496 - val_loss: 0.0442\n",
            "Epoch 34/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 34: val_loss improved from 0.04423 to 0.04422, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0442\n",
            "Epoch 35/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 35: val_loss improved from 0.04422 to 0.04421, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0442\n",
            "Epoch 36/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 36: val_loss improved from 0.04421 to 0.04420, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0442\n",
            "Epoch 37/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 37: val_loss improved from 0.04420 to 0.04420, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0442\n",
            "Epoch 38/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0495\n",
            "Epoch 38: val_loss improved from 0.04420 to 0.04419, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0442\n",
            "Epoch 39/100\n",
            "\u001b[1m676/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 39: val_loss improved from 0.04419 to 0.04418, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0442\n",
            "Epoch 40/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 40: val_loss improved from 0.04418 to 0.04417, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0495 - val_loss: 0.0442\n",
            "Epoch 41/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 41: val_loss improved from 0.04417 to 0.04416, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0495 - val_loss: 0.0442\n",
            "Epoch 42/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 42: val_loss improved from 0.04416 to 0.04416, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0495 - val_loss: 0.0442\n",
            "Epoch 43/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0495\n",
            "Epoch 43: val_loss improved from 0.04416 to 0.04414, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0494 - val_loss: 0.0441\n",
            "Epoch 44/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0495\n",
            "Epoch 44: val_loss improved from 0.04414 to 0.04413, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0441\n",
            "Epoch 45/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 45: val_loss improved from 0.04413 to 0.04412, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0441\n",
            "Epoch 46/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 46: val_loss improved from 0.04412 to 0.04411, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0441\n",
            "Epoch 47/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0494\n",
            "Epoch 47: val_loss improved from 0.04411 to 0.04410, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0441\n",
            "Epoch 48/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 48: val_loss improved from 0.04410 to 0.04409, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0441\n",
            "Epoch 49/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0493\n",
            "Epoch 49: val_loss improved from 0.04409 to 0.04408, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0441\n",
            "Epoch 50/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 50: val_loss improved from 0.04408 to 0.04407, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0441\n",
            "Epoch 51/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 51: val_loss improved from 0.04407 to 0.04406, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0441\n",
            "Epoch 52/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0493\n",
            "Epoch 52: val_loss improved from 0.04406 to 0.04405, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0440\n",
            "Epoch 53/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0493\n",
            "Epoch 53: val_loss improved from 0.04405 to 0.04404, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0440\n",
            "Epoch 54/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0493\n",
            "Epoch 54: val_loss improved from 0.04404 to 0.04403, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0493 - val_loss: 0.0440\n",
            "Epoch 55/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 55: val_loss improved from 0.04403 to 0.04402, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0440\n",
            "Epoch 56/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0492\n",
            "Epoch 56: val_loss improved from 0.04402 to 0.04401, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0492 - val_loss: 0.0440\n",
            "Epoch 57/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0492\n",
            "Epoch 57: val_loss improved from 0.04401 to 0.04400, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0492 - val_loss: 0.0440\n",
            "Epoch 58/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 58: val_loss improved from 0.04400 to 0.04399, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0440\n",
            "Epoch 59/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 59: val_loss improved from 0.04399 to 0.04398, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0440\n",
            "Epoch 60/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 60: val_loss improved from 0.04398 to 0.04396, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0440\n",
            "Epoch 61/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 61: val_loss did not improve from 0.04396\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0440\n",
            "Epoch 62/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0492\n",
            "Epoch 62: val_loss improved from 0.04396 to 0.04395, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 63/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 63: val_loss improved from 0.04395 to 0.04394, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 64/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 64: val_loss improved from 0.04394 to 0.04394, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 65/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 65: val_loss improved from 0.04394 to 0.04392, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 66/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0491\n",
            "Epoch 66: val_loss improved from 0.04392 to 0.04391, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 67/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 67: val_loss improved from 0.04391 to 0.04389, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 68/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 68: val_loss improved from 0.04389 to 0.04389, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0439\n",
            "Epoch 69/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 69: val_loss improved from 0.04389 to 0.04388, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0439\n",
            "Epoch 70/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 70: val_loss improved from 0.04388 to 0.04387, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0439\n",
            "Epoch 71/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 71: val_loss improved from 0.04387 to 0.04384, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 72/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0490\n",
            "Epoch 72: val_loss improved from 0.04384 to 0.04382, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 73/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 73: val_loss improved from 0.04382 to 0.04380, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0438\n",
            "Epoch 74/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 74: val_loss improved from 0.04380 to 0.04375, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0438\n",
            "Epoch 75/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 75: val_loss improved from 0.04375 to 0.04373, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0488 - val_loss: 0.0437\n",
            "Epoch 76/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0488\n",
            "Epoch 76: val_loss improved from 0.04373 to 0.04372, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0488 - val_loss: 0.0437\n",
            "Epoch 77/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0488\n",
            "Epoch 77: val_loss improved from 0.04372 to 0.04371, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0488 - val_loss: 0.0437\n",
            "Epoch 78/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0488\n",
            "Epoch 78: val_loss improved from 0.04371 to 0.04369, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0488 - val_loss: 0.0437\n",
            "Epoch 79/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0488\n",
            "Epoch 79: val_loss improved from 0.04369 to 0.04369, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0488 - val_loss: 0.0437\n",
            "Epoch 80/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487\n",
            "Epoch 80: val_loss improved from 0.04369 to 0.04369, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0437\n",
            "Epoch 81/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487\n",
            "Epoch 81: val_loss improved from 0.04369 to 0.04367, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0487 - val_loss: 0.0437\n",
            "Epoch 82/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487\n",
            "Epoch 82: val_loss improved from 0.04367 to 0.04367, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0437\n",
            "Epoch 83/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0487\n",
            "Epoch 83: val_loss improved from 0.04367 to 0.04366, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0437\n",
            "Epoch 84/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487\n",
            "Epoch 84: val_loss improved from 0.04366 to 0.04364, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0436\n",
            "Epoch 85/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487\n",
            "Epoch 85: val_loss did not improve from 0.04364\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0436\n",
            "Epoch 86/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0486\n",
            "Epoch 86: val_loss improved from 0.04364 to 0.04363, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0486 - val_loss: 0.0436\n",
            "Epoch 87/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0486\n",
            "Epoch 87: val_loss improved from 0.04363 to 0.04363, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0486 - val_loss: 0.0436\n",
            "Epoch 88/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0486\n",
            "Epoch 88: val_loss improved from 0.04363 to 0.04361, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0486 - val_loss: 0.0436\n",
            "Epoch 89/100\n",
            "\u001b[1m678/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0486\n",
            "Epoch 89: val_loss improved from 0.04361 to 0.04360, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0486 - val_loss: 0.0436\n",
            "Epoch 90/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0486\n",
            "Epoch 90: val_loss improved from 0.04360 to 0.04360, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0486 - val_loss: 0.0436\n",
            "Epoch 91/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0485\n",
            "Epoch 91: val_loss improved from 0.04360 to 0.04358, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0485 - val_loss: 0.0436\n",
            "Epoch 92/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0485\n",
            "Epoch 92: val_loss improved from 0.04358 to 0.04357, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0485 - val_loss: 0.0436\n",
            "Epoch 93/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0485\n",
            "Epoch 93: val_loss improved from 0.04357 to 0.04356, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0485 - val_loss: 0.0436\n",
            "Epoch 94/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0485\n",
            "Epoch 94: val_loss improved from 0.04356 to 0.04355, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0485 - val_loss: 0.0435\n",
            "Epoch 95/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0485\n",
            "Epoch 95: val_loss improved from 0.04355 to 0.04353, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0485 - val_loss: 0.0435\n",
            "Epoch 96/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0485\n",
            "Epoch 96: val_loss improved from 0.04353 to 0.04352, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0484 - val_loss: 0.0435\n",
            "Epoch 97/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0485\n",
            "Epoch 97: val_loss improved from 0.04352 to 0.04351, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0484 - val_loss: 0.0435\n",
            "Epoch 98/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0485\n",
            "Epoch 98: val_loss improved from 0.04351 to 0.04349, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0484 - val_loss: 0.0435\n",
            "Epoch 99/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0484\n",
            "Epoch 99: val_loss improved from 0.04349 to 0.04348, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0484 - val_loss: 0.0435\n",
            "Epoch 100/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0484\n",
            "Epoch 100: val_loss improved from 0.04348 to 0.04347, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0484 - val_loss: 0.0435\n",
            "Validation loss with patience=5: 0.043469\n",
            "\n",
            "Training with patience=10\n",
            "Epoch 1/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2111\n",
            "Epoch 1: val_loss improved from inf to 0.04765, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.2109 - val_loss: 0.0477\n",
            "Epoch 2/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0560\n",
            "Epoch 2: val_loss improved from 0.04765 to 0.04546, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0560 - val_loss: 0.0455\n",
            "Epoch 3/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0521\n",
            "Epoch 3: val_loss improved from 0.04546 to 0.04500, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0521 - val_loss: 0.0450\n",
            "Epoch 4/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0512\n",
            "Epoch 4: val_loss improved from 0.04500 to 0.04485, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0512 - val_loss: 0.0448\n",
            "Epoch 5/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0509\n",
            "Epoch 5: val_loss improved from 0.04485 to 0.04478, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0509 - val_loss: 0.0448\n",
            "Epoch 6/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507\n",
            "Epoch 6: val_loss improved from 0.04478 to 0.04472, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0507 - val_loss: 0.0447\n",
            "Epoch 7/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0506\n",
            "Epoch 7: val_loss improved from 0.04472 to 0.04467, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0506 - val_loss: 0.0447\n",
            "Epoch 8/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506\n",
            "Epoch 8: val_loss improved from 0.04467 to 0.04462, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0505 - val_loss: 0.0446\n",
            "Epoch 9/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0505\n",
            "Epoch 9: val_loss improved from 0.04462 to 0.04458, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0505 - val_loss: 0.0446\n",
            "Epoch 10/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0504\n",
            "Epoch 10: val_loss improved from 0.04458 to 0.04453, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0445\n",
            "Epoch 11/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0504\n",
            "Epoch 11: val_loss improved from 0.04453 to 0.04448, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0503 - val_loss: 0.0445\n",
            "Epoch 12/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0503\n",
            "Epoch 12: val_loss improved from 0.04448 to 0.04445, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0503 - val_loss: 0.0445\n",
            "Epoch 13/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 13: val_loss improved from 0.04445 to 0.04442, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0503 - val_loss: 0.0444\n",
            "Epoch 14/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 14: val_loss improved from 0.04442 to 0.04439, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0502 - val_loss: 0.0444\n",
            "Epoch 15/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 15: val_loss improved from 0.04439 to 0.04436, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0502 - val_loss: 0.0444\n",
            "Epoch 16/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 16: val_loss improved from 0.04436 to 0.04435, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0502 - val_loss: 0.0443\n",
            "Epoch 17/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 17: val_loss improved from 0.04435 to 0.04433, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0443\n",
            "Epoch 18/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0501\n",
            "Epoch 18: val_loss improved from 0.04433 to 0.04431, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0443\n",
            "Epoch 19/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 19: val_loss improved from 0.04431 to 0.04429, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0443\n",
            "Epoch 20/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 20: val_loss improved from 0.04429 to 0.04428, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0443\n",
            "Epoch 21/100\n",
            "\u001b[1m678/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 21: val_loss improved from 0.04428 to 0.04426, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0443\n",
            "Epoch 22/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 22: val_loss improved from 0.04426 to 0.04425, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 23/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0501\n",
            "Epoch 23: val_loss improved from 0.04425 to 0.04423, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 24/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 24: val_loss improved from 0.04423 to 0.04422, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 25/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 25: val_loss improved from 0.04422 to 0.04421, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 26/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 26: val_loss improved from 0.04421 to 0.04420, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 27/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0500\n",
            "Epoch 27: val_loss improved from 0.04420 to 0.04419, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 28/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 28: val_loss improved from 0.04419 to 0.04418, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 29/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 29: val_loss improved from 0.04418 to 0.04417, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 30/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 30: val_loss improved from 0.04417 to 0.04416, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 31/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 31: val_loss improved from 0.04416 to 0.04415, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0441\n",
            "Epoch 32/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 32: val_loss improved from 0.04415 to 0.04414, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0441\n",
            "Epoch 33/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0499\n",
            "Epoch 33: val_loss improved from 0.04414 to 0.04413, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0499 - val_loss: 0.0441\n",
            "Epoch 34/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 34: val_loss improved from 0.04413 to 0.04412, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0441\n",
            "Epoch 35/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 35: val_loss improved from 0.04412 to 0.04411, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0441\n",
            "Epoch 36/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 36: val_loss improved from 0.04411 to 0.04410, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0441\n",
            "Epoch 37/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 37: val_loss improved from 0.04410 to 0.04409, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0441\n",
            "Epoch 38/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 38: val_loss improved from 0.04409 to 0.04408, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0441\n",
            "Epoch 39/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 39: val_loss improved from 0.04408 to 0.04407, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0441\n",
            "Epoch 40/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 40: val_loss improved from 0.04407 to 0.04406, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0441\n",
            "Epoch 41/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 41: val_loss improved from 0.04406 to 0.04405, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0440\n",
            "Epoch 42/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 42: val_loss improved from 0.04405 to 0.04404, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0440\n",
            "Epoch 43/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 43: val_loss improved from 0.04404 to 0.04403, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0440\n",
            "Epoch 44/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0498\n",
            "Epoch 44: val_loss improved from 0.04403 to 0.04402, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0498 - val_loss: 0.0440\n",
            "Epoch 45/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 45: val_loss improved from 0.04402 to 0.04401, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0440\n",
            "Epoch 46/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 46: val_loss improved from 0.04401 to 0.04400, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0440\n",
            "Epoch 47/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 47: val_loss improved from 0.04400 to 0.04399, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0440\n",
            "Epoch 48/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 48: val_loss improved from 0.04399 to 0.04399, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0440\n",
            "Epoch 49/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0498\n",
            "Epoch 49: val_loss improved from 0.04399 to 0.04398, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0440\n",
            "Epoch 50/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 50: val_loss improved from 0.04398 to 0.04397, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0440\n",
            "Epoch 51/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 51: val_loss improved from 0.04397 to 0.04397, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0440\n",
            "Epoch 52/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 52: val_loss improved from 0.04397 to 0.04395, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0440\n",
            "Epoch 53/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 53: val_loss improved from 0.04395 to 0.04394, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0439\n",
            "Epoch 54/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 54: val_loss improved from 0.04394 to 0.04394, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0439\n",
            "Epoch 55/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0497\n",
            "Epoch 55: val_loss improved from 0.04394 to 0.04393, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0497 - val_loss: 0.0439\n",
            "Epoch 56/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 56: val_loss improved from 0.04393 to 0.04392, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0439\n",
            "Epoch 57/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 57: val_loss improved from 0.04392 to 0.04392, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0439\n",
            "Epoch 58/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 58: val_loss improved from 0.04392 to 0.04391, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0439\n",
            "Epoch 59/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 59: val_loss improved from 0.04391 to 0.04390, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0439\n",
            "Epoch 60/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 60: val_loss improved from 0.04390 to 0.04390, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0439\n",
            "Epoch 61/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 61: val_loss improved from 0.04390 to 0.04388, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0439\n",
            "Epoch 62/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 62: val_loss improved from 0.04388 to 0.04388, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0439\n",
            "Epoch 63/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 63: val_loss improved from 0.04388 to 0.04387, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0439\n",
            "Epoch 64/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 64: val_loss improved from 0.04387 to 0.04387, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0439\n",
            "Epoch 65/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0496\n",
            "Epoch 65: val_loss improved from 0.04387 to 0.04386, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0439\n",
            "Epoch 66/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 66: val_loss improved from 0.04386 to 0.04385, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0439\n",
            "Epoch 67/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 67: val_loss improved from 0.04385 to 0.04384, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0438\n",
            "Epoch 68/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 68: val_loss improved from 0.04384 to 0.04384, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0438\n",
            "Epoch 69/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 69: val_loss improved from 0.04384 to 0.04383, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 70/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 70: val_loss improved from 0.04383 to 0.04382, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 71/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 71: val_loss improved from 0.04382 to 0.04382, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 72/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 72: val_loss improved from 0.04382 to 0.04381, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 73/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 73: val_loss improved from 0.04381 to 0.04380, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 74/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 74: val_loss improved from 0.04380 to 0.04380, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 75/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 75: val_loss improved from 0.04380 to 0.04379, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 76/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0495\n",
            "Epoch 76: val_loss improved from 0.04379 to 0.04378, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 77/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 77: val_loss improved from 0.04378 to 0.04378, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 78/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 78: val_loss improved from 0.04378 to 0.04377, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 79/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 79: val_loss improved from 0.04377 to 0.04376, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0438\n",
            "Epoch 80/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 80: val_loss did not improve from 0.04376\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0438\n",
            "Epoch 81/100\n",
            "\u001b[1m678/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 81: val_loss improved from 0.04376 to 0.04375, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0438\n",
            "Epoch 82/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 82: val_loss improved from 0.04375 to 0.04375, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0437\n",
            "Epoch 83/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 83: val_loss improved from 0.04375 to 0.04374, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0437\n",
            "Epoch 84/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 84: val_loss improved from 0.04374 to 0.04373, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0437\n",
            "Epoch 85/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 85: val_loss improved from 0.04373 to 0.04372, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0437\n",
            "Epoch 86/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0494\n",
            "Epoch 86: val_loss improved from 0.04372 to 0.04372, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0494 - val_loss: 0.0437\n",
            "Epoch 87/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 87: val_loss improved from 0.04372 to 0.04371, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0437\n",
            "Epoch 88/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 88: val_loss improved from 0.04371 to 0.04371, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0437\n",
            "Epoch 89/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 89: val_loss improved from 0.04371 to 0.04370, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0437\n",
            "Epoch 90/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 90: val_loss improved from 0.04370 to 0.04369, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0437\n",
            "Epoch 91/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 91: val_loss improved from 0.04369 to 0.04369, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0437\n",
            "Epoch 92/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 92: val_loss improved from 0.04369 to 0.04368, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0437\n",
            "Epoch 93/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 93: val_loss improved from 0.04368 to 0.04367, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0437\n",
            "Epoch 94/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 94: val_loss improved from 0.04367 to 0.04366, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0437\n",
            "Epoch 95/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 95: val_loss improved from 0.04366 to 0.04366, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0437\n",
            "Epoch 96/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 96: val_loss improved from 0.04366 to 0.04365, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0437\n",
            "Epoch 97/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 97: val_loss improved from 0.04365 to 0.04365, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0436\n",
            "Epoch 98/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0493\n",
            "Epoch 98: val_loss improved from 0.04365 to 0.04363, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0436\n",
            "Epoch 99/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 99: val_loss improved from 0.04363 to 0.04363, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0436\n",
            "Epoch 100/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 100: val_loss did not improve from 0.04363\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0436\n",
            "Validation loss with patience=10: 0.043630\n",
            "\n",
            "Training with patience=20\n",
            "Epoch 1/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1698\n",
            "Epoch 1: val_loss improved from inf to 0.05180, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.1697 - val_loss: 0.0518\n",
            "Epoch 2/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0618\n",
            "Epoch 2: val_loss improved from 0.05180 to 0.04748, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0618 - val_loss: 0.0475\n",
            "Epoch 3/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0556\n",
            "Epoch 3: val_loss improved from 0.04748 to 0.04573, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0555 - val_loss: 0.0457\n",
            "Epoch 4/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0527\n",
            "Epoch 4: val_loss improved from 0.04573 to 0.04509, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0527 - val_loss: 0.0451\n",
            "Epoch 5/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0515\n",
            "Epoch 5: val_loss improved from 0.04509 to 0.04488, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0515 - val_loss: 0.0449\n",
            "Epoch 6/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0510\n",
            "Epoch 6: val_loss improved from 0.04488 to 0.04481, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0510 - val_loss: 0.0448\n",
            "Epoch 7/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0508\n",
            "Epoch 7: val_loss improved from 0.04481 to 0.04474, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0507 - val_loss: 0.0447\n",
            "Epoch 8/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506\n",
            "Epoch 8: val_loss improved from 0.04474 to 0.04465, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0506 - val_loss: 0.0446\n",
            "Epoch 9/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0505\n",
            "Epoch 9: val_loss improved from 0.04465 to 0.04458, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0505 - val_loss: 0.0446\n",
            "Epoch 10/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0504\n",
            "Epoch 10: val_loss improved from 0.04458 to 0.04453, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0445\n",
            "Epoch 11/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0503\n",
            "Epoch 11: val_loss improved from 0.04453 to 0.04450, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0503 - val_loss: 0.0445\n",
            "Epoch 12/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503\n",
            "Epoch 12: val_loss improved from 0.04450 to 0.04449, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 13/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 13: val_loss improved from 0.04449 to 0.04447, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 14/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 14: val_loss improved from 0.04447 to 0.04446, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 15/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502\n",
            "Epoch 15: val_loss improved from 0.04446 to 0.04444, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 16/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0501\n",
            "Epoch 16: val_loss improved from 0.04444 to 0.04443, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 17/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 17: val_loss improved from 0.04443 to 0.04442, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 18/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 18: val_loss improved from 0.04442 to 0.04441, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 19/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 19: val_loss improved from 0.04441 to 0.04440, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 20/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 20: val_loss improved from 0.04440 to 0.04439, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 21/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0501\n",
            "Epoch 21: val_loss improved from 0.04439 to 0.04439, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 22/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501\n",
            "Epoch 22: val_loss improved from 0.04439 to 0.04438, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0444\n",
            "Epoch 23/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 23: val_loss improved from 0.04438 to 0.04438, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0444\n",
            "Epoch 24/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 24: val_loss improved from 0.04438 to 0.04437, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0444\n",
            "Epoch 25/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0500\n",
            "Epoch 25: val_loss improved from 0.04437 to 0.04437, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0444\n",
            "Epoch 26/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0500\n",
            "Epoch 26: val_loss improved from 0.04437 to 0.04436, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0444\n",
            "Epoch 27/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 27: val_loss improved from 0.04436 to 0.04436, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0444\n",
            "Epoch 28/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 28: val_loss improved from 0.04436 to 0.04435, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0444\n",
            "Epoch 29/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 29: val_loss improved from 0.04435 to 0.04435, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 30/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 30: val_loss improved from 0.04435 to 0.04435, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 31/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0500\n",
            "Epoch 31: val_loss did not improve from 0.04435\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 32/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 32: val_loss improved from 0.04435 to 0.04435, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 33/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 33: val_loss improved from 0.04435 to 0.04434, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 34/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 34: val_loss improved from 0.04434 to 0.04434, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 35/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500\n",
            "Epoch 35: val_loss improved from 0.04434 to 0.04434, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0500 - val_loss: 0.0443\n",
            "Epoch 36/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0500\n",
            "Epoch 36: val_loss improved from 0.04434 to 0.04433, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 37/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 37: val_loss improved from 0.04433 to 0.04429, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 38/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 38: val_loss did not improve from 0.04429\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 39/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 39: val_loss improved from 0.04429 to 0.04429, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 40/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 40: val_loss improved from 0.04429 to 0.04429, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 41/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 41: val_loss improved from 0.04429 to 0.04428, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 42/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0499\n",
            "Epoch 42: val_loss improved from 0.04428 to 0.04428, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0443\n",
            "Epoch 43/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 43: val_loss improved from 0.04428 to 0.04425, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0499 - val_loss: 0.0442\n",
            "Epoch 44/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499\n",
            "Epoch 44: val_loss improved from 0.04425 to 0.04421, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0442\n",
            "Epoch 45/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 45: val_loss improved from 0.04421 to 0.04420, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0442\n",
            "Epoch 46/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0498\n",
            "Epoch 46: val_loss improved from 0.04420 to 0.04417, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0442\n",
            "Epoch 47/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0498\n",
            "Epoch 47: val_loss improved from 0.04417 to 0.04413, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0498 - val_loss: 0.0441\n",
            "Epoch 48/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 48: val_loss improved from 0.04413 to 0.04409, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 49/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 49: val_loss improved from 0.04409 to 0.04406, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0441\n",
            "Epoch 50/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497\n",
            "Epoch 50: val_loss improved from 0.04406 to 0.04405, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0440\n",
            "Epoch 51/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 51: val_loss improved from 0.04405 to 0.04403, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0496 - val_loss: 0.0440\n",
            "Epoch 52/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0496\n",
            "Epoch 52: val_loss improved from 0.04403 to 0.04403, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0496 - val_loss: 0.0440\n",
            "Epoch 53/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496\n",
            "Epoch 53: val_loss did not improve from 0.04403\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 54/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 54: val_loss did not improve from 0.04403\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 55/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 55: val_loss improved from 0.04403 to 0.04403, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 56/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0495\n",
            "Epoch 56: val_loss improved from 0.04403 to 0.04402, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 57/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 57: val_loss did not improve from 0.04402\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 58/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 58: val_loss did not improve from 0.04402\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0495 - val_loss: 0.0440\n",
            "Epoch 59/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495\n",
            "Epoch 59: val_loss improved from 0.04402 to 0.04402, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0440\n",
            "Epoch 60/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 60: val_loss improved from 0.04402 to 0.04401, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0440\n",
            "Epoch 61/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0494\n",
            "Epoch 61: val_loss improved from 0.04401 to 0.04401, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0440\n",
            "Epoch 62/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 62: val_loss improved from 0.04401 to 0.04400, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0440\n",
            "Epoch 63/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 63: val_loss improved from 0.04400 to 0.04399, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0494 - val_loss: 0.0440\n",
            "Epoch 64/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 64: val_loss improved from 0.04399 to 0.04398, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0440\n",
            "Epoch 65/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0494\n",
            "Epoch 65: val_loss improved from 0.04398 to 0.04397, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0440\n",
            "Epoch 66/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 66: val_loss improved from 0.04397 to 0.04396, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0440\n",
            "Epoch 67/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0493\n",
            "Epoch 67: val_loss improved from 0.04396 to 0.04396, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0493 - val_loss: 0.0440\n",
            "Epoch 68/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 68: val_loss improved from 0.04396 to 0.04395, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0440\n",
            "Epoch 69/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 69: val_loss improved from 0.04395 to 0.04394, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0439\n",
            "Epoch 70/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 70: val_loss did not improve from 0.04394\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0440\n",
            "Epoch 71/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493\n",
            "Epoch 71: val_loss did not improve from 0.04394\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0439\n",
            "Epoch 72/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 72: val_loss improved from 0.04394 to 0.04394, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0439\n",
            "Epoch 73/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0492\n",
            "Epoch 73: val_loss improved from 0.04394 to 0.04393, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0439\n",
            "Epoch 74/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 74: val_loss improved from 0.04393 to 0.04392, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0439\n",
            "Epoch 75/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 75: val_loss improved from 0.04392 to 0.04391, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0439\n",
            "Epoch 76/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 76: val_loss improved from 0.04391 to 0.04390, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0439\n",
            "Epoch 77/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492\n",
            "Epoch 77: val_loss improved from 0.04390 to 0.04389, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 78/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0492\n",
            "Epoch 78: val_loss improved from 0.04389 to 0.04388, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 79/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 79: val_loss improved from 0.04388 to 0.04387, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 80/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 80: val_loss improved from 0.04387 to 0.04386, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 81/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 81: val_loss improved from 0.04386 to 0.04385, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 82/100\n",
            "\u001b[1m699/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 82: val_loss improved from 0.04385 to 0.04384, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0438\n",
            "Epoch 83/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0491\n",
            "Epoch 83: val_loss improved from 0.04384 to 0.04383, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0438\n",
            "Epoch 84/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 84: val_loss did not improve from 0.04383\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 85/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 85: val_loss improved from 0.04383 to 0.04382, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 86/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491\n",
            "Epoch 86: val_loss improved from 0.04382 to 0.04381, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 87/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 87: val_loss improved from 0.04381 to 0.04380, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 88/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0490\n",
            "Epoch 88: val_loss improved from 0.04380 to 0.04379, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 89/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 89: val_loss improved from 0.04379 to 0.04378, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 90/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 90: val_loss improved from 0.04378 to 0.04377, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0438\n",
            "Epoch 91/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490\n",
            "Epoch 91: val_loss improved from 0.04377 to 0.04376, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0438\n",
            "Epoch 92/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 92: val_loss improved from 0.04376 to 0.04376, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0438\n",
            "Epoch 93/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0489\n",
            "Epoch 93: val_loss improved from 0.04376 to 0.04375, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0489 - val_loss: 0.0438\n",
            "Epoch 94/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0489\n",
            "Epoch 94: val_loss improved from 0.04375 to 0.04374, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0437\n",
            "Epoch 95/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 95: val_loss improved from 0.04374 to 0.04373, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0437\n",
            "Epoch 96/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 96: val_loss improved from 0.04373 to 0.04372, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0489 - val_loss: 0.0437\n",
            "Epoch 97/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489\n",
            "Epoch 97: val_loss improved from 0.04372 to 0.04371, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0488 - val_loss: 0.0437\n",
            "Epoch 98/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487\n",
            "Epoch 98: val_loss improved from 0.04371 to 0.04371, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0437\n",
            "Epoch 99/100\n",
            "\u001b[1m702/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0487\n",
            "Epoch 99: val_loss improved from 0.04371 to 0.04369, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0487 - val_loss: 0.0437\n",
            "Epoch 100/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487\n",
            "Epoch 100: val_loss improved from 0.04369 to 0.04365, saving model to best_model_earlystop.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0436\n",
            "Validation loss with patience=20: 0.043647\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for patience_val in [2, 5, 10, 20]:\n",
        "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=patience_val,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model_earlystop.weights.h5\",\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    model3 = MLP(N_hidden_layers=2, N_neurons=20, activation='relu')\n",
        "    model3.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "                   loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "    print(f\"Training with patience={patience_val}\")\n",
        "    history = model3.fit(\n",
        "        X_train_scaled, Y_train_scaled,\n",
        "        epochs=100,\n",
        "        batch_size=512,\n",
        "        validation_data=(X_val_scaled, Y_val_scaled),\n",
        "        callbacks=[early_stopping_cb, checkpoint_cb],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    val_loss_best = model3.evaluate(X_val_scaled, Y_val_scaled, verbose=0)\n",
        "    print(f\"Validation loss with patience={patience_val}: {val_loss_best:.6f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfxLB9C3C2PA"
      },
      "source": [
        "## Explore batch size *(optional)*\n",
        "\n",
        "*This task is optional, you do not need to solve it*\n",
        "\n",
        "Let us explore even more model and training parameters. In this section, we will see the impact of batch size on training. Let us use a learning rate of $10^{-3}$ from now on.\n",
        "\n",
        "\n",
        "#### TODO *(optional)*\n",
        "- Run training of the same model used above with\n",
        "    - batch size 1 for **one epoch**\n",
        "    - batch size 1024 for 100 epochs, using early stopping with patience 10\n",
        "- Compare your training results of all three batch sizes you have trained, i.e. batch size 1, 512 (from above) and 1024\n",
        "- Was it smart to set batch size to 1?\n",
        "- How long (in terms of computing time) do your models need to train for the different batch sizes? (You could even measure this with python, using the `time` package)\n",
        "- What is the impact on model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAqdsFALC2PA"
      },
      "outputs": [],
      "source": [
        "# Batch size 1\n",
        "\n",
        "###############################\n",
        "## YOUR CODE HERE - OPTIONAL ##\n",
        "###############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmjOYzg0C2PA"
      },
      "outputs": [],
      "source": [
        "# Batch size 1024\n",
        "\n",
        "###############################\n",
        "## YOUR CODE HERE - OPTIONAL ##\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfdU7HmAC2PB"
      },
      "source": [
        "# Subtask 2.8\n",
        "## What about the architecture?\n",
        "\n",
        "How does architecture affect predictive performance?\n",
        "\n",
        "#### TODO:\n",
        "In the following, try to improve model performance by varying\n",
        "- number of hidden layers\n",
        "- number of neurons per each hidden layer\n",
        "- activation function\n",
        "\n",
        "These parameters are called hyper-parameters, since they are excluded from model optimization. Instead, we have to set them by hand and explore them to find a model with good predictive accuracy.\n",
        "\n",
        "Vary only one hyper-parameter at a time. If you would vary multiple parameters at the same time, it would be harder for you to see the impact that each parameter has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCuTjNFwC2PB"
      },
      "outputs": [],
      "source": [
        "# number of hidden layers\n",
        "for layers in [1, 3, 4]:\n",
        "    print(f\"\\nTraining with {layers} hidden layers\")\n",
        "    model = MLP(hidden_layers=layers, neurons=32, activation='relu')\n",
        "    history = model.fit(X_train_scaled, Y_train_scaled,\n",
        "                        validation_data=(X_val_scaled, Y_val_scaled),\n",
        "                        epochs=10, verbose=0)\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "    print(f\"Validation loss after 20 epochs: {val_loss:.4f}\")\n",
        "\n",
        "# 4 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UuF5XQDC2PB"
      },
      "outputs": [],
      "source": [
        "# number of neurons per each hidden layer\n",
        "\n",
        "for neurons in [20, 50, 100]:\n",
        "    print(f\"\\nTraining with {neurons} hidden layers\")\n",
        "    model = MLP(hidden_layers=2, neurons=neurons, activation='relu')\n",
        "    history = model.fit(X_train_scaled, Y_train_scaled,\n",
        "                        validation_data=(X_val_scaled, Y_val_scaled),\n",
        "                        epochs=10, verbose=0)\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "    print(f\"Validation loss after 20 epochs: {val_loss:.4f}\")\n",
        "\n",
        "# 100 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lldWZHnFC2PC"
      },
      "outputs": [],
      "source": [
        "# activation function\n",
        "\n",
        "print(f\"\\nTraining with activation function: tanh (2 hidden layers, 32 neurons)\")\n",
        "model = MLP(hidden_layers=2, neurons=32, activation='tanh')\n",
        "history = model.fit(X_train_scaled, Y_train_scaled,\n",
        "                    validation_data=(X_val_scaled, Y_val_scaled),\n",
        "                    epochs=10, verbose=0)\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "print(f\"Validation loss after 20 epochs: {val_loss:.4f}\")\n",
        "\n",
        "# Use tanh instead of ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqbuCfzjC2PC"
      },
      "source": [
        "**TODO Your answer here**\n",
        "\n",
        "1. How good do you get?\n",
        "2. Which hyper-parameter makes the largest difference?\n",
        "3. Does it always help to make your model bigger (i.e. wider / deeper)? Why not?\n",
        "\n",
        "**TODO Your answer here**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeMQ6uSiC2PD"
      },
      "source": [
        "**Your answers:**\n",
        "1. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0buENZuzC2PD"
      },
      "source": [
        "# Subtask 2.9\n",
        "## Challenge\n",
        "#### TODO\n",
        "- If you choose your best values for number hidden units, number of layers and activation function that you determined by varying them independently above: Does performance improve? Why?\n",
        "- Vary all of the parameters at the same time to maximize the predictive performance of your model. How good do you get?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZgWXxSnlC2PD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "56ea7f59-48c4-4ce9-bade-3771d36f7ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3364\n",
            "Epoch 1: val_loss improved from inf to 0.10179, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.3363 - val_loss: 0.1018\n",
            "Epoch 2/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1865\n",
            "Epoch 2: val_loss improved from 0.10179 to 0.07723, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1857 - val_loss: 0.0772\n",
            "Epoch 3/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1531\n",
            "Epoch 3: val_loss improved from 0.07723 to 0.06410, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1527 - val_loss: 0.0641\n",
            "Epoch 4/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1331\n",
            "Epoch 4: val_loss improved from 0.06410 to 0.05780, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1330 - val_loss: 0.0578\n",
            "Epoch 5/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1216\n",
            "Epoch 5: val_loss improved from 0.05780 to 0.05417, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1212 - val_loss: 0.0542\n",
            "Epoch 6/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1132\n",
            "Epoch 6: val_loss improved from 0.05417 to 0.05168, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1131 - val_loss: 0.0517\n",
            "Epoch 7/100\n",
            "\u001b[1m677/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1073\n",
            "Epoch 7: val_loss improved from 0.05168 to 0.04989, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1070 - val_loss: 0.0499\n",
            "Epoch 8/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1031\n",
            "Epoch 8: val_loss did not improve from 0.04989\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1030 - val_loss: 0.0500\n",
            "Epoch 9/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1056\n",
            "Epoch 9: val_loss improved from 0.04989 to 0.04912, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1055 - val_loss: 0.0491\n",
            "Epoch 10/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1032\n",
            "Epoch 10: val_loss improved from 0.04912 to 0.04827, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1032 - val_loss: 0.0483\n",
            "Epoch 11/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1008\n",
            "Epoch 11: val_loss improved from 0.04827 to 0.04749, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1008 - val_loss: 0.0475\n",
            "Epoch 12/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0986\n",
            "Epoch 12: val_loss improved from 0.04749 to 0.04677, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0985 - val_loss: 0.0468\n",
            "Epoch 13/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0964\n",
            "Epoch 13: val_loss improved from 0.04677 to 0.04610, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0963 - val_loss: 0.0461\n",
            "Epoch 14/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0939\n",
            "Epoch 14: val_loss improved from 0.04610 to 0.04583, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0939 - val_loss: 0.0458\n",
            "Epoch 15/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0927\n",
            "Epoch 15: val_loss did not improve from 0.04583\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0926 - val_loss: 0.0485\n",
            "Epoch 16/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0956\n",
            "Epoch 16: val_loss did not improve from 0.04583\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0954 - val_loss: 0.0467\n",
            "Epoch 17/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0942\n",
            "Epoch 17: val_loss did not improve from 0.04583\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0940 - val_loss: 0.0466\n",
            "Epoch 18/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0926\n",
            "Epoch 18: val_loss improved from 0.04583 to 0.04485, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0925 - val_loss: 0.0449\n",
            "Epoch 19/100\n",
            "\u001b[1m688/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0906\n",
            "Epoch 19: val_loss improved from 0.04485 to 0.04419, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0905 - val_loss: 0.0442\n",
            "Epoch 20/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0893\n",
            "Epoch 20: val_loss improved from 0.04419 to 0.04385, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0892 - val_loss: 0.0438\n",
            "Epoch 21/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0882\n",
            "Epoch 21: val_loss improved from 0.04385 to 0.04362, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0881 - val_loss: 0.0436\n",
            "Epoch 22/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0872\n",
            "Epoch 22: val_loss improved from 0.04362 to 0.04347, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0871 - val_loss: 0.0435\n",
            "Epoch 23/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0863\n",
            "Epoch 23: val_loss improved from 0.04347 to 0.04335, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0862 - val_loss: 0.0433\n",
            "Epoch 24/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0855\n",
            "Epoch 24: val_loss improved from 0.04335 to 0.04324, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0854 - val_loss: 0.0432\n",
            "Epoch 25/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0847\n",
            "Epoch 25: val_loss improved from 0.04324 to 0.04314, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0847 - val_loss: 0.0431\n",
            "Epoch 26/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0840\n",
            "Epoch 26: val_loss improved from 0.04314 to 0.04304, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0840 - val_loss: 0.0430\n",
            "Epoch 27/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0833\n",
            "Epoch 27: val_loss improved from 0.04304 to 0.04293, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0833 - val_loss: 0.0429\n",
            "Epoch 28/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0827\n",
            "Epoch 28: val_loss improved from 0.04293 to 0.04283, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0827 - val_loss: 0.0428\n",
            "Epoch 29/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0821\n",
            "Epoch 29: val_loss improved from 0.04283 to 0.04272, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0821 - val_loss: 0.0427\n",
            "Epoch 30/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0816\n",
            "Epoch 30: val_loss improved from 0.04272 to 0.04261, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0816 - val_loss: 0.0426\n",
            "Epoch 31/100\n",
            "\u001b[1m696/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0811\n",
            "Epoch 31: val_loss improved from 0.04261 to 0.04251, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0811 - val_loss: 0.0425\n",
            "Epoch 32/100\n",
            "\u001b[1m693/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0806\n",
            "Epoch 32: val_loss improved from 0.04251 to 0.04240, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0806 - val_loss: 0.0424\n",
            "Epoch 33/100\n",
            "\u001b[1m686/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0802\n",
            "Epoch 33: val_loss improved from 0.04240 to 0.04231, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0802 - val_loss: 0.0423\n",
            "Epoch 34/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0797\n",
            "Epoch 34: val_loss improved from 0.04231 to 0.04221, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0797 - val_loss: 0.0422\n",
            "Epoch 35/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0793\n",
            "Epoch 35: val_loss improved from 0.04221 to 0.04213, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0793 - val_loss: 0.0421\n",
            "Epoch 36/100\n",
            "\u001b[1m695/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0790\n",
            "Epoch 36: val_loss improved from 0.04213 to 0.04205, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0789 - val_loss: 0.0421\n",
            "Epoch 37/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0786\n",
            "Epoch 37: val_loss improved from 0.04205 to 0.04198, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0786 - val_loss: 0.0420\n",
            "Epoch 38/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0783\n",
            "Epoch 38: val_loss improved from 0.04198 to 0.04192, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0782 - val_loss: 0.0419\n",
            "Epoch 39/100\n",
            "\u001b[1m683/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0779\n",
            "Epoch 39: val_loss improved from 0.04192 to 0.04186, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0779 - val_loss: 0.0419\n",
            "Epoch 40/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0776\n",
            "Epoch 40: val_loss improved from 0.04186 to 0.04181, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0776 - val_loss: 0.0418\n",
            "Epoch 41/100\n",
            "\u001b[1m681/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0773\n",
            "Epoch 41: val_loss improved from 0.04181 to 0.04176, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0772 - val_loss: 0.0418\n",
            "Epoch 42/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0769\n",
            "Epoch 42: val_loss improved from 0.04176 to 0.04172, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0769 - val_loss: 0.0417\n",
            "Epoch 43/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0767\n",
            "Epoch 43: val_loss improved from 0.04172 to 0.04168, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0766 - val_loss: 0.0417\n",
            "Epoch 44/100\n",
            "\u001b[1m689/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0764\n",
            "Epoch 44: val_loss improved from 0.04168 to 0.04164, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0763 - val_loss: 0.0416\n",
            "Epoch 45/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0761\n",
            "Epoch 45: val_loss improved from 0.04164 to 0.04161, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0761 - val_loss: 0.0416\n",
            "Epoch 46/100\n",
            "\u001b[1m680/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0758\n",
            "Epoch 46: val_loss improved from 0.04161 to 0.04158, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0758 - val_loss: 0.0416\n",
            "Epoch 47/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0756\n",
            "Epoch 47: val_loss improved from 0.04158 to 0.04155, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0755 - val_loss: 0.0415\n",
            "Epoch 48/100\n",
            "\u001b[1m692/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0753\n",
            "Epoch 48: val_loss improved from 0.04155 to 0.04152, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0753 - val_loss: 0.0415\n",
            "Epoch 49/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0750\n",
            "Epoch 49: val_loss improved from 0.04152 to 0.04150, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0750 - val_loss: 0.0415\n",
            "Epoch 50/100\n",
            "\u001b[1m678/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0748\n",
            "Epoch 50: val_loss improved from 0.04150 to 0.04147, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0748 - val_loss: 0.0415\n",
            "Epoch 51/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0745\n",
            "Epoch 51: val_loss improved from 0.04147 to 0.04145, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0745 - val_loss: 0.0415\n",
            "Epoch 52/100\n",
            "\u001b[1m684/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0743\n",
            "Epoch 52: val_loss improved from 0.04145 to 0.04143, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0743 - val_loss: 0.0414\n",
            "Epoch 53/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0741\n",
            "Epoch 53: val_loss improved from 0.04143 to 0.04142, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0741 - val_loss: 0.0414\n",
            "Epoch 54/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0739\n",
            "Epoch 54: val_loss improved from 0.04142 to 0.04140, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0739 - val_loss: 0.0414\n",
            "Epoch 55/100\n",
            "\u001b[1m700/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0736\n",
            "Epoch 55: val_loss improved from 0.04140 to 0.04139, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0736 - val_loss: 0.0414\n",
            "Epoch 56/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0734\n",
            "Epoch 56: val_loss improved from 0.04139 to 0.04138, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0734 - val_loss: 0.0414\n",
            "Epoch 57/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0732\n",
            "Epoch 57: val_loss improved from 0.04138 to 0.04137, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0732 - val_loss: 0.0414\n",
            "Epoch 58/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0730\n",
            "Epoch 58: val_loss improved from 0.04137 to 0.04137, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0730 - val_loss: 0.0414\n",
            "Epoch 59/100\n",
            "\u001b[1m685/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0728\n",
            "Epoch 59: val_loss improved from 0.04137 to 0.04136, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0728 - val_loss: 0.0414\n",
            "Epoch 60/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0726\n",
            "Epoch 60: val_loss improved from 0.04136 to 0.04136, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0726 - val_loss: 0.0414\n",
            "Epoch 61/100\n",
            "\u001b[1m690/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0724\n",
            "Epoch 61: val_loss improved from 0.04136 to 0.04136, saving model to best_model_combined.weights.h5\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0724 - val_loss: 0.0414\n",
            "Epoch 62/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0722\n",
            "Epoch 62: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0722 - val_loss: 0.0414\n",
            "Epoch 63/100\n",
            "\u001b[1m694/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0720\n",
            "Epoch 63: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0720 - val_loss: 0.0414\n",
            "Epoch 64/100\n",
            "\u001b[1m698/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0718\n",
            "Epoch 64: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0718 - val_loss: 0.0414\n",
            "Epoch 65/100\n",
            "\u001b[1m687/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0716\n",
            "Epoch 65: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0716 - val_loss: 0.0414\n",
            "Epoch 66/100\n",
            "\u001b[1m679/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0714\n",
            "Epoch 66: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0714 - val_loss: 0.0414\n",
            "Epoch 67/100\n",
            "\u001b[1m701/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0712\n",
            "Epoch 67: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0712 - val_loss: 0.0414\n",
            "Epoch 68/100\n",
            "\u001b[1m697/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0710\n",
            "Epoch 68: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0710 - val_loss: 0.0414\n",
            "Epoch 69/100\n",
            "\u001b[1m682/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0709\n",
            "Epoch 69: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0709 - val_loss: 0.0414\n",
            "Epoch 70/100\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0707\n",
            "Epoch 70: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0707 - val_loss: 0.0414\n",
            "Epoch 71/100\n",
            "\u001b[1m691/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0705\n",
            "Epoch 71: val_loss did not improve from 0.04136\n",
            "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0705 - val_loss: 0.0414\n",
            "Best validation loss (combined best model): 0.041361\n"
          ]
        }
      ],
      "source": [
        "# Define best model\n",
        "best_model = MLP(N_hidden_layers=3, N_neurons=100, activation='tanh')\n",
        "\n",
        "# Compile\n",
        "best_model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "    loss=tf.keras.losses.MeanSquaredError()\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"best_model_combined.weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train\n",
        "history_best = best_model.fit(\n",
        "    X_train_scaled, Y_train_scaled,\n",
        "    epochs=100,\n",
        "    batch_size=512,\n",
        "    validation_data=(X_val_scaled, Y_val_scaled),\n",
        "    callbacks=[early_stopping_cb, checkpoint_cb],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Load best weights and evaluate\n",
        "best_model.load_weights(\"best_model_combined.weights.h5\")\n",
        "val_loss_best = best_model.evaluate(X_val_scaled, Y_val_scaled, verbose=0)\n",
        "print(f\"Best validation loss (combined best model): {val_loss_best:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2KrIuJhC2PE"
      },
      "source": [
        "# Subtask 2.10\n",
        "## Evaluate your best model on test set, once!\n",
        "When doing a study, at the very end right before writing up your paper, you evaluate the best model you chose on the test set. This is the performance value you will report to the public.\n",
        "\n",
        "#### TODO\n",
        "- Evaluate the model on the testing dataset.\n",
        "- Plot the reference mass flow rate vs. the predicted values for the first 50 samples of the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohxmHN7jC2PE"
      },
      "outputs": [],
      "source": [
        "# TODO evaluate the model\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fLJN21jC2PF"
      },
      "outputs": [],
      "source": [
        "# TODO plot results\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "\n",
        "y_pred = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN6VjcLoC2PF"
      },
      "source": [
        "**TODO Your answer here**\n",
        "\n",
        "1. Is the test loss of your model as good as the validation loss?\n",
        "2. If those values are different: How can you explain the difference?\n",
        "3. Why should you never use test set performance when trying out different hyper-parameters and architectures?\n",
        "\n",
        "**TODO Your answer here**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebqYoZwkC2PG"
      },
      "source": [
        "<span style='color:red'>**Your answer:**</span>\n",
        "\n",
        "1. ..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Exercises1_2021.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "062ad8061f6e7aca79bc9a27e986d47521568c0a72ae67729c623b905ab2cf31"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}