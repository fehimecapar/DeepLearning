{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsqKM5DLlLRm"
      },
      "source": [
        "# Lecture 2 – MNIST dataset and classification via MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqgbLHhc6uzB"
      },
      "source": [
        "# Requirements:\n",
        "\n",
        "- [Google Colab](https://colab.research.google.com/)\n",
        "\n",
        "If running locally:\n",
        "\n",
        "- [NumPy](https://numpy.org/doc/stable/user/quickstart.html)\n",
        "- [Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
        "- [TensorFlow 2](https://www.tensorflow.org/tutorials)\n",
        "- [Matplotlib](https://matplotlib.org/stable/tutorials/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8RW2QHHFQYe"
      },
      "source": [
        "# Objectives:\n",
        "\n",
        "- Getting familiar with MNIST dataset.\n",
        "- Implementing an MLP via TensorFlow.\n",
        "- Training using gradient decent algorithm.\n",
        "- Overfitting, early stopping, regularization etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhw-7EA-10Xm"
      },
      "source": [
        "## Tutorials\n",
        "\n",
        "Some python libraries are required to accomplish the tasks assigned in this homework. If you feel like you need to follow a tutorial before, feel free to do so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMyEloGz__BZ"
      },
      "source": [
        "*   [Scikit-learn Tutorials](https://www.tensorflow.org/tutorials)\n",
        "*   [TensorFlow Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n",
        "*   [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUzkHuvX6uzD"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this exercise we will load MNIST dataset and train an MLP model to classify handwritten digits. We will use the gradient decent algorithm to optimize the parameters of the model during training. Further, we study the influence of different hyperparameters, i.e. learning rate, batch size, activation function, number of hidden layers, and number of neurons, on the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ActTWMg4XZ5v"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LArjND15dGNh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpqN-O0F_atv"
      },
      "source": [
        "## System checks\n",
        "\n",
        "Is there any GPU available?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1r5yPHY_hsl"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "cpus = tf.config.list_physical_devices('CPU')\n",
        "print(gpus)\n",
        "print(cpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djg-c1-FgC6Z"
      },
      "source": [
        "Choose your device for computation. CPU or one of your CUDA devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMCbCpAFao6q"
      },
      "outputs": [],
      "source": [
        "tf.config.set_visible_devices(gpus, 'GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcjytq1x6uzH"
      },
      "source": [
        "# 1. Load the data\n",
        "\n",
        "Let us load the raw data from Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REV3xy1P6uzH"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2wls2q_6uzI"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc5rvfuE6uzI"
      },
      "outputs": [],
      "source": [
        "print(\"Number of original training examples:\", len(x_train))\n",
        "print(\"Number of original test examples:\", len(x_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYXm68Ur6uzI"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "print(\"minimum of x_train:\", x_train.min(), \"maximum of x_train:\", x_train.max())\n",
        "print(\"minimum of y_train:\", y_train.min(), \"maximum of y_train:\", y_train.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKo0aG326uzJ"
      },
      "source": [
        "Show the first example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsszUIWI6uzJ"
      },
      "outputs": [],
      "source": [
        "print(\"Label:\", y_train[0])\n",
        "plt.imshow(x_train[0])\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IofCHBr26uzJ"
      },
      "source": [
        "# 2. Implementing an MLP from scratch in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbnVS5q16uzJ"
      },
      "source": [
        "We need a so-called **Dense** layer. Let us implement it as a Python class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k21lBVBW6uzJ"
      },
      "outputs": [],
      "source": [
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.activation = activation\n",
        "\n",
        "        w_shape = (input_size, output_size)\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "        self.W = tf.Variable(w_initial_value)\n",
        "\n",
        "        b_shape = (output_size,)S\n",
        "        b_initial_value = tf.zeros(b_shape)\n",
        "        self.b = tf.Variable(b_initial_value)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return [self.W, self.b]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKU7V4bp6uzJ"
      },
      "source": [
        "Now, let’s create a **NaiveSequential** class to chain these layers. It wraps a list of layers\n",
        "and exposes a `__call__()` method that simply calls the underlying layers on the\n",
        "inputs, in order. It also features a `weights` property to easily keep track of the `layers`’\n",
        "parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNai-IEY6uzK"
      },
      "outputs": [],
      "source": [
        "class NaiveSequential:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "           x = layer(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "       weights = []\n",
        "       for layer in self.layers:\n",
        "           weights += layer.weights\n",
        "       return weights\n",
        "\n",
        "    def compile(self, loss, optimizer):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x_batch, y_batch = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(x_batch)\n",
        "            loss = self.loss(y_batch, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.weights)\n",
        "\n",
        "        # for g, w in zip(gradients, self.weights):\n",
        "        #     w.assign_sub(g * self.lr)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.weights))\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x_batch, y_batch = data\n",
        "\n",
        "        predictions = self(x_batch)\n",
        "        loss = self.loss(y_batch, predictions)\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def fit(self, train_dataset, epochs, test_dataset):\n",
        "\n",
        "        history = {'loss':[], 'val_loss': []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Train loop\n",
        "            train_loss = tf.keras.metrics.Mean()\n",
        "            for x_batch, y_batch in train_dataset:\n",
        "                train_loss(self.train_step((x_batch, y_batch))[\"loss\"])\n",
        "            history['loss'].append(train_loss.result())\n",
        "\n",
        "            # Test loop\n",
        "            test_loss = tf.keras.metrics.Mean()\n",
        "            for x_batch, y_batch in test_dataset:\n",
        "                test_loss(self.test_step((x_batch, y_batch))[\"loss\"])\n",
        "            history['val_loss'].append(test_loss.result())\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch {epoch + 1}: train loss = {history['loss'][-1]:.4f}, test loss = {history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4BNGgPl6uzK"
      },
      "source": [
        "Using this **NaiveDense** class and this **NaiveSequential** class, we can create a mock\n",
        "Keras model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFGba8J36uzK"
      },
      "outputs": [],
      "source": [
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size=28 * 28, output_size=32, activation=tf.nn.relu),\n",
        "    NaiveDense(input_size=32, output_size=10, activation=tf.nn.softmax)\n",
        "])\n",
        "assert len(model.weights) == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG1aiVmq6uzL"
      },
      "source": [
        "In `compile` function, we specify the loss function and the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncxdRLD76uzL"
      },
      "outputs": [],
      "source": [
        "class GD():\n",
        "    def __init__(self, learning_rate = 0.001):\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def apply_gradients(self, zip_grads_weights):\n",
        "        for g, w in zip_grads_weights:\n",
        "            w.assign_sub(g * self.lr)\n",
        "\n",
        "optimizer = GD()\n",
        "# optimizer = tf.keras.optimizers.SGD()\n",
        "model.compile(tf.keras.losses.SparseCategoricalCrossentropy(), optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hghh8Ue6uzL"
      },
      "source": [
        "Finally, let's create batches of the data using `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCzxinZY6uzL"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "x_train = x_train.reshape(-1, 28*28).astype(np.float32)\n",
        "x_test = x_test.reshape(-1, 28*28).astype(np.float32)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0IzMBnF6uzL"
      },
      "source": [
        "# 3. Training loop\n",
        "An epoch of training simply consists of repeating the training step for each batch in\n",
        "the training data, and the full training loop is simply the repetition of one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGsn4BhH6uzL"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "history = model.fit(train_dataset, num_epochs, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5_6QQ496uzM"
      },
      "source": [
        "### Evaluating the model\n",
        "We can evaluate the model by taking the `argmax` of its predictions over the test images,\n",
        "and comparing it to the expected labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imP5Po0w6uzM"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model(x_test.reshape(-1, 28*28).astype(np.float32)).numpy(), axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5AZUhkv6uzM"
      },
      "source": [
        "Let us report the prediction for one of the sample images in the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRLqQQDt6uzM"
      },
      "outputs": [],
      "source": [
        "n = 0\n",
        "print(\"Label:\", y_test[n], \",  Prediction:\", y_pred[n])\n",
        "plt.imshow(x_test[n].reshape((28, 28)))\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwSLNxUi6uzM"
      },
      "source": [
        "Now, we can report accuracy, precision, and recall using `sklearn.metrics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YhFfsr_6uzN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "precision = precision_score(y_pred, y_test, average='macro')\n",
        "recall = recall_score(y_pred, y_test, average='macro')\n",
        "\n",
        "print('accuracy: ', accuracy, '\\nprecision: ', precision, '\\nrecall: ', recall)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.06.2025"
      ],
      "metadata": {
        "id": "pPVigSyR6x4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers, models\n",
        "\n",
        "def get_model():\n",
        "  inp = layers.Input(shape=(28,28,3))\n",
        "  out = layers.Conv2D(2,5)(inp)\n",
        "  #out = layers.MaxPool2D(2)(x)\n",
        "  return models.Model(inp, out)\n",
        "\n",
        "nn = get_model()\n",
        "nn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "052muL6w6zwf",
        "outputId": "9a2694f2-e57a-4281-9e3a-4944a3badfc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │           \u001b[38;5;34m152\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m152\u001b[0m (608.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">152</span> (608.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m152\u001b[0m (608.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">152</span> (608.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7iFKsdCe8wHQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "062ad8061f6e7aca79bc9a27e986d47521568c0a72ae67729c623b905ab2cf31"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}